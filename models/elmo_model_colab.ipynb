{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.6"
    },
    "pycharm": {
      "stem_cell": {
        "cell_type": "raw",
        "metadata": {
          "collapsed": false
        },
        "source": []
      }
    },
    "colab": {
      "name": "project_model_colab.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "jNQxJylqH94_",
        "colab_type": "code",
        "outputId": "5c636109-2270-4b87-d25e-13bbe803ff79",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "pycharm": {
          "is_executing": false
        },
        "id": "5_ew5nQJH2iy",
        "colab_type": "code",
        "outputId": "03376c89-32fb-45f6-8378-7576f525b06a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "! pip install allennlp\n",
        "! pip install seqeval\n",
        "\n",
        "from allennlp.modules.elmo import Elmo, batch_to_ids\n",
        "from allennlp.modules.conditional_random_field import ConditionalRandomField, allowed_transitions\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torch.nn.utils.rnn import pad_sequence, pack_sequence, pack_padded_sequence, pad_packed_sequence\n",
        "from torch.utils.data.sampler import SubsetRandomSampler\n",
        "from torch.utils.tensorboard import SummaryWriter\n",
        "\n",
        "import numpy as np\n",
        "from time import time\n",
        "from datetime import datetime\n",
        "\n",
        "from pathlib import Path\n",
        "from collections import Counter\n",
        "\n",
        "from typing import List, Dict\n",
        "\n",
        "from seqeval.metrics import f1_score, classification_report"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: allennlp in /usr/local/lib/python3.6/dist-packages (0.9.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from allennlp) (1.18.4)\n",
            "Requirement already satisfied: requests>=2.18 in /usr/local/lib/python3.6/dist-packages (from allennlp) (2.23.0)\n",
            "Requirement already satisfied: boto3 in /usr/local/lib/python3.6/dist-packages (from allennlp) (1.13.13)\n",
            "Requirement already satisfied: parsimonious>=0.8.0 in /usr/local/lib/python3.6/dist-packages (from allennlp) (0.8.1)\n",
            "Requirement already satisfied: jsonpickle in /usr/local/lib/python3.6/dist-packages (from allennlp) (1.4.1)\n",
            "Requirement already satisfied: tqdm>=4.19 in /usr/local/lib/python3.6/dist-packages (from allennlp) (4.41.1)\n",
            "Requirement already satisfied: numpydoc>=0.8.0 in /usr/local/lib/python3.6/dist-packages (from allennlp) (1.0.0)\n",
            "Requirement already satisfied: gevent>=1.3.6 in /usr/local/lib/python3.6/dist-packages (from allennlp) (20.5.2)\n",
            "Requirement already satisfied: jsonnet>=0.10.0; sys_platform != \"win32\" in /usr/local/lib/python3.6/dist-packages (from allennlp) (0.16.0)\n",
            "Requirement already satisfied: h5py in /usr/local/lib/python3.6/dist-packages (from allennlp) (2.10.0)\n",
            "Requirement already satisfied: flaky in /usr/local/lib/python3.6/dist-packages (from allennlp) (3.6.1)\n",
            "Requirement already satisfied: sqlparse>=0.2.4 in /usr/local/lib/python3.6/dist-packages (from allennlp) (0.3.1)\n",
            "Requirement already satisfied: flask>=1.0.2 in /usr/local/lib/python3.6/dist-packages (from allennlp) (1.1.2)\n",
            "Requirement already satisfied: responses>=0.7 in /usr/local/lib/python3.6/dist-packages (from allennlp) (0.10.14)\n",
            "Requirement already satisfied: pytz>=2017.3 in /usr/local/lib/python3.6/dist-packages (from allennlp) (2018.9)\n",
            "Requirement already satisfied: pytest in /usr/local/lib/python3.6/dist-packages (from allennlp) (3.6.4)\n",
            "Requirement already satisfied: pytorch-pretrained-bert>=0.6.0 in /usr/local/lib/python3.6/dist-packages (from allennlp) (0.6.2)\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.6/dist-packages (from allennlp) (3.2.5)\n",
            "Requirement already satisfied: word2number>=1.1 in /usr/local/lib/python3.6/dist-packages (from allennlp) (1.1)\n",
            "Requirement already satisfied: matplotlib>=2.2.3 in /usr/local/lib/python3.6/dist-packages (from allennlp) (3.2.1)\n",
            "Requirement already satisfied: spacy<2.2,>=2.1.0 in /usr/local/lib/python3.6/dist-packages (from allennlp) (2.1.9)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.6/dist-packages (from allennlp) (0.22.2.post1)\n",
            "Requirement already satisfied: conllu==1.3.1 in /usr/local/lib/python3.6/dist-packages (from allennlp) (1.3.1)\n",
            "Requirement already satisfied: pytorch-transformers==1.1.0 in /usr/local/lib/python3.6/dist-packages (from allennlp) (1.1.0)\n",
            "Requirement already satisfied: tensorboardX>=1.2 in /usr/local/lib/python3.6/dist-packages (from allennlp) (2.0)\n",
            "Requirement already satisfied: ftfy in /usr/local/lib/python3.6/dist-packages (from allennlp) (5.7)\n",
            "Requirement already satisfied: torch>=1.2.0 in /usr/local/lib/python3.6/dist-packages (from allennlp) (1.5.0+cu101)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.6/dist-packages (from allennlp) (1.4.1)\n",
            "Requirement already satisfied: overrides in /usr/local/lib/python3.6/dist-packages (from allennlp) (3.0.0)\n",
            "Requirement already satisfied: unidecode in /usr/local/lib/python3.6/dist-packages (from allennlp) (1.1.1)\n",
            "Requirement already satisfied: editdistance in /usr/local/lib/python3.6/dist-packages (from allennlp) (0.5.3)\n",
            "Requirement already satisfied: flask-cors>=3.0.7 in /usr/local/lib/python3.6/dist-packages (from allennlp) (3.0.8)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests>=2.18->allennlp) (1.24.3)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests>=2.18->allennlp) (3.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests>=2.18->allennlp) (2020.4.5.1)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests>=2.18->allennlp) (2.9)\n",
            "Requirement already satisfied: botocore<1.17.0,>=1.16.13 in /usr/local/lib/python3.6/dist-packages (from boto3->allennlp) (1.16.13)\n",
            "Requirement already satisfied: s3transfer<0.4.0,>=0.3.0 in /usr/local/lib/python3.6/dist-packages (from boto3->allennlp) (0.3.3)\n",
            "Requirement already satisfied: jmespath<1.0.0,>=0.7.1 in /usr/local/lib/python3.6/dist-packages (from boto3->allennlp) (0.10.0)\n",
            "Requirement already satisfied: six>=1.9.0 in /usr/local/lib/python3.6/dist-packages (from parsimonious>=0.8.0->allennlp) (1.12.0)\n",
            "Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.6/dist-packages (from jsonpickle->allennlp) (1.6.0)\n",
            "Requirement already satisfied: sphinx>=1.6.5 in /usr/local/lib/python3.6/dist-packages (from numpydoc>=0.8.0->allennlp) (1.8.5)\n",
            "Requirement already satisfied: Jinja2>=2.3 in /usr/local/lib/python3.6/dist-packages (from numpydoc>=0.8.0->allennlp) (2.11.2)\n",
            "Requirement already satisfied: zope.interface in /usr/local/lib/python3.6/dist-packages (from gevent>=1.3.6->allennlp) (5.1.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.6/dist-packages (from gevent>=1.3.6->allennlp) (47.1.1)\n",
            "Requirement already satisfied: greenlet>=0.4.14; platform_python_implementation == \"CPython\" in /usr/local/lib/python3.6/dist-packages (from gevent>=1.3.6->allennlp) (0.4.15)\n",
            "Requirement already satisfied: zope.event in /usr/local/lib/python3.6/dist-packages (from gevent>=1.3.6->allennlp) (4.4)\n",
            "Requirement already satisfied: Werkzeug>=0.15 in /usr/local/lib/python3.6/dist-packages (from flask>=1.0.2->allennlp) (1.0.1)\n",
            "Requirement already satisfied: itsdangerous>=0.24 in /usr/local/lib/python3.6/dist-packages (from flask>=1.0.2->allennlp) (1.1.0)\n",
            "Requirement already satisfied: click>=5.1 in /usr/local/lib/python3.6/dist-packages (from flask>=1.0.2->allennlp) (7.1.2)\n",
            "Requirement already satisfied: pluggy<0.8,>=0.5 in /usr/local/lib/python3.6/dist-packages (from pytest->allennlp) (0.7.1)\n",
            "Requirement already satisfied: attrs>=17.4.0 in /usr/local/lib/python3.6/dist-packages (from pytest->allennlp) (19.3.0)\n",
            "Requirement already satisfied: py>=1.5.0 in /usr/local/lib/python3.6/dist-packages (from pytest->allennlp) (1.8.1)\n",
            "Requirement already satisfied: more-itertools>=4.0.0 in /usr/local/lib/python3.6/dist-packages (from pytest->allennlp) (8.3.0)\n",
            "Requirement already satisfied: atomicwrites>=1.0 in /usr/local/lib/python3.6/dist-packages (from pytest->allennlp) (1.4.0)\n",
            "Requirement already satisfied: regex in /usr/local/lib/python3.6/dist-packages (from pytorch-pretrained-bert>=0.6.0->allennlp) (2019.12.20)\n",
            "Requirement already satisfied: python-dateutil>=2.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib>=2.2.3->allennlp) (2.8.1)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.6/dist-packages (from matplotlib>=2.2.3->allennlp) (0.10.0)\n",
            "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib>=2.2.3->allennlp) (2.4.7)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib>=2.2.3->allennlp) (1.2.0)\n",
            "Requirement already satisfied: srsly<1.1.0,>=0.0.6 in /usr/local/lib/python3.6/dist-packages (from spacy<2.2,>=2.1.0->allennlp) (1.0.2)\n",
            "Requirement already satisfied: thinc<7.1.0,>=7.0.8 in /usr/local/lib/python3.6/dist-packages (from spacy<2.2,>=2.1.0->allennlp) (7.0.8)\n",
            "Requirement already satisfied: plac<1.0.0,>=0.9.6 in /usr/local/lib/python3.6/dist-packages (from spacy<2.2,>=2.1.0->allennlp) (0.9.6)\n",
            "Requirement already satisfied: wasabi<1.1.0,>=0.2.0 in /usr/local/lib/python3.6/dist-packages (from spacy<2.2,>=2.1.0->allennlp) (0.6.0)\n",
            "Requirement already satisfied: preshed<2.1.0,>=2.0.1 in /usr/local/lib/python3.6/dist-packages (from spacy<2.2,>=2.1.0->allennlp) (2.0.1)\n",
            "Requirement already satisfied: blis<0.3.0,>=0.2.2 in /usr/local/lib/python3.6/dist-packages (from spacy<2.2,>=2.1.0->allennlp) (0.2.4)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.6/dist-packages (from spacy<2.2,>=2.1.0->allennlp) (2.0.3)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.6/dist-packages (from spacy<2.2,>=2.1.0->allennlp) (1.0.2)\n",
            "Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.6/dist-packages (from scikit-learn->allennlp) (0.15.1)\n",
            "Requirement already satisfied: sentencepiece in /usr/local/lib/python3.6/dist-packages (from pytorch-transformers==1.1.0->allennlp) (0.1.91)\n",
            "Requirement already satisfied: protobuf>=3.8.0 in /usr/local/lib/python3.6/dist-packages (from tensorboardX>=1.2->allennlp) (3.10.0)\n",
            "Requirement already satisfied: wcwidth in /usr/local/lib/python3.6/dist-packages (from ftfy->allennlp) (0.1.9)\n",
            "Requirement already satisfied: future in /usr/local/lib/python3.6/dist-packages (from torch>=1.2.0->allennlp) (0.16.0)\n",
            "Requirement already satisfied: docutils<0.16,>=0.10 in /usr/local/lib/python3.6/dist-packages (from botocore<1.17.0,>=1.16.13->boto3->allennlp) (0.15.2)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.6/dist-packages (from importlib-metadata->jsonpickle->allennlp) (3.1.0)\n",
            "Requirement already satisfied: alabaster<0.8,>=0.7 in /usr/local/lib/python3.6/dist-packages (from sphinx>=1.6.5->numpydoc>=0.8.0->allennlp) (0.7.12)\n",
            "Requirement already satisfied: sphinxcontrib-websupport in /usr/local/lib/python3.6/dist-packages (from sphinx>=1.6.5->numpydoc>=0.8.0->allennlp) (1.2.2)\n",
            "Requirement already satisfied: snowballstemmer>=1.1 in /usr/local/lib/python3.6/dist-packages (from sphinx>=1.6.5->numpydoc>=0.8.0->allennlp) (2.0.0)\n",
            "Requirement already satisfied: imagesize in /usr/local/lib/python3.6/dist-packages (from sphinx>=1.6.5->numpydoc>=0.8.0->allennlp) (1.2.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.6/dist-packages (from sphinx>=1.6.5->numpydoc>=0.8.0->allennlp) (20.4)\n",
            "Requirement already satisfied: babel!=2.0,>=1.3 in /usr/local/lib/python3.6/dist-packages (from sphinx>=1.6.5->numpydoc>=0.8.0->allennlp) (2.8.0)\n",
            "Requirement already satisfied: Pygments>=2.0 in /usr/local/lib/python3.6/dist-packages (from sphinx>=1.6.5->numpydoc>=0.8.0->allennlp) (2.1.3)\n",
            "Requirement already satisfied: MarkupSafe>=0.23 in /usr/local/lib/python3.6/dist-packages (from Jinja2>=2.3->numpydoc>=0.8.0->allennlp) (1.1.1)\n",
            "Requirement already satisfied: seqeval in /usr/local/lib/python3.6/dist-packages (0.0.12)\n",
            "Requirement already satisfied: Keras>=2.2.4 in /usr/local/lib/python3.6/dist-packages (from seqeval) (2.3.1)\n",
            "Requirement already satisfied: numpy>=1.14.0 in /usr/local/lib/python3.6/dist-packages (from seqeval) (1.18.4)\n",
            "Requirement already satisfied: six>=1.9.0 in /usr/local/lib/python3.6/dist-packages (from Keras>=2.2.4->seqeval) (1.12.0)\n",
            "Requirement already satisfied: scipy>=0.14 in /usr/local/lib/python3.6/dist-packages (from Keras>=2.2.4->seqeval) (1.4.1)\n",
            "Requirement already satisfied: keras-applications>=1.0.6 in /usr/local/lib/python3.6/dist-packages (from Keras>=2.2.4->seqeval) (1.0.8)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.6/dist-packages (from Keras>=2.2.4->seqeval) (3.13)\n",
            "Requirement already satisfied: keras-preprocessing>=1.0.5 in /usr/local/lib/python3.6/dist-packages (from Keras>=2.2.4->seqeval) (1.1.2)\n",
            "Requirement already satisfied: h5py in /usr/local/lib/python3.6/dist-packages (from Keras>=2.2.4->seqeval) (2.10.0)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "i-yyNic_2yQ1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "plt.rcParams['figure.figsize'] = (10.0, 8.0) # default size of plots\n",
        "\n",
        "%config InlineBackend.figure_format = 'svg'"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "pycharm": {
          "is_executing": false
        },
        "id": "rn5eohf1H2i4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "pycharm": {
          "is_executing": false
        },
        "id": "635ylzEAH2jO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class BaseVocab:\n",
        "    def __init__(self, data, idx=0, lower=False):\n",
        "        self.data = data\n",
        "        self.lower = lower\n",
        "        self.idx = idx\n",
        "        self.build_vocab()\n",
        "        \n",
        "    def normalize_unit(self, unit):\n",
        "        if self.lower:\n",
        "            return unit.lower()\n",
        "        else:\n",
        "            return unit\n",
        "        \n",
        "    def unit2id(self, unit):\n",
        "        unit = self.normalize_unit(unit)\n",
        "        if unit in self._unit2id:\n",
        "            return self._unit2id[unit]\n",
        "        else:\n",
        "            return self._unit2id[UNK]\n",
        "    \n",
        "    def id2unit(self, id):\n",
        "        return self._id2unit[id]\n",
        "    \n",
        "    def map(self, units):\n",
        "        return [self.unit2id(unit) for unit in units]\n",
        "\n",
        "    def unmap(self, ids):\n",
        "        return [self.id2unit(idx) for idx in ids]\n",
        "        \n",
        "    def build_vocab(self):\n",
        "        NotImplementedError()\n",
        "        \n",
        "    def __len__(self):\n",
        "        return len(self._unit2id)\n",
        "    "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "pycharm": {
          "is_executing": false
        },
        "id": "9OQY7hwaH2jS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class PretrainedWordVocab(BaseVocab):\n",
        "    def build_vocab(self):\n",
        "        self._id2unit = VOCAB_PREFIX + self.data\n",
        "        self._unit2id = {w:i for i, w in enumerate(self._id2unit)}"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "pycharm": {
          "is_executing": false
        },
        "id": "0Y6FhE3MH2jY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class WordVocab(BaseVocab):\n",
        "    def build_vocab(self):\n",
        "        if self.lower:\n",
        "            counter = Counter([w[self.idx].lower() for sent in self.data for w in sent])\n",
        "        else:\n",
        "            counter = Counter([w[self.idx] for sent in self.data for w in sent])\n",
        "\n",
        "        self._id2unit = VOCAB_PREFIX + list(sorted(list(counter.keys()), key=lambda k: counter[k], reverse=True))\n",
        "        self._unit2id = {w:i for i, w in enumerate(self._id2unit)}\n",
        "        \n",
        "    @property\n",
        "    def items(self):\n",
        "        return {i:w for i, w in enumerate(self._id2unit)}"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "pycharm": {
          "is_executing": false
        },
        "id": "wm2zO6jXH2jk",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "FIELD_NUM = 2\n",
        "\n",
        "class Word:\n",
        "    def __init__(self, word):\n",
        "        self._text = word[0]\n",
        "        self._label = word[1]\n",
        "\n",
        "    @property\n",
        "    def text(self):\n",
        "        return self._text\n",
        "    \n",
        "    @property\n",
        "    def label(self):\n",
        "        return self._label\n",
        "\n",
        "\n",
        "class Sentence:\n",
        "    def __init__(self, words):\n",
        "        self._words = [Word(w) for w in words]\n",
        "\n",
        "    @property\n",
        "    def words(self):\n",
        "        return self._words\n",
        "\n",
        "    \n",
        "class Document:\n",
        "    def __init__(self, file_path):\n",
        "        self._sentences = []\n",
        "        self.load_conll(open(file_path, encoding='utf-8'))\n",
        "\n",
        "\n",
        "    def load_conll(self, f, ignore_gapping=False):\n",
        "        \"\"\" Load the file or string into the CoNLL-U format data.\n",
        "        Input: file or string reader, where the data is in CoNLL-U format.\n",
        "        Output: a list of list of list for each token in each sentence in the data, where the innermost list represents \n",
        "        all fields of a token.\n",
        "        \"\"\"\n",
        "        doc, sent = [], []\n",
        "        for line in f:\n",
        "            line = line.strip()\n",
        "            if len(line) == 0:\n",
        "                if len(sent) > 0:\n",
        "                    doc.append(Sentence(sent))\n",
        "                    sent = []\n",
        "            else:\n",
        "                if line.startswith('#'): # skip comment line\n",
        "                    continue\n",
        "                array = line.split('\\t')\n",
        "                if ignore_gapping and '.' in array[0]:\n",
        "                    continue\n",
        "                assert len(array) == FIELD_NUM, \\\n",
        "                        f\"Cannot parse CoNLL line: expecting {FIELD_NUM} fields, {len(array)} found.\"\n",
        "                sent += [array]\n",
        "        if len(sent) > 0:\n",
        "            doc.append(Sentence(sent))\n",
        "        self._sentences = doc\n",
        "\n",
        "    \n",
        "    @property\n",
        "    def sentences(self):\n",
        "        return self._sentences\n",
        "\n",
        "\n",
        "    def get(self, fields, as_sentences=False):\n",
        "        assert isinstance(fields, list), \"Must provide field names as a list.\"\n",
        "        assert len(fields) >= 1, \"Must have at least one field.\"\n",
        "\n",
        "        results = []\n",
        "        for sentence in self.sentences:\n",
        "            cursent = []\n",
        "            units = sentence.words\n",
        "            for unit in units:\n",
        "                if len(fields) == 1:\n",
        "                    cursent += [getattr(unit, fields[0])]\n",
        "                else:\n",
        "                    cursent += [[getattr(unit, field) for field in fields]]\n",
        "\n",
        "            # decide whether append the results as a sentence or a whole list\n",
        "            if as_sentences:\n",
        "                results.append(cursent)\n",
        "            else:\n",
        "                results += cursent\n",
        "        return results"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "pycharm": {
          "is_executing": false
        },
        "id": "ronJqgiXH2jp",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class CONLLUDataset(Dataset):\n",
        "    def __init__(self, doc, vocab=None, test=False):\n",
        "        self.test = test\n",
        "        data = self.load_doc(doc)\n",
        "\n",
        "        if vocab is None:\n",
        "            self.vocab = self.init_vocab(data)\n",
        "        else:\n",
        "            self.vocab = vocab\n",
        "\n",
        "        self.data = self.preprocess(data, self.vocab)\n",
        "\n",
        "    def init_vocab(self, data):\n",
        "        wordvocab = WordVocab(data, idx=0)\n",
        "        labelvocab = WordVocab(data, idx=1)\n",
        "        vocab = {\n",
        "            'word': wordvocab,\n",
        "            'label': labelvocab}\n",
        "        return vocab\n",
        "\n",
        "    def preprocess(self, data, vocab):\n",
        "        processed = []    \n",
        "        for sent in data:\n",
        "            processed_sent = [vocab['word'].map([w[0] for w in sent])]\n",
        "            processed_sent += [vocab['label'].map([w[1] for w in sent])]\n",
        "            processed_sent += [[w[0] for w in sent]]\n",
        "            processed.append(processed_sent)\n",
        "        return processed\n",
        "        \n",
        "    def load_doc(self, doc):\n",
        "        data = doc.get(['text', 'label'], as_sentences=True)\n",
        "        return data\n",
        "            \n",
        "    def __len__(self):\n",
        "        return len(self.data)\n",
        "    \n",
        "    def __getitem__(self, idx):\n",
        "        return self.data[idx]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "pycharm": {
          "is_executing": false
        },
        "id": "Z9yK1Cc3H2kN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def pad_collate(batch):\n",
        "    (sents, labels, sents_as_strings) = zip(*batch)\n",
        "\n",
        "    elmo_pad = batch_to_ids(sents_as_strings)\n",
        "\n",
        "    sent_lens = [len(s) for s in sents]\n",
        "\n",
        "    sents = [torch.LongTensor(w).to(device) for w in sents]\n",
        "    labels = [torch.LongTensor(label).to(device) for label in labels]\n",
        "\n",
        "    sent_pad = pad_sequence(sents, batch_first=True, padding_value=PAD_ID)\n",
        "    label_pad = pad_sequence(labels, batch_first=True, padding_value=PAD_ID)\n",
        "\n",
        "    sent_pad = sent_pad.to(device)\n",
        "    label_pad = label_pad.to(device)\n",
        "    elmo_pad = elmo_pad.to(device)\n",
        "\n",
        "    return sent_pad, label_pad, elmo_pad, sent_lens"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "pycharm": {
          "is_executing": false
        },
        "id": "Iv1BtNW-H2ks",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Tagger(nn.Module):\n",
        "    def __init__(self, vocab: Dict[str, BaseVocab], word_emb_dim: int,\n",
        "                 char_emb_dim: int, transformed_dim: int, \n",
        "                 hidden_dim: int, num_layers: int, dropout: int):\n",
        "        super().__init__()\n",
        "\n",
        "        self.vocab = vocab\n",
        "        self.num_layers = num_layers\n",
        "        self.hidden_dim = hidden_dim\n",
        "\n",
        "        input_size = 0\n",
        "\n",
        "        self.elmo_pretrained_emb = elmo\n",
        "        self.elmo_trans_pretrained = nn.Linear(word_emb_dim, transformed_dim, bias=False)\n",
        "\n",
        "        input_size += transformed_dim\n",
        "\n",
        "        self.lstm = nn.LSTM(input_size, hidden_dim, num_layers, batch_first=True, bidirectional=True)\n",
        "        \n",
        "        # Hidden-to-label linear layer (2* is because of the bidirectionality)\n",
        "        self.label_clf = nn.Linear(2* hidden_dim, len(vocab['label']))\n",
        "\n",
        "        # Add CRF layer, extracting constraints according to label scheme\n",
        "        self.crf = ConditionalRandomField(len(vocab['label']), constraints=allowed_transitions('BIO', vocab['label'].items))\n",
        "\n",
        "        self.drop = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, sent_pad, label_pad, elmo_pad, sent_lens):\n",
        "\n",
        "        inputs = []\n",
        "        \n",
        "        elmo_dict = self.elmo_pretrained_emb(elmo_pad)\n",
        "        elmo_pretrained_emb_list = elmo_dict['elmo_representations']\n",
        "        elmo_mask = elmo_dict['mask']\n",
        "        elmo_pretrained_emb = torch.cat(elmo_pretrained_emb_list)\n",
        "        elmo_pretrained_emb = self.elmo_trans_pretrained(elmo_pretrained_emb)\n",
        "\n",
        "        # Pretrained word emb shape: [batch_size, seq_length, word_emb_dim]\n",
        "        inputs += [elmo_pretrained_emb]\n",
        "\n",
        "        inputs = torch.cat(inputs)\n",
        "\n",
        "        lstm_inputs = self.drop(inputs)\n",
        "        lstm_inputs = pack_padded_sequence(lstm_inputs, sent_lens, batch_first=True, enforce_sorted=False)\n",
        "        # LSTM inputs shape: [seq_length, concatenation of embeddings]\n",
        "        \n",
        "        lstm_outputs, _ = self.lstm(lstm_inputs)\n",
        "        lstm_outputs, _ = pad_packed_sequence(lstm_outputs, batch_first=True)\n",
        "        # LSTM outputs shape: [seq_length, 2*hidden_dim]\n",
        "\n",
        "        pred = self.label_clf(self.drop(lstm_outputs))\n",
        "        # Pred shape: [batch_size, seq_length, num_tags]\n",
        "        \n",
        "        mask = elmo_mask\n",
        "        # Mask shape: [batch_size, seq_length]\n",
        "\n",
        "        # Replace the softmax probs with Viterbi-generated most likely tags\n",
        "        label_pred = self.crf.viterbi_tags(pred, mask)\n",
        "        \n",
        "        # Extract tag sequences for each example in the batch\n",
        "        tag_seqs = [result[0] for result in label_pred]\n",
        "        # Convert the predicted tag sequences into a tensor and pad if necessary\n",
        "        tag_seqs = [torch.LongTensor(seq).to(device) for seq in tag_seqs]\n",
        "        tag_seqs_pad = pad_sequence(tag_seqs, batch_first=True, padding_value=PAD_ID)\n",
        "\n",
        "        # Loss: negative of CRF log-likelihood\n",
        "        loss = -self.crf(pred, label_pad, mask)\n",
        "\n",
        "        return loss, tag_seqs_pad"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "pycharm": {
          "is_executing": false
        },
        "id": "8hVywO4VH2k2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Trainer:\n",
        "    def __init__(self, vocab, word_emb_dim, char_emb_dim, transformed_dim,\n",
        "                 hidden_dim, num_layers, dropout, init_lr, lr_decay):\n",
        "        self.vocab = vocab\n",
        "        self.model = Tagger(vocab, word_emb_dim, char_emb_dim, transformed_dim, \n",
        "                            hidden_dim, num_layers, dropout)\n",
        "        self.parameters = [p for p in self.model.parameters() if p.requires_grad]\n",
        "\n",
        "        self.model.to(device)\n",
        "\n",
        "        self.init_lr = init_lr\n",
        "        self.lr_decay = lr_decay\n",
        "        \n",
        "        self.optimizer = torch.optim.Adam(self.parameters, lr=0.0015)\n",
        "        \n",
        "    def update(self, batch, eval=False):\n",
        "        sent_pad, label_pad, elmo_pad, sent_lens = batch\n",
        "\n",
        "        if eval:\n",
        "            self.model.eval()\n",
        "        else:\n",
        "            self.model.train()\n",
        "            self.optimizer.zero_grad()\n",
        "\n",
        "        loss, _ = self.model(sent_pad, label_pad, elmo_pad, sent_lens)\n",
        "        loss_val = loss.data.item()\n",
        "        if eval:\n",
        "            return loss_val\n",
        "\n",
        "        loss.backward()\n",
        "        self.optimizer.step()\n",
        "\n",
        "        return loss_val\n",
        "\n",
        "    def predict(self, batch):\n",
        "        sent_pad, label_pad, elmo_pad, sent_lens = batch\n",
        "\n",
        "        self.model.eval()\n",
        "        batch_size = sent_pad.size(0)\n",
        "        _, pred = self.model(sent_pad, label_pad, elmo_pad, sent_lens)\n",
        "        \n",
        "        # Transform the indices to the NER-labels\n",
        "        pred = [self.vocab['label'].unmap(sent) for sent in pred.tolist()]\n",
        "        # Trim the predictions to their original lengths\n",
        "        pred_tokens = [[pred[i][j] for j in range(sent_lens[i])] for i in range(batch_size)]\n",
        "        gold_labels = [vocab['label'].unmap(label) for label in [label for label in label_pad]]\n",
        "        gold_tokens = [[gold_labels[i][j] for j in range(sent_lens[i])] for i in range(batch_size)]\n",
        "\n",
        "        return pred_tokens, gold_tokens\n",
        "      \n",
        "    def evaluate(self, iterator, test=False):    \n",
        "        epoch_loss = 0\n",
        "\n",
        "        preds = []\n",
        "        golds = []\n",
        "        words = []\n",
        "        \n",
        "        self.model.eval()     \n",
        "        with torch.no_grad():     \n",
        "            for batch in iterator:\n",
        "                batch_size = batch[0].size(0)\n",
        "                pred, gold = self.predict(batch)\n",
        "                # Extend because predict outputs lists of lists already\n",
        "                preds.extend(pred)\n",
        "                golds.extend(gold)\n",
        "\n",
        "                # Keep the original sentence\n",
        "                pred_sents = [[batch[0][i][j] for j in range(batch[3][i])] for i in range(batch_size)]\n",
        "                words.extend([vocab['word'].unmap(sent) for sent in [sent for sent in pred_sents]])\n",
        "                \n",
        "                loss = self.update(batch, eval=True) \n",
        "                epoch_loss += loss\n",
        "\n",
        "            #golds_converted = self.bilou_to_bioes(golds)\n",
        "            #preds_converted = self.bilou_to_bioes(preds)\n",
        "\n",
        "            #score = f1_score(golds_converted, preds_converted)\n",
        "            score = f1_score(golds, preds)\n",
        "            if test:\n",
        "              #report = classification_report(golds_converted, preds_converted, digits=3)\n",
        "              report = classification_report(golds, preds, digits=3)\n",
        "            else:\n",
        "              report = ''\n",
        "        \n",
        "        loss = epoch_loss / len(iterator)\n",
        "            \n",
        "        return loss, score, words, preds, report\n",
        "\n",
        "    def bilou_to_bioes(self, input):\n",
        "      output = []\n",
        "      for label_list in input:\n",
        "        output_list = []\n",
        "        for label in label_list:\n",
        "          split = label.split('-')\n",
        "          if len(split) > 1:\n",
        "            if split[0] == 'U':\n",
        "              output_list.append(f'S-{split[1]}')\n",
        "              continue\n",
        "            elif split[0] == 'L':\n",
        "              output_list.append(f'E-{split[1]}')\n",
        "              continue\n",
        "          output_list.append(label)\n",
        "        output.append(output_list)\n",
        "      return output"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "pycharm": {
          "is_executing": false
        },
        "id": "-E0gOvTCH2jI",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "PAD = '<PAD>'\n",
        "PAD_ID = 0\n",
        "UNK = '<UNK>'\n",
        "UNK_ID = 1\n",
        "VOCAB_PREFIX = [PAD, UNK]\n",
        "\n",
        "VEC_PATH = Path('drive') / 'My Drive' / 'data' / 'estonian-elmo' / 'estonian'\n",
        "DATA_PATH = Path('drive') / 'My Drive' / 'data' / 'project_estner'\n",
        "\n",
        "BATCH_SIZE = 64\n",
        "shuffle_dataset = True\n",
        "random_seed = 42"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "asDCw8VxUL5Y",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "options_file = VEC_PATH / 'options.json'\n",
        "weights_file = VEC_PATH / 'estonian-elmo-weights.hdf5'\n",
        "\n",
        "elmo = Elmo(options_file, weights_file, num_output_representations=1, \n",
        "            scalar_mix_parameters = [0.1, 0.1, 0.1], dropout=0.5)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "pycharm": {
          "is_executing": false
        },
        "id": "XP61SMJFH2jz",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train_doc = Document(DATA_PATH / 'project_train.txt')\n",
        "#train_doc = Document(DATA_PATH / 'project_train_bilou.txt')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tliD07v4nv_F",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train_dataset = CONLLUDataset(train_doc)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "O6hsAuC8oy9v",
        "colab_type": "code",
        "outputId": "e845ee34-17fb-4137-b576-4f128a78c0bb",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "train_dataset[0]"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[[34, 5773, 2492, 3], [4, 2, 2, 2], ['Tallinna', 'Ãµhusaaste', 'suureneb', '.']]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 72
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "pycharm": {
          "is_executing": false
        },
        "id": "JuxcdFuEH2kE",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "vocab = train_dataset.vocab\n",
        "dev_doc = Document(DATA_PATH / 'project_dev.txt')\n",
        "#dev_doc = Document(DATA_PATH / 'project_dev_bilou.txt')\n",
        "dev_dataset = CONLLUDataset(dev_doc, vocab=vocab, test=True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "pycharm": {
          "is_executing": false
        },
        "id": "wqK5EfFRH2kJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#train_subset = train_dataset[:100]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "pycharm": {
          "is_executing": false
        },
        "id": "kc8JtlR3H2kR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=shuffle_dataset, collate_fn=pad_collate)\n",
        "#small_train_loader = DataLoader(train_subset, batch_size=BATCH_SIZE, shuffle=shuffle_dataset, collate_fn=pad_collate)\n",
        "dev_loader = DataLoader(dev_dataset, batch_size=BATCH_SIZE, shuffle=shuffle_dataset, collate_fn=pad_collate)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "pycharm": {
          "is_executing": false
        },
        "id": "dl2twM6ZH2k5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "word_emb_dim = 1024\n",
        "char_emb_dim = 30\n",
        "transformed_dim = 400\n",
        "hidden_dim = 100 # Since we use a bi-LSTM -> in total 100 x 2\n",
        "num_layers = 1\n",
        "dropout = 0.5"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DALSCOyVq3ub",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "init_lr=0.015\n",
        "lr_decay=0.05"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "pycharm": {
          "is_executing": false
        },
        "id": "UuT7WKzwH2k8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "trainer = Trainer(vocab, word_emb_dim, char_emb_dim, transformed_dim,\n",
        "                  hidden_dim, num_layers, dropout, init_lr, lr_decay)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "N4qjSkQdP7rL",
        "colab_type": "code",
        "outputId": "221a5d0c-7b9b-4ee0-9f0f-8a7cc29fdb4b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "def count_parameters(model):\n",
        "    return sum(p.numel() for p in model.parameters if p.requires_grad)\n",
        "\n",
        "print(f'The model has {count_parameters(trainer):,} trainable parameters')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "The model has 813,108 trainable parameters\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JP8hPvzIQ6nV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def epoch_time(start_time, end_time):\n",
        "    elapsed_time = end_time - start_time\n",
        "    elapsed_mins = int(elapsed_time / 60)\n",
        "    elapsed_secs = int(elapsed_time - (elapsed_mins * 60))\n",
        "    return elapsed_mins, elapsed_secs"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Nu922P1DTZ2G",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Taken from https://github.com/jiesutd/NCRFpp/blob/master/main.py\n",
        "def decay_learning_rate(optimizer, epoch, decay_rate, init_lr):\n",
        "    lr = init_lr/(1+decay_rate*epoch)\n",
        "    print(\" Learning rate is set as:\", lr)\n",
        "    for param_group in optimizer.param_groups:\n",
        "        param_group['lr'] = lr\n",
        "    return optimizer"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VjiqLGH7AzPs",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Code taken from https://github.com/Bjarten/early-stopping-pytorch/blob/master/pytorchtools.py\n",
        "class EarlyStopping:\n",
        "    \"\"\"Early stops the training if validation loss doesn't improve after a given patience.\"\"\"\n",
        "    def __init__(self, patience=7, verbose=False, delta=0):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            patience (int): How long to wait after last time validation loss improved.\n",
        "                            Default: 7\n",
        "            verbose (bool): If True, prints a message for each validation loss improvement. \n",
        "                            Default: False\n",
        "            delta (float): Minimum change in the monitored quantity to qualify as an improvement.\n",
        "                            Default: 0\n",
        "        \"\"\"\n",
        "        self.patience = patience\n",
        "        self.verbose = verbose\n",
        "        self.counter = 0\n",
        "        self.best_score = None\n",
        "        self.early_stop = False\n",
        "        self.val_loss_min = np.Inf\n",
        "        self.delta = delta\n",
        "\n",
        "    def __call__(self, val_loss, model):\n",
        "\n",
        "        score = -val_loss\n",
        "\n",
        "        if self.best_score is None:\n",
        "            self.best_score = score\n",
        "            self.save_checkpoint(val_loss, model)\n",
        "        elif score < self.best_score + self.delta:\n",
        "            self.counter += 1\n",
        "            print(f'Early stopping counter: {self.counter} out of {self.patience}')\n",
        "            if self.counter >= self.patience:\n",
        "                self.early_stop = True\n",
        "        else:\n",
        "            self.best_score = score\n",
        "            self.save_checkpoint(val_loss, model)\n",
        "            self.counter = 0\n",
        "\n",
        "    def save_checkpoint(self, val_loss, model):\n",
        "        '''Saves model when validation loss decrease.'''\n",
        "        if self.verbose:\n",
        "            print(f'Validation loss decreased ({self.val_loss_min:.6f} --> {val_loss:.6f}).  Saving model ...')\n",
        "        torch.save(model.state_dict(), DATA_PATH / 'checkpoint.pt')\n",
        "        self.val_loss_min = val_loss"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "pycharm": {
          "is_executing": false
        },
        "id": "K1JsHQF3H2lC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "global_step = 0\n",
        "dev_score_history = []\n",
        "train_loss_history = []\n",
        "dev_loss_history = []\n",
        "format_str = '{}: step {}/{}, loss = {:.6f} ({:.3f} sec/batch)'\n",
        "last_best_step = 0\n",
        "\n",
        "log_step = 100\n",
        "\n",
        "epochs = 30\n",
        "patience = 7"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "pycharm": {
          "is_executing": false
        },
        "id": "u_7OjQbCH2lF",
        "colab_type": "code",
        "outputId": "9b7966cb-08fb-4137-abd5-9b550060a60c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "train_loss = 0\n",
        "\n",
        "# initialize the early_stopping object\n",
        "early_stopping = EarlyStopping(patience=patience, verbose=True)\n",
        "\n",
        "for idx in range(epochs):\n",
        "    print(f'Epoch: {idx+1}/{epochs}')\n",
        "    epoch_start = time()\n",
        "    # Initialize train loss\n",
        "    train_loss = 0\n",
        "\n",
        "    #trainer.optimizer = decay_learning_rate(trainer.optimizer, idx, trainer.lr_decay, trainer.init_lr)\n",
        "    for batch in train_loader:\n",
        "        start_time = time()\n",
        "        global_step += 1\n",
        "        loss = trainer.update(batch, eval=False)\n",
        "        train_loss += loss\n",
        "\n",
        "        if global_step % log_step == 0:\n",
        "            duration = time() - start_time\n",
        "            print(\"step {}: train_loss = {:.6f}, duration = {:.6f}\".format(global_step, loss, duration))           \n",
        "        \n",
        "    print('Evaluating on dev set...')\n",
        "    dev_loss, dev_score, dev_words, dev_preds, dev_report = trainer.evaluate(dev_loader)\n",
        "          \n",
        "    # Calculate losses and span-based f1-score\n",
        "    dev_score_history.append(dev_score)\n",
        "    train_loss = train_loss / len(train_loader)\n",
        "    train_loss_history.append(train_loss)\n",
        "    dev_loss_history.append(dev_loss)\n",
        "    print(\"step {}: train_loss = {:.6f}, dev_score = {:.6f}\".format(global_step, train_loss, dev_score))\n",
        "\n",
        "    # Show one prediction for a sanity check\n",
        "    print(f'Preds: {list(zip(dev_preds[0], dev_words[0]))}')\n",
        "\n",
        "    epoch_end = time()\n",
        "    epoch_mins, epoch_secs = epoch_time(epoch_start, epoch_end)\n",
        "    print(f'Training epoch took: {epoch_mins}m {epoch_secs}s')\n",
        "\n",
        "    # Early_stopping needs the validation loss to check if it has decreased, \n",
        "    # and if it has, it will make a checkpoint of the current model\n",
        "    early_stopping(dev_loss, trainer.model)\n",
        "    \n",
        "    if early_stopping.early_stop:\n",
        "        print(\"Early stopping\")\n",
        "        break\n",
        "        \n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch: 1/30\n",
            "step 100: train_loss = 65.472588, duration = 0.469213\n",
            "Evaluating on dev set...\n",
            "step 162: train_loss = 66.542174, dev_score = 0.859903\n",
            "Preds: [('O', 'Kui'), ('O', 'ka'), ('O', 'nendega'), ('O', 'Ã¼hele'), ('O', 'poole'), ('O', 'saadakse'), ('O', ','), ('O', 'mis'), ('O', 'on'), ('O', 'Ã¼limalt'), ('O', 'tÃµenÃ¤oline'), ('O', ','), ('O', 'peaks'), ('O', 'olema'), ('O', 'kÃµigil'), ('O', '<UNK>'), ('O', '<UNK>'), ('O', '.')]\n",
            "Training epoch took: 1m 49s\n",
            "Validation loss decreased (inf --> 36.909170).  Saving model ...\n",
            "Epoch: 2/30\n",
            "step 200: train_loss = 141.050110, duration = 1.113894\n",
            "step 300: train_loss = 44.853886, duration = 0.540273\n",
            "Evaluating on dev set...\n",
            "step 324: train_loss = 59.888852, dev_score = 0.863937\n",
            "Preds: [('O', '27.'), ('O', 'mail'), ('O', '1966'), ('O', 'tegi'), ('O', 'tromb'), ('B-LOC', 'LÃµuna-Eestis'), ('O', 'Ã¼hele'), ('O', 'noormehele'), ('O', '<UNK>'), ('O', '<UNK>'), ('O', 'ning'), ('O', '<UNK>'), ('O', '<UNK>'), ('O', 'vÃ¤lja'), ('O', '<UNK>'), ('O', '<UNK>'), ('O', '.')]\n",
            "Training epoch took: 1m 50s\n",
            "Validation loss decreased (36.909170 --> 32.717598).  Saving model ...\n",
            "Epoch: 3/30\n",
            "step 400: train_loss = 59.836452, duration = 0.532678\n",
            "Evaluating on dev set...\n",
            "step 486: train_loss = 53.689216, dev_score = 0.866784\n",
            "Preds: [('O', '<UNK>'), ('O', 'saadeti'), ('O', 'teadlastele'), ('O', '<UNK>'), ('O', ','), ('O', 'kohalikud'), ('O', 'elanikud'), ('O', 'aga'), ('O', '<UNK>'), ('O', 'igaks'), ('O', 'juhuks'), ('O', 'oma'), ('O', '<UNK>'), ('O', ','), ('O', 'kartes'), ('O', ','), ('O', 'et'), ('O', 'punane'), ('O', 'sadu'), ('O', 'vÃµis'), ('O', 'olla'), ('O', '<UNK>'), ('O', '.')]\n",
            "Training epoch took: 1m 49s\n",
            "Early stopping counter: 1 out of 7\n",
            "Epoch: 4/30\n",
            "step 500: train_loss = 64.475464, duration = 0.564021\n",
            "step 600: train_loss = 69.395813, duration = 0.706959\n",
            "Evaluating on dev set...\n",
            "step 648: train_loss = 50.206941, dev_score = 0.871252\n",
            "Preds: [('B-ORG', 'VÃ¤lisministeeriumi'), ('O', '<UNK>'), ('O', 'Ã¶eldi'), ('B-ORG', 'Eesti'), ('I-ORG', 'PÃ¤evalehele'), ('O', ','), ('O', 'et'), ('O', 'suursaadiku'), ('O', '<UNK>'), ('B-LOC', 'Strasbourg'), ('I-LOC', \"'\"), ('O', 'i'), ('O', 'kuulub'), ('O', 'presidendi'), ('O', 'kompetentsi'), ('O', '.')]\n",
            "Training epoch took: 1m 48s\n",
            "Validation loss decreased (32.717598 --> 30.441933).  Saving model ...\n",
            "Epoch: 5/30\n",
            "step 700: train_loss = 63.821720, duration = 0.564096\n",
            "step 800: train_loss = 55.679409, duration = 0.440582\n",
            "Evaluating on dev set...\n",
            "step 810: train_loss = 48.461769, dev_score = 0.877641\n",
            "Preds: [('O', 'PÃ¤Ã¤stjad'), ('O', 'lasid'), ('O', 'tuletÃµrjeauto'), ('O', '<UNK>'), ('O', '<UNK>'), ('O', 'voolata'), ('O', 'kaheksa'), ('O', 'tonni'), ('O', 'vett'), ('O', ','), ('O', 'kirjutas'), ('B-ORG', 'PÃ¤rnu'), ('I-ORG', 'Postimees'), ('O', '.')]\n",
            "Training epoch took: 1m 49s\n",
            "Validation loss decreased (30.441933 --> 29.334484).  Saving model ...\n",
            "Epoch: 6/30\n",
            "step 900: train_loss = 35.831093, duration = 0.504209\n",
            "Evaluating on dev set...\n",
            "step 972: train_loss = 46.916071, dev_score = 0.876773\n",
            "Preds: [('B-PER', 'Kaljurand'), ('O', 'ei'), ('O', 'osanud'), ('O', 'Ã¶elda'), ('O', ','), ('O', 'kas'), ('O', '<UNK>'), ('O', '<UNK>'), ('O', 'on'), ('O', 'pÃµhjendatud'), ('O', '.')]\n",
            "Training epoch took: 1m 49s\n",
            "Early stopping counter: 1 out of 7\n",
            "Epoch: 7/30\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-86-4f2ff588227b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     14\u001b[0m         \u001b[0mstart_time\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m         \u001b[0mglobal_step\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 16\u001b[0;31m         \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0meval\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     17\u001b[0m         \u001b[0mtrain_loss\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-67-0429f8c699fd>\u001b[0m in \u001b[0;36mupdate\u001b[0;34m(self, batch, eval)\u001b[0m\n\u001b[1;32m     23\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 25\u001b[0;31m         \u001b[0mloss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msent_pad\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabel_pad\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0melmo_pad\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msent_lens\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     26\u001b[0m         \u001b[0mloss_val\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0meval\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    548\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    549\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 550\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    551\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    552\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-66-a99eb5745a36>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, sent_pad, label_pad, elmo_pad, sent_lens)\u001b[0m\n\u001b[1;32m     65\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     66\u001b[0m         \u001b[0;31m# Loss: negative of CRF log-likelihood\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 67\u001b[0;31m         \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcrf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpred\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabel_pad\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmask\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     68\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     69\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtag_seqs_pad\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    548\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    549\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 550\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    551\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    552\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/allennlp/modules/conditional_random_field.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, inputs, tags, mask)\u001b[0m\n\u001b[1;32m    317\u001b[0m             \u001b[0mmask\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mones\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mtags\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlong\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    318\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 319\u001b[0;31m         \u001b[0mlog_denominator\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_input_likelihood\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmask\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    320\u001b[0m         \u001b[0mlog_numerator\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_joint_likelihood\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtags\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmask\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    321\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/allennlp/modules/conditional_random_field.py\u001b[0m in \u001b[0;36m_input_likelihood\u001b[0;34m(self, logits, mask)\u001b[0m\n\u001b[1;32m    240\u001b[0m             \u001b[0;31m# of ``inner``. Otherwise (mask == 0) we want to retain the previous alpha.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    241\u001b[0m             alpha = (util.logsumexp(inner, 1) * mask[i].view(batch_size, 1) +\n\u001b[0;32m--> 242\u001b[0;31m                      alpha * (1 - mask[i]).view(batch_size, 1))\n\u001b[0m\u001b[1;32m    243\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    244\u001b[0m         \u001b[0;31m# Every sequence needs to end with a transition to the stop_tag.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gDz5Gjmv33Wr",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "plt.plot(train_loss_history, '-o')\n",
        "plt.plot(dev_loss_history, '-o')\n",
        "plt.legend(['train', 'val'], loc='upper left')\n",
        "plt.xlabel('epoch')\n",
        "plt.ylabel('loss')\n",
        "#plt.savefig(DATA_PATH / 'elmo_adam_losses_lr_0_0015.svg')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IbcO-roYFW4Z",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "plt.plot(dev_score_history, '-o')\n",
        "plt.xlabel('epoch')\n",
        "plt.ylabel('f1-score')\n",
        "#plt.savefig(DATA_PATH / 'elmo_adam_dev_fscore_lr_0_0015.svg')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wjMFrGEwmLkv",
        "colab_type": "code",
        "outputId": "d8ce2ba8-65d8-41af-fc85-9c658138d4ea",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "# Load the last checkpoint with the best model\n",
        "trainer.model.load_state_dict(torch.load(DATA_PATH / 'checkpoint.pt'))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<All keys matched successfully>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 89
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jyOiRo3egEew",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Read the test set and evaluate on that\n",
        "test_doc = Document(DATA_PATH / 'project_test_manual_corrs.txt')\n",
        "#test_doc = Document(DATA_PATH / 'project_test_manual_corrs_bilou.txt')\n",
        "test_dataset = CONLLUDataset(test_doc, vocab=vocab, test=True)\n",
        "test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=shuffle_dataset, collate_fn=pad_collate)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bK-jGPQCiXX9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "test_loss, test_score, test_words, test_preds, test_report = trainer.evaluate(test_loader, test=True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Bfu5AYbxi6tf",
        "colab_type": "code",
        "outputId": "b318df45-5ce7-4487-fcfd-9c22afe20e52",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "test_score"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.8966545919454507"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 92
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "E_sQxb1VkDA7",
        "colab_type": "code",
        "outputId": "300ae66d-5936-4fea-b1ef-c63ba814e894",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 153
        }
      },
      "source": [
        "from pprint import pprint\n",
        "pprint(test_report)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "('           precision    recall  f1-score   support\\n'\n",
            " '\\n'\n",
            " '      PER      0.921     0.937     0.929       883\\n'\n",
            " '      LOC      0.903     0.935     0.918       825\\n'\n",
            " '      ORG      0.834     0.808     0.821       626\\n'\n",
            " '\\n'\n",
            " 'micro avg      0.892     0.901     0.897      2334\\n'\n",
            " 'macro avg      0.891     0.901     0.896      2334\\n')\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wr1e6quKjCAV",
        "colab_type": "code",
        "outputId": "670328a3-34a3-4596-9681-02b0c156df64",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "# Show one test prediction\n",
        "print(f'Preds: {list(zip(test_preds[0], test_words[0]))}')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Preds: [('B-LOC', 'Austraalia'), ('O', '<UNK>'), ('O', '.')]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5Fjsv-ITH2lJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Test whether the model can overfit on small subset of data\n",
        "\n",
        "trainer = Trainer(vocab, word_emb_dim, char_emb_dim, transformed_dim,\n",
        "                  hidden_dim, num_layers, dropout, init_lr, lr_decay)\n",
        "\n",
        "epochs = 5\n",
        "eval_interval = 100\n",
        "global_step = 0\n",
        "train_loss = 0\n",
        "for idx in range(epochs):\n",
        "    print(f'Epoch: {idx+1}/{epochs}')\n",
        "    for batch in small_train_loader:\n",
        "        start_time = time()\n",
        "        global_step += 1\n",
        "        loss = trainer.update(batch, eval=False)\n",
        "        train_loss += loss\n",
        "\n",
        "        if global_step % log_step == 0:\n",
        "            duration = time() - start_time\n",
        "            print(format_str.format(datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\"), \n",
        "                                    global_step, loss, duration))\n",
        "            \n",
        "\n",
        "    print('Evaluating on train set...')\n",
        "    train_loss, train_score, train_words, train_preds = trainer.evaluate(small_train_loader)\n",
        "    \n",
        "    print('Evaluating on dev set...')\n",
        "    dev_loss, dev_score, dev_words, dev_preds = trainer.evaluate(dev_loader)\n",
        "\n",
        "    print(\"step {}: train_score = {:.6f}, dev_score = {:.6f}\".format(global_step, train_score, dev_score))\n",
        "    # Shows one prediction for a sanity check\n",
        "    print(f'Preds: {list(zip(dev_preds[0], dev_words[0]))}')\n",
        "    train_loss = 0"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}