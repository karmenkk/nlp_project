{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "project_model_regular_colab.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.6"
    },
    "pycharm": {
      "stem_cell": {
        "cell_type": "raw",
        "metadata": {
          "collapsed": false
        },
        "source": []
      }
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "jNQxJylqH94_",
        "outputId": "80acce02-7edd-45bc-c5b6-1a62cab0353e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive', force_remount=True)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "5_ew5nQJH2iy",
        "outputId": "ff85875e-c1fd-4664-e97f-cfc95549b4f2",
        "pycharm": {
          "is_executing": false
        },
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "! pip install allennlp\n",
        "! pip install seqeval\n",
        "\n",
        "from allennlp.modules.conditional_random_field import ConditionalRandomField, allowed_transitions\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torch.nn.utils.rnn import pack_sequence, pad_sequence, pack_padded_sequence, pad_packed_sequence, PackedSequence\n",
        "from torch.utils.data.sampler import SubsetRandomSampler\n",
        "\n",
        "import numpy as np\n",
        "from time import time\n",
        "from datetime import datetime\n",
        "\n",
        "from pathlib import Path\n",
        "from collections import Counter\n",
        "\n",
        "from typing import List, Dict\n",
        "\n",
        "from seqeval.metrics import f1_score, classification_report"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: allennlp in /usr/local/lib/python3.6/dist-packages (0.9.0)\n",
            "Requirement already satisfied: numpydoc>=0.8.0 in /usr/local/lib/python3.6/dist-packages (from allennlp) (1.0.0)\n",
            "Requirement already satisfied: requests>=2.18 in /usr/local/lib/python3.6/dist-packages (from allennlp) (2.23.0)\n",
            "Requirement already satisfied: parsimonious>=0.8.0 in /usr/local/lib/python3.6/dist-packages (from allennlp) (0.8.1)\n",
            "Requirement already satisfied: tensorboardX>=1.2 in /usr/local/lib/python3.6/dist-packages (from allennlp) (2.0)\n",
            "Requirement already satisfied: conllu==1.3.1 in /usr/local/lib/python3.6/dist-packages (from allennlp) (1.3.1)\n",
            "Requirement already satisfied: flask>=1.0.2 in /usr/local/lib/python3.6/dist-packages (from allennlp) (1.1.2)\n",
            "Requirement already satisfied: boto3 in /usr/local/lib/python3.6/dist-packages (from allennlp) (1.13.13)\n",
            "Requirement already satisfied: flaky in /usr/local/lib/python3.6/dist-packages (from allennlp) (3.6.1)\n",
            "Requirement already satisfied: jsonnet>=0.10.0; sys_platform != \"win32\" in /usr/local/lib/python3.6/dist-packages (from allennlp) (0.16.0)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.6/dist-packages (from allennlp) (1.4.1)\n",
            "Requirement already satisfied: jsonpickle in /usr/local/lib/python3.6/dist-packages (from allennlp) (1.4.1)\n",
            "Requirement already satisfied: ftfy in /usr/local/lib/python3.6/dist-packages (from allennlp) (5.7)\n",
            "Requirement already satisfied: tqdm>=4.19 in /usr/local/lib/python3.6/dist-packages (from allennlp) (4.41.1)\n",
            "Requirement already satisfied: editdistance in /usr/local/lib/python3.6/dist-packages (from allennlp) (0.5.3)\n",
            "Requirement already satisfied: word2number>=1.1 in /usr/local/lib/python3.6/dist-packages (from allennlp) (1.1)\n",
            "Requirement already satisfied: pytest in /usr/local/lib/python3.6/dist-packages (from allennlp) (3.6.4)\n",
            "Requirement already satisfied: overrides in /usr/local/lib/python3.6/dist-packages (from allennlp) (3.0.0)\n",
            "Requirement already satisfied: pytz>=2017.3 in /usr/local/lib/python3.6/dist-packages (from allennlp) (2018.9)\n",
            "Requirement already satisfied: h5py in /usr/local/lib/python3.6/dist-packages (from allennlp) (2.10.0)\n",
            "Requirement already satisfied: unidecode in /usr/local/lib/python3.6/dist-packages (from allennlp) (1.1.1)\n",
            "Requirement already satisfied: spacy<2.2,>=2.1.0 in /usr/local/lib/python3.6/dist-packages (from allennlp) (2.1.9)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.6/dist-packages (from allennlp) (0.22.2.post1)\n",
            "Requirement already satisfied: flask-cors>=3.0.7 in /usr/local/lib/python3.6/dist-packages (from allennlp) (3.0.8)\n",
            "Requirement already satisfied: sqlparse>=0.2.4 in /usr/local/lib/python3.6/dist-packages (from allennlp) (0.3.1)\n",
            "Requirement already satisfied: responses>=0.7 in /usr/local/lib/python3.6/dist-packages (from allennlp) (0.10.14)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from allennlp) (1.18.4)\n",
            "Requirement already satisfied: pytorch-pretrained-bert>=0.6.0 in /usr/local/lib/python3.6/dist-packages (from allennlp) (0.6.2)\n",
            "Requirement already satisfied: matplotlib>=2.2.3 in /usr/local/lib/python3.6/dist-packages (from allennlp) (3.2.1)\n",
            "Requirement already satisfied: torch>=1.2.0 in /usr/local/lib/python3.6/dist-packages (from allennlp) (1.5.0+cu101)\n",
            "Requirement already satisfied: pytorch-transformers==1.1.0 in /usr/local/lib/python3.6/dist-packages (from allennlp) (1.1.0)\n",
            "Requirement already satisfied: gevent>=1.3.6 in /usr/local/lib/python3.6/dist-packages (from allennlp) (20.5.1)\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.6/dist-packages (from allennlp) (3.2.5)\n",
            "Requirement already satisfied: Jinja2>=2.3 in /usr/local/lib/python3.6/dist-packages (from numpydoc>=0.8.0->allennlp) (2.11.2)\n",
            "Requirement already satisfied: sphinx>=1.6.5 in /usr/local/lib/python3.6/dist-packages (from numpydoc>=0.8.0->allennlp) (1.8.5)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests>=2.18->allennlp) (2.9)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests>=2.18->allennlp) (1.24.3)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests>=2.18->allennlp) (3.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests>=2.18->allennlp) (2020.4.5.1)\n",
            "Requirement already satisfied: six>=1.9.0 in /usr/local/lib/python3.6/dist-packages (from parsimonious>=0.8.0->allennlp) (1.12.0)\n",
            "Requirement already satisfied: protobuf>=3.8.0 in /usr/local/lib/python3.6/dist-packages (from tensorboardX>=1.2->allennlp) (3.10.0)\n",
            "Requirement already satisfied: itsdangerous>=0.24 in /usr/local/lib/python3.6/dist-packages (from flask>=1.0.2->allennlp) (1.1.0)\n",
            "Requirement already satisfied: click>=5.1 in /usr/local/lib/python3.6/dist-packages (from flask>=1.0.2->allennlp) (7.1.2)\n",
            "Requirement already satisfied: Werkzeug>=0.15 in /usr/local/lib/python3.6/dist-packages (from flask>=1.0.2->allennlp) (1.0.1)\n",
            "Requirement already satisfied: s3transfer<0.4.0,>=0.3.0 in /usr/local/lib/python3.6/dist-packages (from boto3->allennlp) (0.3.3)\n",
            "Requirement already satisfied: jmespath<1.0.0,>=0.7.1 in /usr/local/lib/python3.6/dist-packages (from boto3->allennlp) (0.10.0)\n",
            "Requirement already satisfied: botocore<1.17.0,>=1.16.13 in /usr/local/lib/python3.6/dist-packages (from boto3->allennlp) (1.16.13)\n",
            "Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.6/dist-packages (from jsonpickle->allennlp) (1.6.0)\n",
            "Requirement already satisfied: wcwidth in /usr/local/lib/python3.6/dist-packages (from ftfy->allennlp) (0.1.9)\n",
            "Requirement already satisfied: more-itertools>=4.0.0 in /usr/local/lib/python3.6/dist-packages (from pytest->allennlp) (8.3.0)\n",
            "Requirement already satisfied: attrs>=17.4.0 in /usr/local/lib/python3.6/dist-packages (from pytest->allennlp) (19.3.0)\n",
            "Requirement already satisfied: atomicwrites>=1.0 in /usr/local/lib/python3.6/dist-packages (from pytest->allennlp) (1.4.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.6/dist-packages (from pytest->allennlp) (46.3.0)\n",
            "Requirement already satisfied: py>=1.5.0 in /usr/local/lib/python3.6/dist-packages (from pytest->allennlp) (1.8.1)\n",
            "Requirement already satisfied: pluggy<0.8,>=0.5 in /usr/local/lib/python3.6/dist-packages (from pytest->allennlp) (0.7.1)\n",
            "Requirement already satisfied: wasabi<1.1.0,>=0.2.0 in /usr/local/lib/python3.6/dist-packages (from spacy<2.2,>=2.1.0->allennlp) (0.6.0)\n",
            "Requirement already satisfied: thinc<7.1.0,>=7.0.8 in /usr/local/lib/python3.6/dist-packages (from spacy<2.2,>=2.1.0->allennlp) (7.0.8)\n",
            "Requirement already satisfied: blis<0.3.0,>=0.2.2 in /usr/local/lib/python3.6/dist-packages (from spacy<2.2,>=2.1.0->allennlp) (0.2.4)\n",
            "Requirement already satisfied: preshed<2.1.0,>=2.0.1 in /usr/local/lib/python3.6/dist-packages (from spacy<2.2,>=2.1.0->allennlp) (2.0.1)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.6/dist-packages (from spacy<2.2,>=2.1.0->allennlp) (1.0.2)\n",
            "Requirement already satisfied: srsly<1.1.0,>=0.0.6 in /usr/local/lib/python3.6/dist-packages (from spacy<2.2,>=2.1.0->allennlp) (1.0.2)\n",
            "Requirement already satisfied: plac<1.0.0,>=0.9.6 in /usr/local/lib/python3.6/dist-packages (from spacy<2.2,>=2.1.0->allennlp) (0.9.6)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.6/dist-packages (from spacy<2.2,>=2.1.0->allennlp) (2.0.3)\n",
            "Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.6/dist-packages (from scikit-learn->allennlp) (0.15.1)\n",
            "Requirement already satisfied: regex in /usr/local/lib/python3.6/dist-packages (from pytorch-pretrained-bert>=0.6.0->allennlp) (2019.12.20)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib>=2.2.3->allennlp) (1.2.0)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.6/dist-packages (from matplotlib>=2.2.3->allennlp) (0.10.0)\n",
            "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib>=2.2.3->allennlp) (2.4.7)\n",
            "Requirement already satisfied: python-dateutil>=2.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib>=2.2.3->allennlp) (2.8.1)\n",
            "Requirement already satisfied: future in /usr/local/lib/python3.6/dist-packages (from torch>=1.2.0->allennlp) (0.16.0)\n",
            "Requirement already satisfied: sentencepiece in /usr/local/lib/python3.6/dist-packages (from pytorch-transformers==1.1.0->allennlp) (0.1.91)\n",
            "Requirement already satisfied: zope.event in /usr/local/lib/python3.6/dist-packages (from gevent>=1.3.6->allennlp) (4.4)\n",
            "Requirement already satisfied: greenlet>=0.4.14; platform_python_implementation == \"CPython\" in /usr/local/lib/python3.6/dist-packages (from gevent>=1.3.6->allennlp) (0.4.15)\n",
            "Requirement already satisfied: zope.interface in /usr/local/lib/python3.6/dist-packages (from gevent>=1.3.6->allennlp) (5.1.0)\n",
            "Requirement already satisfied: MarkupSafe>=0.23 in /usr/local/lib/python3.6/dist-packages (from Jinja2>=2.3->numpydoc>=0.8.0->allennlp) (1.1.1)\n",
            "Requirement already satisfied: babel!=2.0,>=1.3 in /usr/local/lib/python3.6/dist-packages (from sphinx>=1.6.5->numpydoc>=0.8.0->allennlp) (2.8.0)\n",
            "Requirement already satisfied: sphinxcontrib-websupport in /usr/local/lib/python3.6/dist-packages (from sphinx>=1.6.5->numpydoc>=0.8.0->allennlp) (1.2.2)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.6/dist-packages (from sphinx>=1.6.5->numpydoc>=0.8.0->allennlp) (20.4)\n",
            "Requirement already satisfied: snowballstemmer>=1.1 in /usr/local/lib/python3.6/dist-packages (from sphinx>=1.6.5->numpydoc>=0.8.0->allennlp) (2.0.0)\n",
            "Requirement already satisfied: alabaster<0.8,>=0.7 in /usr/local/lib/python3.6/dist-packages (from sphinx>=1.6.5->numpydoc>=0.8.0->allennlp) (0.7.12)\n",
            "Requirement already satisfied: docutils>=0.11 in /usr/local/lib/python3.6/dist-packages (from sphinx>=1.6.5->numpydoc>=0.8.0->allennlp) (0.15.2)\n",
            "Requirement already satisfied: Pygments>=2.0 in /usr/local/lib/python3.6/dist-packages (from sphinx>=1.6.5->numpydoc>=0.8.0->allennlp) (2.1.3)\n",
            "Requirement already satisfied: imagesize in /usr/local/lib/python3.6/dist-packages (from sphinx>=1.6.5->numpydoc>=0.8.0->allennlp) (1.2.0)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.6/dist-packages (from importlib-metadata->jsonpickle->allennlp) (3.1.0)\n",
            "Requirement already satisfied: seqeval in /usr/local/lib/python3.6/dist-packages (0.0.12)\n",
            "Requirement already satisfied: numpy>=1.14.0 in /usr/local/lib/python3.6/dist-packages (from seqeval) (1.18.4)\n",
            "Requirement already satisfied: Keras>=2.2.4 in /usr/local/lib/python3.6/dist-packages (from seqeval) (2.3.1)\n",
            "Requirement already satisfied: keras-preprocessing>=1.0.5 in /usr/local/lib/python3.6/dist-packages (from Keras>=2.2.4->seqeval) (1.1.2)\n",
            "Requirement already satisfied: h5py in /usr/local/lib/python3.6/dist-packages (from Keras>=2.2.4->seqeval) (2.10.0)\n",
            "Requirement already satisfied: keras-applications>=1.0.6 in /usr/local/lib/python3.6/dist-packages (from Keras>=2.2.4->seqeval) (1.0.8)\n",
            "Requirement already satisfied: scipy>=0.14 in /usr/local/lib/python3.6/dist-packages (from Keras>=2.2.4->seqeval) (1.4.1)\n",
            "Requirement already satisfied: six>=1.9.0 in /usr/local/lib/python3.6/dist-packages (from Keras>=2.2.4->seqeval) (1.12.0)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.6/dist-packages (from Keras>=2.2.4->seqeval) (3.13)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xk3269f_9Pwg",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "%config InlineBackend.figure_format = 'svg'"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "i-yyNic_2yQ1",
        "colab": {}
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "plt.rcParams['figure.figsize'] = (10.0, 8.0) # default size of plots"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "rn5eohf1H2i4",
        "pycharm": {
          "is_executing": false
        },
        "colab": {}
      },
      "source": [
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "635ylzEAH2jO",
        "pycharm": {
          "is_executing": false
        },
        "colab": {}
      },
      "source": [
        "# Code based on https://github.com/501Good/tartu-nlp-2020/blob/master/labs/lab5/Lab5_SequenceTagging.ipynb\n",
        "\n",
        "class BaseVocab:\n",
        "    def __init__(self, data, idx=0, lower=False):\n",
        "        self.data = data\n",
        "        self.lower = lower\n",
        "        self.idx = idx\n",
        "        self.build_vocab()\n",
        "        \n",
        "    def normalize_unit(self, unit):\n",
        "        if self.lower:\n",
        "            return unit.lower()\n",
        "        else:\n",
        "            return unit\n",
        "        \n",
        "    def unit2id(self, unit):\n",
        "        unit = self.normalize_unit(unit)\n",
        "        if unit in self._unit2id:\n",
        "            return self._unit2id[unit]\n",
        "        else:\n",
        "            return self._unit2id[UNK]\n",
        "    \n",
        "    def id2unit(self, id):\n",
        "        return self._id2unit[id]\n",
        "    \n",
        "    def map(self, units):\n",
        "        return [self.unit2id(unit) for unit in units]\n",
        "\n",
        "    def unmap(self, ids):\n",
        "        return [self.id2unit(idx) for idx in ids]\n",
        "        \n",
        "    def build_vocab(self):\n",
        "        NotImplementedError()\n",
        "        \n",
        "    def __len__(self):\n",
        "        return len(self._unit2id)\n",
        "    "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "9OQY7hwaH2jS",
        "pycharm": {
          "is_executing": false
        },
        "colab": {}
      },
      "source": [
        "class PretrainedWordVocab(BaseVocab):\n",
        "    def build_vocab(self):\n",
        "        self._id2unit = VOCAB_PREFIX + self.data\n",
        "        self._unit2id = {w:i for i, w in enumerate(self._id2unit)}"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "0Y6FhE3MH2jY",
        "pycharm": {
          "is_executing": false
        },
        "colab": {}
      },
      "source": [
        "class WordVocab(BaseVocab):\n",
        "    def build_vocab(self):\n",
        "        if self.lower:\n",
        "            counter = Counter([w[self.idx].lower() for sent in self.data for w in sent])\n",
        "        else:\n",
        "            counter = Counter([w[self.idx] for sent in self.data for w in sent])\n",
        "\n",
        "        self._id2unit = VOCAB_PREFIX + list(sorted(list(counter.keys()), key=lambda k: counter[k], reverse=True))\n",
        "        self._unit2id = {w:i for i, w in enumerate(self._id2unit)}\n",
        "        \n",
        "    @property\n",
        "    def items(self):\n",
        "        return {i:w for i, w in enumerate(self._id2unit)}"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "xY_BClLZH2jc",
        "pycharm": {
          "is_executing": false
        },
        "colab": {}
      },
      "source": [
        "class CharVocab(BaseVocab):\n",
        "    def build_vocab(self):\n",
        "        counter = Counter([c for sent in self.data for w in sent for c in w[self.idx]])\n",
        "        self._id2unit = VOCAB_PREFIX + list(sorted(list(counter.keys()), key=lambda k: (counter[k], k), reverse=True))\n",
        "        self._unit2id = {w:i for i, w in enumerate(self._id2unit)}"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "3rxYKc46H2jg",
        "pycharm": {
          "is_executing": false
        },
        "colab": {}
      },
      "source": [
        "class Pretrain:\n",
        "    def __init__(self, vec_filename):\n",
        "        self._vec_filename = vec_filename\n",
        "        \n",
        "    @property\n",
        "    def vocab(self):\n",
        "        if not hasattr(self, '_vocab'):\n",
        "            self._vocab, self._emb = self.read()\n",
        "        return self._vocab\n",
        "    \n",
        "    @property\n",
        "    def emb(self):\n",
        "        if not hasattr(self, '_emb'):\n",
        "            self._vocab, self._emb = self.read()\n",
        "        return self._emb\n",
        "        \n",
        "    def read(self):\n",
        "        if self._vec_filename is None:\n",
        "            raise Exception(\"Vector file is not provided.\")\n",
        "        print(f\"Reading pretrained vectors from {self._vec_filename}...\")\n",
        "        \n",
        "        words, emb, failed = self.read_from_file(self._vec_filename, open_func=open)\n",
        "        \n",
        "        if failed > 0: # recover failure\n",
        "            emb = emb[:-failed]\n",
        "        if len(emb) - len(VOCAB_PREFIX) != len(words):\n",
        "            raise Exception(f'Loaded number of vectors ({len(emb)}) does not match number of words ({len(words)}).')\n",
        "\n",
        "        # Changed lower to False because fasttext embeddings are for cased tokens      \n",
        "        vocab = PretrainedWordVocab(words, lower=False)\n",
        "        \n",
        "        return vocab, emb\n",
        "        \n",
        "    def read_from_file(self, filename, open_func=open):\n",
        "        \"\"\"\n",
        "        Open a vector file using the provided function and read from it.\n",
        "        \"\"\"\n",
        "        first = True\n",
        "        words = []\n",
        "        failed = 0\n",
        "        with open_func(filename, 'rb') as f:\n",
        "            for i, line in enumerate(f):\n",
        "                try:\n",
        "                    line = line.decode()\n",
        "                except UnicodeDecodeError:\n",
        "                    failed += 1\n",
        "                    continue\n",
        "                if first:\n",
        "                    # the first line contains the number of word vectors and the dimensionality\n",
        "                    first = False\n",
        "                    line = line.strip().split(' ')\n",
        "                    rows, cols = [int(x) for x in line]\n",
        "                    emb = np.zeros((rows + len(VOCAB_PREFIX), cols), dtype=np.float32)\n",
        "                    continue\n",
        "\n",
        "                line = line.rstrip().split(' ')\n",
        "                emb[i+len(VOCAB_PREFIX)-1-failed, :] = [float(x) for x in line[-cols:]]\n",
        "                word = ' '.join(line[:-cols])\n",
        "                words.append(word)\n",
        "\n",
        "        print(f'Finished loading embeddings, failed to read {failed} vectors')\n",
        "\n",
        "        return words, emb, failed"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "wm2zO6jXH2jk",
        "pycharm": {
          "is_executing": false
        },
        "colab": {}
      },
      "source": [
        "FIELD_NUM = 2\n",
        "\n",
        "class Word:\n",
        "    def __init__(self, word):\n",
        "        self._text = word[0]\n",
        "        self._label = word[1]\n",
        "\n",
        "    @property\n",
        "    def text(self):\n",
        "        return self._text\n",
        "    \n",
        "    @property\n",
        "    def label(self):\n",
        "        return self._label\n",
        "\n",
        "\n",
        "class Sentence:\n",
        "    def __init__(self, words):\n",
        "        self._words = [Word(w) for w in words]\n",
        "\n",
        "    @property\n",
        "    def words(self):\n",
        "        return self._words\n",
        "\n",
        "    \n",
        "class Document:\n",
        "    def __init__(self, file_path):\n",
        "        self._sentences = []\n",
        "        self.load_conll(open(file_path, encoding='utf-8'))\n",
        "\n",
        "\n",
        "    def load_conll(self, f, ignore_gapping=False):\n",
        "        \"\"\" Load the file or string into the CoNLL-U format data.\n",
        "        Input: file or string reader, where the data is in CoNLL-U format.\n",
        "        Output: a list of list of list for each token in each sentence in the data, where the innermost list represents \n",
        "        all fields of a token.\n",
        "        \"\"\"\n",
        "        doc, sent = [], []\n",
        "        for line in f:\n",
        "            line = line.strip()\n",
        "            if len(line) == 0:\n",
        "                if len(sent) > 0:\n",
        "                    doc.append(Sentence(sent))\n",
        "                    sent = []\n",
        "            else:\n",
        "                if line.startswith('#'): # skip comment line\n",
        "                    continue\n",
        "                array = line.split('\\t')\n",
        "                if ignore_gapping and '.' in array[0]:\n",
        "                    continue\n",
        "                assert len(array) == FIELD_NUM, \\\n",
        "                        f\"Cannot parse CoNLL line: expecting {FIELD_NUM} fields, {len(array)} found.\"\n",
        "                sent += [array]\n",
        "        if len(sent) > 0:\n",
        "            doc.append(Sentence(sent))\n",
        "        self._sentences = doc\n",
        "\n",
        "    \n",
        "    @property\n",
        "    def sentences(self):\n",
        "        return self._sentences\n",
        "\n",
        "\n",
        "    def get(self, fields, as_sentences=False):\n",
        "        assert isinstance(fields, list), \"Must provide field names as a list.\"\n",
        "        assert len(fields) >= 1, \"Must have at least one field.\"\n",
        "\n",
        "        results = []\n",
        "        for sentence in self.sentences:\n",
        "            cursent = []\n",
        "            units = sentence.words\n",
        "            for unit in units:\n",
        "                if len(fields) == 1:\n",
        "                    cursent += [getattr(unit, fields[0])]\n",
        "                else:\n",
        "                    cursent += [[getattr(unit, field) for field in fields]]\n",
        "\n",
        "            # decide whether append the results as a sentence or a whole list\n",
        "            if as_sentences:\n",
        "                results.append(cursent)\n",
        "            else:\n",
        "                results += cursent\n",
        "        return results"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "ronJqgiXH2jp",
        "pycharm": {
          "is_executing": false
        },
        "colab": {}
      },
      "source": [
        "class CONLLUDataset(Dataset):\n",
        "    def __init__(self, doc, pretrain, vocab=None, test=False):\n",
        "        self.pretrain_vocab = pretrain.vocab\n",
        "        self.test = test\n",
        "        data = self.load_doc(doc)\n",
        "\n",
        "        if vocab is None:\n",
        "            self.vocab = self.init_vocab(data)\n",
        "        else:\n",
        "            self.vocab = vocab\n",
        "\n",
        "        self.data = self.preprocess(data, self.vocab, self.pretrain_vocab)\n",
        "\n",
        "    def init_vocab(self, data):\n",
        "        wordvocab = WordVocab(data, idx=0)\n",
        "        charvocab = CharVocab(data, idx=0)\n",
        "        labelvocab = WordVocab(data, idx=1)\n",
        "        vocab = {\n",
        "            'word': wordvocab,\n",
        "            'char': charvocab,\n",
        "            'label': labelvocab}\n",
        "        return vocab\n",
        "\n",
        "    def preprocess(self, data, vocab, pretrain_vocab):\n",
        "        processed = []\n",
        "        for sent in data:\n",
        "            processed_sent = [vocab['word'].map([w[0] for w in sent])]\n",
        "            processed_sent += [[vocab['char'].map([char for char in w[0]]) for w in sent]]\n",
        "            processed_sent += [vocab['label'].map([w[1] for w in sent])]\n",
        "            processed_sent += [pretrain_vocab.map([w[0] for w in sent])]\n",
        "            processed.append(processed_sent)\n",
        "        return processed\n",
        "        \n",
        "    def load_doc(self, doc):\n",
        "        data = doc.get(['text', 'label'], as_sentences=True)\n",
        "        return data\n",
        "            \n",
        "    def __len__(self):\n",
        "        return len(self.data)\n",
        "    \n",
        "    def __getitem__(self, idx):\n",
        "        return self.data[idx]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "Z9yK1Cc3H2kN",
        "pycharm": {
          "is_executing": false
        },
        "colab": {}
      },
      "source": [
        "def pad_collate(batch):\n",
        "    (sents, chars, labels, pretrained) = zip(*batch)\n",
        "\n",
        "    sent_lens = [len(s) for s in sents]\n",
        "    word_lens = [len(c) for w in chars for c in w]\n",
        "\n",
        "    sents = [torch.LongTensor(w).to(device) for w in sents]\n",
        "    chars = [torch.LongTensor(c).to(device) for w in chars for c in w]\n",
        "    labels = [torch.LongTensor(label).to(device) for label in labels]\n",
        "    pretrained = [torch.LongTensor(p).to(device) for p in pretrained]\n",
        "\n",
        "    sent_pad = pad_sequence(sents, batch_first=True, padding_value=PAD_ID)\n",
        "    chars_pad = pad_sequence(chars, batch_first=True, padding_value=PAD_ID)\n",
        "    label_pad = pad_sequence(labels, batch_first=True, padding_value=PAD_ID)\n",
        "    pretrained_pad = pad_sequence(pretrained, batch_first=True, padding_value=PAD_ID)\n",
        "\n",
        "    sent_pad = sent_pad.to(device)\n",
        "    chars_pad = chars_pad.to(device)\n",
        "    label_pad = label_pad.to(device)\n",
        "    pretrained_pad = pretrained_pad.to(device)\n",
        "\n",
        "    return sent_pad, chars_pad, label_pad, pretrained_pad, sent_lens, word_lens"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "LXAXNE9zH2kn",
        "pycharm": {
          "is_executing": false
        },
        "colab": {}
      },
      "source": [
        "# Code based on https://github.com/ahmedbesbes/character-based-cnn/blob/master/src/model.py\n",
        "# and https://github.com/jiesutd/NCRFpp/blob/master/model/charcnn.py\n",
        "class CharCNN(nn.Module):\n",
        "    def __init__(self, vocab, char_emb_dim, char_hidden_dim, dropout):        \n",
        "        super().__init__()       \n",
        "        self.char_emb = nn.Embedding(len(vocab['char']), char_emb_dim, padding_idx=PAD_ID)  \n",
        "        self.char_emb.weight.data.copy_(torch.from_numpy(self.random_embedding(len(vocab['char']), \n",
        "                                                                                    char_emb_dim)))\n",
        "\n",
        "        self.conv = nn.Conv1d(in_channels = char_emb_dim, \n",
        "                              out_channels=char_hidden_dim, \n",
        "                              kernel_size=3, padding=1)\n",
        "        \n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "        \n",
        "    def random_embedding(self, vocab_size, char_emb_dim):\n",
        "        pretrain_emb = np.empty([vocab_size, char_emb_dim])\n",
        "        scale = np.sqrt(3.0 / char_emb_dim)\n",
        "        for index in range(vocab_size):\n",
        "            pretrain_emb[index,:] = np.random.uniform(-scale, scale, \n",
        "                                                      [1, char_emb_dim])\n",
        "        return pretrain_emb\n",
        "        \n",
        "    def forward(self, chars_pad):       \n",
        "        char_emb = self.dropout(self.char_emb(chars_pad))        \n",
        "        char_emb = char_emb.transpose(2,1).contiguous()\n",
        "       \n",
        "        batch_size = char_emb.size(0)\n",
        "        \n",
        "        x = self.conv(char_emb) \n",
        "        \n",
        "        # Take the max over the entire sequence\n",
        "        out = F.max_pool1d(x, x.size(2))\n",
        "        \n",
        "        # Remove the third dimension (of shape 1)\n",
        "        result = out.view(batch_size, -1)          \n",
        "        return result"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "Iv1BtNW-H2ks",
        "pycharm": {
          "is_executing": false
        },
        "colab": {}
      },
      "source": [
        "class Tagger(nn.Module):\n",
        "    def __init__(self, vocab: Dict[str, BaseVocab], word_emb_dim: int,\n",
        "                 char_emb_dim: int, transformed_dim: int, emb_matrix: np.ndarray, \n",
        "                 hidden_dim: int, char_hidden_dim: int, num_layers: int, dropout: int):\n",
        "        super().__init__()\n",
        "\n",
        "        self.vocab = vocab\n",
        "        self.num_layers = num_layers\n",
        "        self.hidden_dim = hidden_dim\n",
        "\n",
        "        input_size = 0\n",
        "        \n",
        "        self.char_model = CharCNN(vocab, char_emb_dim, char_hidden_dim, dropout)\n",
        "        \n",
        "        input_size += char_hidden_dim\n",
        "\n",
        "        # Trainable word embedding layer with random initialization\n",
        "        self.word_emb = nn.Embedding(len(vocab['word']), word_emb_dim, padding_idx=PAD_ID)\n",
        "        self.word_emb.weight.data.copy_(torch.from_numpy(self.random_embedding(len(vocab['word']), \n",
        "                                                                               word_emb_dim)))\n",
        "        input_size += word_emb_dim\n",
        "\n",
        "        self.pretrained_emb = nn.Embedding.from_pretrained(torch.from_numpy(emb_matrix), freeze=True)\n",
        "\n",
        "        input_size += emb_matrix.shape[1]\n",
        "\n",
        "        self.lstm = nn.LSTM(input_size, hidden_dim, num_layers, batch_first=True, bidirectional=True)\n",
        "        \n",
        "        # Hidden-to-label linear layer (2* is because of the bidirectionality)\n",
        "        self.label_clf = nn.Linear(2* hidden_dim, len(vocab['label']))\n",
        "\n",
        "        # Add CRF layer, extracting constraints according to label scheme\n",
        "        #'BIOUL'\n",
        "        self.crf = ConditionalRandomField(len(vocab['label']), constraints=allowed_transitions('BIO', vocab['label'].items))\n",
        "\n",
        "        self.drop = nn.Dropout(dropout)\n",
        "\n",
        "    def random_embedding(self, vocab_size, embedding_dim):\n",
        "        pretrain_emb = np.empty([vocab_size, embedding_dim])\n",
        "        scale = np.sqrt(3.0 / embedding_dim)\n",
        "        for index in range(vocab_size):\n",
        "            pretrain_emb[index,:] = np.random.uniform(-scale, scale, [1, embedding_dim])\n",
        "        return pretrain_emb\n",
        "\n",
        "    def forward(self, sent_pad, chars_pad, label_pad, pretrained_pad, sent_lens, word_lens):\n",
        "        inputs = []\n",
        "\n",
        "        # Trainable word embeddings\n",
        "        word_emb = self.drop(self.word_emb(sent_pad))\n",
        "        inputs += [word_emb]\n",
        "\n",
        "        pretrained_emb = self.drop(self.pretrained_emb(pretrained_pad))\n",
        "\n",
        "        # Pretrained word emb shape after transform: [batch_size, seq_length, transformed_dim]\n",
        "        inputs += [pretrained_emb]\n",
        "        \n",
        "        char_emb = self.char_model(chars_pad)\n",
        "               \n",
        "        # Split the char embedding into sentences and return the padded version\n",
        "        char_emb = pad_sequence(char_emb.split(sent_lens), batch_first=True, padding_value=PAD_ID)\n",
        "        inputs += [char_emb]\n",
        "        # Char emb shape after transform: [batch_size, seq_length, transformed_dim]\n",
        "        \n",
        "        # Concatenate the embeddings\n",
        "        lstm_inputs = torch.cat([x for x in inputs], 2)\n",
        "        lstm_inputs = self.drop(lstm_inputs)\n",
        "        lstm_inputs = pack_padded_sequence(lstm_inputs, sent_lens, batch_first=True, enforce_sorted=False)\n",
        "        # LSTM inputs shape: [seq_length, concatenation of embeddings]\n",
        "        \n",
        "        lstm_outputs, _ = self.lstm(lstm_inputs)\n",
        "        lstm_outputs, _ = pad_packed_sequence(lstm_outputs, batch_first=True)\n",
        "        # LSTM outputs shape: [seq_length, 2*hidden_dim]\n",
        "\n",
        "        pred = self.label_clf(self.drop(lstm_outputs))\n",
        "        \n",
        "        # Replace the softmax probs with Viterbi-generated most likely tags\n",
        "        mask = label_pad != PAD_ID\n",
        "        # Mask shape: [batch_size, seq_length]\n",
        "        label_pred = self.crf.viterbi_tags(pred, mask)\n",
        "        \n",
        "        # Extract tag sequences for each example in the batch\n",
        "        tag_seqs = [result[0] for result in label_pred]\n",
        "        # Convert the predicted tag sequences into a tensor and pad if necessary\n",
        "        tag_seqs = [torch.LongTensor(seq).to(device) for seq in tag_seqs]\n",
        "        tag_seqs_pad = pad_sequence(tag_seqs, batch_first=True, padding_value=PAD_ID)\n",
        "        \n",
        "        # Replace softmax+ce loss with the negative of CRF log-likelihood\n",
        "        # Label_pad shape: [batch_size, seq_length]\n",
        "        loss = -self.crf(pred, label_pad, mask)\n",
        "\n",
        "        return loss, tag_seqs_pad"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "8hVywO4VH2k2",
        "pycharm": {
          "is_executing": false
        },
        "colab": {}
      },
      "source": [
        "class Trainer:\n",
        "    def __init__(self, vocab, word_emb_dim, char_emb_dim, transformed_dim,\n",
        "                 emb_matrix, hidden_dim, char_hidden_dim, num_layers, dropout, \n",
        "                 init_lr, lr_decay):\n",
        "        self.vocab = vocab\n",
        "        self.model = Tagger(vocab, word_emb_dim, char_emb_dim, transformed_dim, \n",
        "                            emb_matrix, hidden_dim, char_hidden_dim, num_layers, dropout)\n",
        "        self.parameters = [p for p in self.model.parameters() if p.requires_grad]\n",
        "\n",
        "        self.model.to(device)\n",
        "\n",
        "        self.init_lr = init_lr\n",
        "        self.lr_decay = lr_decay\n",
        "        \n",
        "        self.optimizer = torch.optim.Adam(self.parameters)\n",
        "        \n",
        "    def update(self, batch, eval=False):\n",
        "        sent_pad, chars_pad, label_pad, pretrained_pad, sent_lens, word_lens = batch\n",
        "\n",
        "        if eval:\n",
        "            self.model.eval()\n",
        "        else:\n",
        "            self.model.train()\n",
        "            self.optimizer.zero_grad()\n",
        "\n",
        "        loss, _ = self.model(sent_pad, chars_pad, label_pad, pretrained_pad, sent_lens, word_lens)\n",
        "        loss_val = loss.data.item()\n",
        "        if eval:\n",
        "            return loss_val\n",
        "\n",
        "        loss.backward()\n",
        "        self.optimizer.step()\n",
        "\n",
        "        return loss_val\n",
        "\n",
        "    def predict(self, batch):\n",
        "        sent_pad, chars_pad, label_pad, pretrained_pad, sent_lens, word_lens = batch\n",
        "\n",
        "        self.model.eval()\n",
        "        batch_size = sent_pad.size(0)\n",
        "        _, pred = self.model(sent_pad, chars_pad, label_pad, pretrained_pad, sent_lens, word_lens)\n",
        "        \n",
        "        # Transform the indices to the NER-labels\n",
        "        pred = [self.vocab['label'].unmap(sent) for sent in pred.tolist()]\n",
        "        # Trim the predictions to their original lengths\n",
        "        pred_tokens = [[pred[i][j] for j in range(sent_lens[i])] for i in range(batch_size)]\n",
        "        gold_labels = [vocab['label'].unmap(label) for label in [label for label in label_pad]]\n",
        "        gold_tokens = [[gold_labels[i][j] for j in range(sent_lens[i])] for i in range(batch_size)]\n",
        "\n",
        "        return pred_tokens, gold_tokens\n",
        "      \n",
        "    def evaluate(self, iterator, test=False):    \n",
        "        epoch_loss = 0\n",
        "\n",
        "        preds = []\n",
        "        golds = []\n",
        "        words = []\n",
        "        \n",
        "        self.model.eval()     \n",
        "        with torch.no_grad():     \n",
        "            for batch in iterator:\n",
        "                batch_size = batch[0].size(0)\n",
        "                pred, gold = self.predict(batch)\n",
        "                \n",
        "                # Extend because predict outputs lists of lists already\n",
        "                preds.extend(pred)\n",
        "                golds.extend(gold)\n",
        "\n",
        "                # Keep the original sentence\n",
        "                pred_sents = [[batch[0][i][j] for j in range(batch[4][i])] for i in range(batch_size)]\n",
        "                words.extend([vocab['word'].unmap(sent) for sent in [sent for sent in pred_sents]])\n",
        "                \n",
        "                loss = self.update(batch, eval=True) \n",
        "                epoch_loss += loss\n",
        "\n",
        "            #golds_converted = self.bilou_to_bioes(golds)\n",
        "            #preds_converted = self.bilou_to_bioes(preds)\n",
        "\n",
        "            #score = f1_score(golds_converted, preds_converted)\n",
        "            score = f1_score(golds, preds)\n",
        "            if test:\n",
        "              #report = classification_report(golds_converted, preds_converted, digits=3)\n",
        "              report = classification_report(golds, preds, digits=3)\n",
        "            else:\n",
        "              report = ''\n",
        "        \n",
        "        loss = epoch_loss / len(iterator)\n",
        "            \n",
        "        return loss, score, words, preds, report\n",
        "\n",
        "    def bilou_to_bioes(self, input):\n",
        "      output = []\n",
        "      for label_list in input:\n",
        "        output_list = []\n",
        "        for label in label_list:\n",
        "          split = label.split('-')\n",
        "          if len(split) > 1:\n",
        "            if split[0] == 'U':\n",
        "              output_list.append(f'S-{split[1]}')\n",
        "              continue\n",
        "            elif split[0] == 'L':\n",
        "              output_list.append(f'E-{split[1]}')\n",
        "              continue\n",
        "          output_list.append(label)\n",
        "        output.append(output_list)\n",
        "      return output"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EO6McbCyJi1d",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Code taken from https://github.com/Bjarten/early-stopping-pytorch/blob/master/pytorchtools.py\n",
        "class EarlyStopping:\n",
        "    \"\"\"Early stops the training if validation loss doesn't improve after a given patience.\"\"\"\n",
        "    def __init__(self, patience=7, verbose=False, delta=0):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            patience (int): How long to wait after last time validation loss improved.\n",
        "                            Default: 7\n",
        "            verbose (bool): If True, prints a message for each validation loss improvement. \n",
        "                            Default: False\n",
        "            delta (float): Minimum change in the monitored quantity to qualify as an improvement.\n",
        "                            Default: 0\n",
        "        \"\"\"\n",
        "        self.patience = patience\n",
        "        self.verbose = verbose\n",
        "        self.counter = 0\n",
        "        self.best_score = None\n",
        "        self.early_stop = False\n",
        "        self.val_loss_min = np.Inf\n",
        "        self.delta = delta\n",
        "\n",
        "    def __call__(self, val_loss, model):\n",
        "\n",
        "        score = -val_loss\n",
        "\n",
        "        if self.best_score is None:\n",
        "            self.best_score = score\n",
        "            self.save_checkpoint(val_loss, model)\n",
        "        elif score < self.best_score + self.delta:\n",
        "            self.counter += 1\n",
        "            print(f'Early stopping counter: {self.counter} out of {self.patience}')\n",
        "            if self.counter >= self.patience:\n",
        "                self.early_stop = True\n",
        "        else:\n",
        "            self.best_score = score\n",
        "            self.save_checkpoint(val_loss, model)\n",
        "            self.counter = 0\n",
        "\n",
        "    def save_checkpoint(self, val_loss, model):\n",
        "        '''Saves model when validation loss decrease.'''\n",
        "        if self.verbose:\n",
        "            print(f'Validation loss decreased ({self.val_loss_min:.6f} --> {val_loss:.6f}).  Saving model ...')\n",
        "        torch.save(model.state_dict(), DATA_PATH / 'regular_checkpoint.pt')\n",
        "        self.val_loss_min = val_loss"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "-E0gOvTCH2jI",
        "pycharm": {
          "is_executing": false
        },
        "colab": {}
      },
      "source": [
        "PAD = '<PAD>'\n",
        "PAD_ID = 0\n",
        "UNK = '<UNK>'\n",
        "UNK_ID = 1\n",
        "VOCAB_PREFIX = [PAD, UNK]\n",
        "\n",
        "VEC_PATH = Path('drive') / 'My Drive' / 'data' / 'estonian_fasttext' / 'cc.et.300.vec'\n",
        "DATA_PATH = Path('drive') / 'My Drive' / 'data' / 'project_estner'\n",
        "\n",
        "BATCH_SIZE = 64\n",
        "shuffle_dataset = True\n",
        "random_seed = 42"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "CZee0apHH2ju",
        "pycharm": {
          "is_executing": false
        },
        "colab": {}
      },
      "source": [
        "pretrain = Pretrain(VEC_PATH)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "XP61SMJFH2jz",
        "pycharm": {
          "is_executing": false
        },
        "colab": {}
      },
      "source": [
        "#train_doc = Document(DATA_PATH / 'project_train_bilou.txt')\n",
        "train_doc = Document(DATA_PATH / 'project_train.txt')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "tliD07v4nv_F",
        "outputId": "656812af-56f8-45a4-e002-1a798d0ac76a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        }
      },
      "source": [
        "train_dataset = CONLLUDataset(train_doc, pretrain)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Reading pretrained vectors from drive/My Drive/data/estonian_fasttext/cc.et.300.vec...\n",
            "Finished loading embeddings, failed to read 0 vectors\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "JuxcdFuEH2kE",
        "pycharm": {
          "is_executing": false
        },
        "colab": {}
      },
      "source": [
        "vocab = train_dataset.vocab\n",
        "#dev_doc = Document(DATA_PATH / 'project_dev_bilou.txt')\n",
        "dev_doc = Document(DATA_PATH / 'project_dev.txt')\n",
        "dev_dataset = CONLLUDataset(dev_doc, pretrain, vocab=vocab, test=True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "wqK5EfFRH2kJ",
        "pycharm": {
          "is_executing": false
        },
        "colab": {}
      },
      "source": [
        "train_subset = train_dataset[:100]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "kc8JtlR3H2kR",
        "pycharm": {
          "is_executing": false
        },
        "colab": {}
      },
      "source": [
        "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=shuffle_dataset, collate_fn=pad_collate)\n",
        "small_train_loader = DataLoader(train_subset, batch_size=BATCH_SIZE, shuffle=shuffle_dataset, collate_fn=pad_collate)\n",
        "dev_loader = DataLoader(dev_dataset, batch_size=BATCH_SIZE, shuffle=shuffle_dataset, collate_fn=pad_collate)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "dl2twM6ZH2k5",
        "outputId": "ee632360-fc43-4253-a6aa-6935669e4b75",
        "pycharm": {
          "is_executing": false
        },
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "word_emb_dim = 100 # Yang et al.\n",
        "char_emb_dim = 30 # Yang et al.\n",
        "transformed_dim = 125\n",
        "emb_matrix = pretrain.emb\n",
        "print(f'Shape of emb matrix: {emb_matrix.shape}')\n",
        "hidden_dim = 100 # Since we use a bi-LSTM -> in total 100 x 2\n",
        "char_hidden_dim = 50 # Yang et al.\n",
        "num_layers = 1\n",
        "char_num_layers = 1\n",
        "dropout = 0.5"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Shape of emb matrix: (2000002, 300)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "DALSCOyVq3ub",
        "colab": {}
      },
      "source": [
        "init_lr=0.015\n",
        "lr_decay=0.05"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "UuT7WKzwH2k8",
        "pycharm": {
          "is_executing": false
        },
        "colab": {}
      },
      "source": [
        "trainer = Trainer(vocab, word_emb_dim, char_emb_dim, transformed_dim,\n",
        "                  emb_matrix, hidden_dim, char_hidden_dim,\n",
        "                  num_layers, dropout, init_lr, lr_decay)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "N4qjSkQdP7rL",
        "outputId": "4a7894ff-2b9c-47d8-b35d-3a7b5f0ff1f5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "def count_parameters(model):\n",
        "    return sum(p.numel() for p in model.parameters if p.requires_grad)\n",
        "\n",
        "print(f'The model has {count_parameters(trainer):,} trainable parameters')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "The model has 2,585,380 trainable parameters\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "JP8hPvzIQ6nV",
        "colab": {}
      },
      "source": [
        "def epoch_time(start_time, end_time):\n",
        "    elapsed_time = end_time - start_time\n",
        "    elapsed_mins = int(elapsed_time / 60)\n",
        "    elapsed_secs = int(elapsed_time - (elapsed_mins * 60))\n",
        "    return elapsed_mins, elapsed_secs"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "Nu922P1DTZ2G",
        "colab": {}
      },
      "source": [
        "# Code taken from https://github.com/jiesutd/NCRFpp/blob/master/main.py\n",
        "def decay_learning_rate(optimizer, epoch, decay_rate, init_lr):\n",
        "    lr = init_lr/(1+decay_rate*epoch)\n",
        "    print(\" Learning rate is set as:\", lr)\n",
        "    for param_group in optimizer.param_groups:\n",
        "        param_group['lr'] = lr\n",
        "    return optimizer"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "K1JsHQF3H2lC",
        "pycharm": {
          "is_executing": false
        },
        "colab": {}
      },
      "source": [
        "global_step = 0\n",
        "max_steps = 10000000\n",
        "dev_score_history = []\n",
        "train_loss_history = []\n",
        "dev_loss_history = []\n",
        "format_str = '{}: step {}/{}, loss = {:.6f} ({:.3f} sec/batch)'\n",
        "last_best_step = 0\n",
        "\n",
        "log_step = 100\n",
        "eval_interval = 500\n",
        "\n",
        "epochs = 30\n",
        "patience = 7"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "u_7OjQbCH2lF",
        "outputId": "7dcf15d0-0e9d-48d8-e94d-95ad038c5be5",
        "pycharm": {
          "is_executing": false
        },
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "train_loss = 0\n",
        "\n",
        "# initialize the early_stopping object\n",
        "early_stopping = EarlyStopping(patience=patience, verbose=True)\n",
        "\n",
        "for idx in range(epochs):\n",
        "    print(f'Epoch: {idx+1}/{epochs}')\n",
        "    epoch_start = time()\n",
        "    #trainer.optimizer = decay_learning_rate(trainer.optimizer, idx, trainer.lr_decay, trainer.init_lr)\n",
        "    for batch in train_loader:\n",
        "        start_time = time()\n",
        "        global_step += 1\n",
        "        loss = trainer.update(batch, eval=False)\n",
        "        train_loss += loss\n",
        "\n",
        "        if global_step % log_step == 0:\n",
        "            duration = time() - start_time\n",
        "            print(format_str.format(datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\"), global_step,\\\n",
        "                    max_steps, loss, duration))     \n",
        "        \n",
        "\n",
        "    # Evaluation after each epoch\n",
        "    print('Evaluating on dev set...')\n",
        "    dev_loss, dev_score, dev_words, dev_preds, dev_report = trainer.evaluate(dev_loader)\n",
        "          \n",
        "    # Calculate losses and span-based f1-score\n",
        "    dev_score_history.append(dev_score)\n",
        "    train_loss = train_loss / len(train_loader)\n",
        "    train_loss_history.append(train_loss)\n",
        "    dev_loss_history.append(dev_loss)\n",
        "    print(\"step {}: train_loss = {:.6f}, dev_score = {:.6f}\".format(global_step, train_loss, dev_score))\n",
        "\n",
        "    # Show one prediction for a sanity check\n",
        "    print(f'Preds: {list(zip(dev_preds[0], dev_words[0]))}')\n",
        "    train_loss = 0\n",
        "\n",
        "    epoch_end = time()\n",
        "    epoch_mins, epoch_secs = epoch_time(epoch_start, epoch_end)\n",
        "    print(f'Training epoch took: {epoch_mins}m {epoch_secs}s')\n",
        "    \n",
        "    # Early_stopping needs the validation loss to check if it has decreased, \n",
        "    # and if it has, it will make a checkpoint of the current model\n",
        "    early_stopping(dev_loss, trainer.model)\n",
        "    \n",
        "    if early_stopping.early_stop:\n",
        "        print(\"Early stopping\")\n",
        "        break\n",
        "        "
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch: 1/30\n",
            "2020-05-28 16:45:36: step 100/10000000, loss = 525.253845 (0.154 sec/batch)\n",
            "Evaluating on dev set...\n",
            "step 162: train_loss = 670.939462, dev_score = 0.022907\n",
            "Preds: [('O', ''), ('O', '<UNK>'), ('O', 'avab'), ('O', 'oma'), ('O', 'rahakoti'), ('O', ','), ('O', 'et'), ('O', 'aidata'), ('O', 'Ida-Euroopa'), ('O', '<UNK>'), ('O', '<UNK>'), ('O', '<UNK>'), ('O', '<UNK>'), ('O', 'ja'), ('O', 'htlasi'), ('O', 'nidata'), ('O', ','), ('O', 'kuidas'), ('O', 'kasutada'), ('O', 'hiljem'), ('O', '<UNK>'), ('O', 'Euroopa'), ('O', 'Liidu'), ('O', '<UNK>'), ('O', '<UNK>'), ('O', '.')]\n",
            "Training epoch took: 0m 45s\n",
            "Validation loss decreased (inf --> 357.913012).  Saving model ...\n",
            "Epoch: 2/30\n",
            "2020-05-28 16:47:36: step 200/10000000, loss = 320.449768 (0.173 sec/batch)\n",
            "2020-05-28 16:48:03: step 300/10000000, loss = 319.746704 (0.186 sec/batch)\n",
            "Evaluating on dev set...\n",
            "step 324: train_loss = 377.033743, dev_score = 0.353856\n",
            "Preds: [('O', 'Eks'), ('O', '<UNK>'), ('O', 'ole'), ('O', 'omad'), ('O', '<UNK>'), ('O', '.')]\n",
            "Training epoch took: 0m 49s\n",
            "Validation loss decreased (357.913012 --> 221.173112).  Saving model ...\n",
            "Epoch: 3/30\n",
            "2020-05-28 16:49:02: step 400/10000000, loss = 253.056992 (0.234 sec/batch)\n",
            "Evaluating on dev set...\n",
            "step 486: train_loss = 277.647102, dev_score = 0.562533\n",
            "Preds: [('B-ORG', 'Euroopa'), ('L-ORG', 'Liit'), ('O', 'toetab'), ('U-LOC', 'Eestit'), ('O', '<UNK>'), ('B-PER', 'ARGO'), ('L-PER', 'IDEON'), ('O', 'Tuleval'), ('O', 'aastal'), ('O', '<UNK>'), ('U-LOC', 'Eestile'), ('O', 'mrkimisvrne'), ('O', '<UNK>'), ('U-LOC', 'Euroopa'), ('O', 'Liidult'), ('O', ','), ('O', 'mille'), ('O', '<UNK>'), ('O', '<UNK>'), ('O', 'toetus'), ('O', 'ulatub'), ('O', 'eri'), ('O', '<UNK>'), ('O', 'hest'), ('O', 'kuni'), ('O', 'kahe'), ('O', 'miljardi'), ('O', 'kroonini'), ('O', 'aastas'), ('O', '.')]\n",
            "Training epoch took: 0m 49s\n",
            "Validation loss decreased (221.173112 --> 161.838230).  Saving model ...\n",
            "Epoch: 4/30\n",
            "2020-05-28 16:50:00: step 500/10000000, loss = 227.790939 (0.198 sec/batch)\n",
            "2020-05-28 16:50:27: step 600/10000000, loss = 159.340454 (0.199 sec/batch)\n",
            "Evaluating on dev set...\n",
            "step 648: train_loss = 222.619481, dev_score = 0.616487\n",
            "Preds: [('O', ''), ('O', 'Uue'), ('O', 'meeskonna'), ('O', 'viie'), ('O', 'aasta'), ('O', 't'), ('O', 'thtsaimaks'), ('O', '<UNK>'), ('O', 'on'), ('O', '<UNK>'), ('O', 'ja'), ('O', 'teadust'), ('O', 'hendamine'), ('O', '.')]\n",
            "Training epoch took: 0m 49s\n",
            "Validation loss decreased (161.838230 --> 136.369782).  Saving model ...\n",
            "Epoch: 5/30\n",
            "2020-05-28 16:51:23: step 700/10000000, loss = 181.649963 (0.174 sec/batch)\n",
            "2020-05-28 16:51:50: step 800/10000000, loss = 115.370239 (0.166 sec/batch)\n",
            "Evaluating on dev set...\n",
            "step 810: train_loss = 187.684260, dev_score = 0.659639\n",
            "Preds: [('O', 'Erinevalt'), ('O', 'varasemast'), ('O', 'on'), ('O', 'uus'), ('O', '<UNK>'), ('O', '<UNK>'), ('O', ','), ('O', 'kuhu'), ('O', 'lisaks'), ('O', '<UNK>'), ('O', 'kuuluvad'), ('O', '<UNK>'), ('O', 'ja'), ('O', '<UNK>'), ('O', '.')]\n",
            "Training epoch took: 0m 49s\n",
            "Validation loss decreased (136.369782 --> 113.550328).  Saving model ...\n",
            "Epoch: 6/30\n",
            "2020-05-28 16:52:49: step 900/10000000, loss = 138.597244 (0.220 sec/batch)\n",
            "Evaluating on dev set...\n",
            "step 972: train_loss = 158.276155, dev_score = 0.696122\n",
            "Preds: [('O', '!'), ('O', 'Eelkige'), ('O', 'on'), ('O', 'see'), ('O', 'ksimus'), ('O', 'suunatud'), ('O', '<UNK>'), ('B-PER', 'Ainar'), ('L-PER', '<UNK>'), ('O', 'ja'), ('B-PER', '<UNK>'), ('L-PER', '<UNK>'), ('O', '.')]\n",
            "Training epoch took: 0m 50s\n",
            "Validation loss decreased (113.550328 --> 98.283205).  Saving model ...\n",
            "Epoch: 7/30\n",
            "2020-05-28 16:53:57: step 1000/10000000, loss = 193.487076 (0.192 sec/batch)\n",
            "2020-05-28 16:54:24: step 1100/10000000, loss = 187.386108 (0.201 sec/batch)\n",
            "Evaluating on dev set...\n",
            "step 1134: train_loss = 134.757218, dev_score = 0.713111\n",
            "Preds: [('O', 'Ndalavahetusel'), ('O', 'sai'), ('O', '<UNK>'), ('O', 'ligi'), ('O', '80'), ('O', 'meetri'), ('O', 'pikkuse'), ('O', '<UNK>'), ('O', ','), ('O', 'kuid'), ('O', '<UNK>'), ('O', 'lpliku'), ('O', '<UNK>'), ('O', 'kulub'), ('O', 'veel'), ('O', 'le'), ('O', 'poole'), ('O', 'aasta'), ('O', '.')]\n",
            "Training epoch took: 0m 49s\n",
            "Validation loss decreased (98.283205 --> 87.219114).  Saving model ...\n",
            "Epoch: 8/30\n",
            "2020-05-28 16:55:20: step 1200/10000000, loss = 143.540771 (0.201 sec/batch)\n",
            "Evaluating on dev set...\n",
            "step 1296: train_loss = 117.849676, dev_score = 0.738581\n",
            "Preds: [('O', 'Tema'), ('O', 'snul'), ('O', 'vib'), ('O', 'suurimat'), ('O', 'osa'), ('O', 'ehk'), ('O', 'Phare'), ('O', 'raha'), ('O', 'oodata'), ('O', 'tenoliselt'), ('O', 'alles'), ('O', 'jrgmise'), ('O', 'aasta'), ('O', 'lpuks'), ('O', '.')]\n",
            "Training epoch took: 0m 49s\n",
            "Validation loss decreased (87.219114 --> 80.657724).  Saving model ...\n",
            "Epoch: 9/30\n",
            "2020-05-28 16:56:18: step 1300/10000000, loss = 95.274048 (0.160 sec/batch)\n",
            "2020-05-28 16:56:44: step 1400/10000000, loss = 112.415642 (0.376 sec/batch)\n",
            "Evaluating on dev set...\n",
            "step 1458: train_loss = 104.894137, dev_score = 0.743989\n",
            "Preds: [('O', 'Opositsioon'), ('O', 'algatas'), ('U-PER', 'Ansipi'), ('O', 'umbusaldamise'), ('O', '.')]\n",
            "Training epoch took: 0m 49s\n",
            "Validation loss decreased (80.657724 --> 74.246675).  Saving model ...\n",
            "Epoch: 10/30\n",
            "2020-05-28 16:57:53: step 1500/10000000, loss = 73.068436 (0.168 sec/batch)\n",
            "2020-05-28 16:58:19: step 1600/10000000, loss = 73.423096 (0.162 sec/batch)\n",
            "Evaluating on dev set...\n",
            "step 1620: train_loss = 95.274637, dev_score = 0.773486\n",
            "Preds: [('O', 'Minu'), ('O', 'arvates'), ('O', 'on'), ('O', 'sna'), ('O', '\"'), ('O', '<UNK>'), ('O', '\"'), ('O', 'meie'), ('O', '<UNK>'), ('O', '<UNK>'), ('O', 'aadressil'), ('U-LOC', 'Euroopas'), ('O', 'piisavalt'), ('O', 'karm'), ('O', 'kriitika'), ('O', ','), ('O', '\"'), ('O', 'tles'), ('B-ORG', 'Valge'), ('L-ORG', 'Maja'), ('O', 'esindaja'), ('B-PER', '<UNK>'), ('L-PER', '<UNK>'), ('O', ','), ('O', 'lisades'), ('O', ','), ('O', 'et'), ('O', 'president'), ('B-PER', 'Bill'), ('L-PER', 'Clinton'), ('O', 'vtab'), ('U-LOC', 'Prantsuse'), ('O', 'presidendi'), ('B-PER', 'Jacques'), ('L-PER', 'Chiraci'), ('O', '<UNK>'), ('U-LOC', 'Washingtoni'), ('O', 'visiidi'), ('O', 'ajal'), ('O', 'kindlasti'), ('O', '<UNK>'), ('O', 'ka'), ('O', '<UNK>'), ('O', '.')]\n",
            "Training epoch took: 0m 49s\n",
            "Validation loss decreased (74.246675 --> 67.795408).  Saving model ...\n",
            "Epoch: 11/30\n",
            "2020-05-28 16:59:28: step 1700/10000000, loss = 99.659027 (0.190 sec/batch)\n",
            "Evaluating on dev set...\n",
            "step 1782: train_loss = 85.867424, dev_score = 0.777046\n",
            "Preds: [('O', '<UNK>'), ('O', '<UNK>'), ('O', 'vib'), ('U-PER', '<UNK>'), ('O', 'pseda'), ('O', 'sel'), ('O', 'ndalavahetusel'), ('O', ','), ('O', 'kui'), ('U-LOC', 'Tallinnas'), ('B-LOC', 'Harku'), ('L-LOC', 'jrve'), ('O', 'res'), ('O', 'peetakse'), ('U-LOC', 'Eesti'), ('O', 'meistrivistluste'), ('O', 'II'), ('O', 'etapp'), ('O', '<UNK>'), ('O', ','), ('O', 'mille'), ('O', 'esimesed'), ('O', '<UNK>'), ('O', 'maailmas'), ('O', 'toimusid'), ('U-LOC', 'Prnumaal'), ('O', 'juba'), ('O', 'kaks'), ('O', 'aastat'), ('O', 'tagasi'), ('O', '.')]\n",
            "Training epoch took: 0m 49s\n",
            "Validation loss decreased (67.795408 --> 65.866496).  Saving model ...\n",
            "Epoch: 12/30\n",
            "2020-05-28 17:00:24: step 1800/10000000, loss = 62.523560 (0.193 sec/batch)\n",
            "2020-05-28 17:00:51: step 1900/10000000, loss = 81.801826 (0.211 sec/batch)\n",
            "Evaluating on dev set...\n",
            "step 1944: train_loss = 78.658310, dev_score = 0.787623\n",
            "Preds: [('O', 'Kll'), ('O', 'vib'), ('O', 'taotleda'), ('O', 'ja'), ('O', 'saab'), ('O', 'sel'), ('O', 'juhul'), ('O', 'erastada'), ('O', 'ehitise'), ('O', '<UNK>'), ('O', 'selleks'), ('O', 'vajalikus'), ('O', 'ulatuses'), ('O', '.')]\n",
            "Training epoch took: 0m 49s\n",
            "Validation loss decreased (65.866496 --> 62.205383).  Saving model ...\n",
            "Epoch: 13/30\n",
            "2020-05-28 17:01:48: step 2000/10000000, loss = 57.443787 (0.181 sec/batch)\n",
            "2020-05-28 17:02:15: step 2100/10000000, loss = 76.951477 (0.176 sec/batch)\n",
            "Evaluating on dev set...\n",
            "step 2106: train_loss = 70.968218, dev_score = 0.791509\n",
            "Preds: [('O', '<UNK>'), ('O', 'langevad'), ('O', 'ning'), ('O', '<UNK>'), ('O', 'hoolimata'), ('O', 'ei'), ('O', '<UNK>'), ('O', 'talud'), ('O', 'enam'), ('O', 'tehtud'), ('O', 'kulutusi'), ('O', '.')]\n",
            "Training epoch took: 0m 49s\n",
            "Validation loss decreased (62.205383 --> 59.837179).  Saving model ...\n",
            "Epoch: 14/30\n",
            "2020-05-28 17:03:21: step 2200/10000000, loss = 67.329971 (0.206 sec/batch)\n",
            "Evaluating on dev set...\n",
            "step 2268: train_loss = 65.480135, dev_score = 0.799626\n",
            "Preds: [('O', 'Valitsus'), ('O', 'peab'), ('O', 'piirama'), ('O', 'praegu'), ('O', 'kehtiva'), ('O', '<UNK>'), ('O', 'alusel'), ('O', 'teravilja'), ('O', 'ja'), ('O', 'jahu'), ('O', '<UNK>'), ('O', '.')]\n",
            "Training epoch took: 0m 50s\n",
            "Validation loss decreased (59.837179 --> 57.358425).  Saving model ...\n",
            "Epoch: 15/30\n",
            "2020-05-28 17:04:19: step 2300/10000000, loss = 52.310204 (0.156 sec/batch)\n",
            "2020-05-28 17:04:46: step 2400/10000000, loss = 73.669739 (0.182 sec/batch)\n",
            "Evaluating on dev set...\n",
            "step 2430: train_loss = 59.599212, dev_score = 0.812356\n",
            "Preds: [('O', 'Ameeriklaste'), ('O', 'firma'), ('U-ORG', 'NRG'), ('O', 'esimese'), ('O', 'riplaani'), ('O', 'kohaselt'), ('O', 'sltub'), ('O', '<UNK>'), ('O', '<UNK>'), ('O', 'eluiga'), ('O', 'selle'), ('O', '<UNK>'), ('O', '<UNK>'), ('O', '.')]\n",
            "Training epoch took: 0m 50s\n",
            "Validation loss decreased (57.358425 --> 55.067244).  Saving model ...\n",
            "Epoch: 16/30\n",
            "2020-05-28 17:05:53: step 2500/10000000, loss = 63.698215 (0.232 sec/batch)\n",
            "Evaluating on dev set...\n",
            "step 2592: train_loss = 54.532326, dev_score = 0.809895\n",
            "Preds: [('O', ''), ('O', 'ksis'), ('U-PER', '<UNK>'), ('O', '.')]\n",
            "Training epoch took: 0m 50s\n",
            "Early stopping counter: 1 out of 7\n",
            "Epoch: 17/30\n",
            "2020-05-28 17:06:26: step 2600/10000000, loss = 51.838177 (0.173 sec/batch)\n",
            "2020-05-28 17:06:50: step 2700/10000000, loss = 84.692505 (0.165 sec/batch)\n",
            "Evaluating on dev set...\n",
            "step 2754: train_loss = 50.992623, dev_score = 0.820018\n",
            "Preds: [('O', '<UNK>'), ('O', 'peetakse'), ('O', 'poliitikuid'), ('O', 'hiskonna'), ('O', '<UNK>'), ('O', '<UNK>'), ('O', ','), ('O', 'kuid'), ('O', 'nad'), ('O', 'pole'), ('O', 'seda'), ('O', '.')]\n",
            "Training epoch took: 0m 45s\n",
            "Validation loss decreased (55.067244 --> 52.749636).  Saving model ...\n",
            "Epoch: 18/30\n",
            "2020-05-28 17:07:53: step 2800/10000000, loss = 60.147865 (0.231 sec/batch)\n",
            "2020-05-28 17:08:21: step 2900/10000000, loss = 26.514982 (0.182 sec/batch)\n",
            "Evaluating on dev set...\n",
            "step 2916: train_loss = 48.295022, dev_score = 0.828283\n",
            "Preds: [('O', '<UNK>'), ('B-PER', '<UNK>'), ('I-PER', '<UNK>'), ('L-PER', '<UNK>'), ('O', 'vttis'), ('O', 'enne'), ('O', '<UNK>'), ('U-PER', '<UNK>'), ('O', 'lahkumist'), ('O', '<UNK>'), ('O', 'ktust'), ('O', 'tis'), ('O', ','), ('O', 'et'), ('O', 'jrgmisel'), ('O', 'peval'), ('O', 'edasi'), ('U-LOC', '<UNK>'), ('O', 'lennata'), ('O', ','), ('O', 'teatas'), ('O', 'ajaleht'), ('U-PER', '<UNK>'), ('O', '.')]\n",
            "Training epoch took: 0m 49s\n",
            "Early stopping counter: 1 out of 7\n",
            "Epoch: 19/30\n",
            "2020-05-28 17:08:51: step 3000/10000000, loss = 59.937889 (0.172 sec/batch)\n",
            "Evaluating on dev set...\n",
            "step 3078: train_loss = 42.521463, dev_score = 0.807551\n",
            "Preds: [('O', 'Praegu'), ('O', 'on'), ('O', 'parim'), ('O', ''), ('O', '<UNK>'), ('O', ''), ('O', ','), ('O', '<UNK>'), ('O', 'on'), ('O', ''), ('O', '<UNK>'), ('O', ''), ('O', 'ja'), ('O', ''), ('O', '<UNK>'), ('O', ''), ('O', 'ka'), ('O', 'head'), ('O', ','), ('O', 'tulekul'), ('O', 'on'), ('O', ''), ('O', '<UNK>'), ('O', ''), ('O', '.')]\n",
            "Training epoch took: 0m 45s\n",
            "Early stopping counter: 2 out of 7\n",
            "Epoch: 20/30\n",
            "2020-05-28 17:09:22: step 3100/10000000, loss = 57.340504 (0.180 sec/batch)\n",
            "2020-05-28 17:09:46: step 3200/10000000, loss = 40.531559 (0.173 sec/batch)\n",
            "Evaluating on dev set...\n",
            "step 3240: train_loss = 40.590938, dev_score = 0.824545\n",
            "Preds: [('O', 'Kui'), ('O', '<UNK>'), ('O', 'tekib'), ('O', 'mitmesuguseid'), ('O', '<UNK>'), ('O', '<UNK>'), ('O', ','), ('O', 'siis'), ('O', '<UNK>'), ('O', '<UNK>'), ('O', '<UNK>'), ('O', 'need'), ('O', '<UNK>'), ('O', '<UNK>'), ('O', '.')]\n",
            "Training epoch took: 0m 45s\n",
            "Early stopping counter: 3 out of 7\n",
            "Epoch: 21/30\n",
            "2020-05-28 17:10:16: step 3300/10000000, loss = 36.080788 (0.164 sec/batch)\n",
            "2020-05-28 17:10:40: step 3400/10000000, loss = 35.570908 (0.165 sec/batch)\n",
            "Evaluating on dev set...\n",
            "step 3402: train_loss = 37.232498, dev_score = 0.809502\n",
            "Preds: [('O', ''), ('O', '<UNK>'), ('B-ORG', 'Vabariigi'), ('L-ORG', '<UNK>'), ('O', 'juhataja'), ('B-PER', 'Rein'), ('L-PER', '<UNK>'), ('O', 'tles'), ('O', ','), ('O', 'et'), ('O', '<UNK>'), ('O', '<UNK>'), ('O', 'on'), ('U-LOC', 'Eesti'), ('O', ','), ('O', 'sest'), ('O', 'esimesed'), ('O', 'sellised'), ('O', 'metallist'), ('O', '<UNK>'), ('O', 'tehti'), ('O', 'ksitsi'), ('O', 'just'), ('O', 'siin'), ('O', '.')]\n",
            "Training epoch took: 0m 45s\n",
            "Early stopping counter: 4 out of 7\n",
            "Epoch: 22/30\n",
            "2020-05-28 17:11:10: step 3500/10000000, loss = 22.517208 (0.162 sec/batch)\n",
            "Evaluating on dev set...\n",
            "step 3564: train_loss = 35.169556, dev_score = 0.825092\n",
            "Preds: [('O', ''), ('O', '<UNK>'), ('O', 'kahju'), ('O', 'ei'), ('O', 'saa'), ('O', 'rahaga'), ('O', '<UNK>'), ('O', ','), ('O', 'fsilist'), ('O', 'aga'), ('O', 'kll'), ('O', '.')]\n",
            "Training epoch took: 0m 45s\n",
            "Early stopping counter: 5 out of 7\n",
            "Epoch: 23/30\n",
            "2020-05-28 17:11:41: step 3600/10000000, loss = 22.396044 (0.319 sec/batch)\n",
            "2020-05-28 17:12:05: step 3700/10000000, loss = 36.873508 (0.159 sec/batch)\n",
            "Evaluating on dev set...\n",
            "step 3726: train_loss = 33.356472, dev_score = 0.830189\n",
            "Preds: [('O', '<UNK>'), ('O', 'suudame'), ('O', 'ehk'), ('O', '<UNK>'), ('O', 'osas'), ('O', ','), ('O', 'kuid'), ('O', '<UNK>'), ('O', 'tuleb'), ('O', 'sisse'), ('B-LOC', 'Euroopa'), ('L-LOC', 'Liidu'), ('O', 'tugeva'), ('O', '<UNK>'), ('O', '.')]\n",
            "Training epoch took: 0m 45s\n",
            "Early stopping counter: 6 out of 7\n",
            "Epoch: 24/30\n",
            "2020-05-28 17:12:35: step 3800/10000000, loss = 29.948170 (0.153 sec/batch)\n",
            "Evaluating on dev set...\n",
            "step 3888: train_loss = 31.109237, dev_score = 0.824287\n",
            "Preds: [('O', 'Samas'), ('O', 'stestab'), ('O', 'maareformi'), ('O', 'seadus'), ('O', ','), ('O', 'et'), ('O', 'maa'), ('O', 'tagastamise'), ('O', 'kigus'), ('O', 'vidakse'), ('O', 'lbi'), ('O', 'viia'), ('O', '<UNK>'), ('O', 'ja'), ('O', 'maa'), ('O', 'erastamine'), ('O', 'vib'), ('O', 'toimuda'), ('O', 'ka'), ('O', '<UNK>'), ('O', 'alusel'), ('O', '.')]\n",
            "Training epoch took: 0m 45s\n",
            "Early stopping counter: 7 out of 7\n",
            "Early stopping\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "gDz5Gjmv33Wr",
        "outputId": "aaad3325-dece-4c2c-ca7f-f690449b2e08",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 663
        }
      },
      "source": [
        "plt.plot(train_loss_history, '-o')\n",
        "plt.plot(dev_loss_history, '-o')\n",
        "plt.legend(['train', 'val'], loc='upper left')\n",
        "plt.xlabel('epoch')\n",
        "plt.ylabel('loss')\n",
        "#plt.savefig(DATA_PATH / 'regular_adam_losses.svg')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 720x576 with 1 Axes>"
            ],
            "image/svg+xml": "<?xml version=\"1.0\" encoding=\"utf-8\" standalone=\"no\"?>\n<!DOCTYPE svg PUBLIC \"-//W3C//DTD SVG 1.1//EN\"\n  \"http://www.w3.org/Graphics/SVG/1.1/DTD/svg11.dtd\">\n<!-- Created with matplotlib (https://matplotlib.org/) -->\n<svg height=\"481.624446pt\" version=\"1.1\" viewBox=\"0 0 612.165625 481.624446\" width=\"612.165625pt\" xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">\n <defs>\n  <style type=\"text/css\">\n*{stroke-linecap:butt;stroke-linejoin:round;}\n  </style>\n </defs>\n <g id=\"figure_1\">\n  <g id=\"patch_1\">\n   <path d=\"M 0 481.624446 \nL 612.165625 481.624446 \nL 612.165625 0 \nL 0 0 \nz\n\" style=\"fill:none;\"/>\n  </g>\n  <g id=\"axes_1\">\n   <g id=\"patch_2\">\n    <path d=\"M 46.965625 444.068196 \nL 604.965625 444.068196 \nL 604.965625 9.188196 \nL 46.965625 9.188196 \nz\n\" style=\"fill:#ffffff;\"/>\n   </g>\n   <g id=\"matplotlib.axis_1\">\n    <g id=\"xtick_1\">\n     <g id=\"line2d_1\">\n      <defs>\n       <path d=\"M 0 0 \nL 0 3.5 \n\" id=\"m8c660d4275\" style=\"stroke:#000000;stroke-width:0.8;\"/>\n      </defs>\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"72.329261\" xlink:href=\"#m8c660d4275\" y=\"444.068196\"/>\n      </g>\n     </g>\n     <g id=\"text_1\">\n      <!-- 0 -->\n      <defs>\n       <path d=\"M 31.78125 66.40625 \nQ 24.171875 66.40625 20.328125 58.90625 \nQ 16.5 51.421875 16.5 36.375 \nQ 16.5 21.390625 20.328125 13.890625 \nQ 24.171875 6.390625 31.78125 6.390625 \nQ 39.453125 6.390625 43.28125 13.890625 \nQ 47.125 21.390625 47.125 36.375 \nQ 47.125 51.421875 43.28125 58.90625 \nQ 39.453125 66.40625 31.78125 66.40625 \nz\nM 31.78125 74.21875 \nQ 44.046875 74.21875 50.515625 64.515625 \nQ 56.984375 54.828125 56.984375 36.375 \nQ 56.984375 17.96875 50.515625 8.265625 \nQ 44.046875 -1.421875 31.78125 -1.421875 \nQ 19.53125 -1.421875 13.0625 8.265625 \nQ 6.59375 17.96875 6.59375 36.375 \nQ 6.59375 54.828125 13.0625 64.515625 \nQ 19.53125 74.21875 31.78125 74.21875 \nz\n\" id=\"DejaVuSans-48\"/>\n      </defs>\n      <g transform=\"translate(69.148011 458.666634)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-48\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_2\">\n     <g id=\"line2d_2\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"182.605941\" xlink:href=\"#m8c660d4275\" y=\"444.068196\"/>\n      </g>\n     </g>\n     <g id=\"text_2\">\n      <!-- 5 -->\n      <defs>\n       <path d=\"M 10.796875 72.90625 \nL 49.515625 72.90625 \nL 49.515625 64.59375 \nL 19.828125 64.59375 \nL 19.828125 46.734375 \nQ 21.96875 47.46875 24.109375 47.828125 \nQ 26.265625 48.1875 28.421875 48.1875 \nQ 40.625 48.1875 47.75 41.5 \nQ 54.890625 34.8125 54.890625 23.390625 \nQ 54.890625 11.625 47.5625 5.09375 \nQ 40.234375 -1.421875 26.90625 -1.421875 \nQ 22.3125 -1.421875 17.546875 -0.640625 \nQ 12.796875 0.140625 7.71875 1.703125 \nL 7.71875 11.625 \nQ 12.109375 9.234375 16.796875 8.0625 \nQ 21.484375 6.890625 26.703125 6.890625 \nQ 35.15625 6.890625 40.078125 11.328125 \nQ 45.015625 15.765625 45.015625 23.390625 \nQ 45.015625 31 40.078125 35.4375 \nQ 35.15625 39.890625 26.703125 39.890625 \nQ 22.75 39.890625 18.8125 39.015625 \nQ 14.890625 38.140625 10.796875 36.28125 \nz\n\" id=\"DejaVuSans-53\"/>\n      </defs>\n      <g transform=\"translate(179.424691 458.666634)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-53\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_3\">\n     <g id=\"line2d_3\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"292.882621\" xlink:href=\"#m8c660d4275\" y=\"444.068196\"/>\n      </g>\n     </g>\n     <g id=\"text_3\">\n      <!-- 10 -->\n      <defs>\n       <path d=\"M 12.40625 8.296875 \nL 28.515625 8.296875 \nL 28.515625 63.921875 \nL 10.984375 60.40625 \nL 10.984375 69.390625 \nL 28.421875 72.90625 \nL 38.28125 72.90625 \nL 38.28125 8.296875 \nL 54.390625 8.296875 \nL 54.390625 0 \nL 12.40625 0 \nz\n\" id=\"DejaVuSans-49\"/>\n      </defs>\n      <g transform=\"translate(286.520121 458.666634)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-49\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-48\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_4\">\n     <g id=\"line2d_4\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"403.159301\" xlink:href=\"#m8c660d4275\" y=\"444.068196\"/>\n      </g>\n     </g>\n     <g id=\"text_4\">\n      <!-- 15 -->\n      <g transform=\"translate(396.796801 458.666634)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-49\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-53\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_5\">\n     <g id=\"line2d_5\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"513.435981\" xlink:href=\"#m8c660d4275\" y=\"444.068196\"/>\n      </g>\n     </g>\n     <g id=\"text_5\">\n      <!-- 20 -->\n      <defs>\n       <path d=\"M 19.1875 8.296875 \nL 53.609375 8.296875 \nL 53.609375 0 \nL 7.328125 0 \nL 7.328125 8.296875 \nQ 12.9375 14.109375 22.625 23.890625 \nQ 32.328125 33.6875 34.8125 36.53125 \nQ 39.546875 41.84375 41.421875 45.53125 \nQ 43.3125 49.21875 43.3125 52.78125 \nQ 43.3125 58.59375 39.234375 62.25 \nQ 35.15625 65.921875 28.609375 65.921875 \nQ 23.96875 65.921875 18.8125 64.3125 \nQ 13.671875 62.703125 7.8125 59.421875 \nL 7.8125 69.390625 \nQ 13.765625 71.78125 18.9375 73 \nQ 24.125 74.21875 28.421875 74.21875 \nQ 39.75 74.21875 46.484375 68.546875 \nQ 53.21875 62.890625 53.21875 53.421875 \nQ 53.21875 48.921875 51.53125 44.890625 \nQ 49.859375 40.875 45.40625 35.40625 \nQ 44.1875 33.984375 37.640625 27.21875 \nQ 31.109375 20.453125 19.1875 8.296875 \nz\n\" id=\"DejaVuSans-50\"/>\n      </defs>\n      <g transform=\"translate(507.073481 458.666634)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-50\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-48\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"text_6\">\n     <!-- epoch -->\n     <defs>\n      <path d=\"M 56.203125 29.59375 \nL 56.203125 25.203125 \nL 14.890625 25.203125 \nQ 15.484375 15.921875 20.484375 11.0625 \nQ 25.484375 6.203125 34.421875 6.203125 \nQ 39.59375 6.203125 44.453125 7.46875 \nQ 49.3125 8.734375 54.109375 11.28125 \nL 54.109375 2.78125 \nQ 49.265625 0.734375 44.1875 -0.34375 \nQ 39.109375 -1.421875 33.890625 -1.421875 \nQ 20.796875 -1.421875 13.15625 6.1875 \nQ 5.515625 13.8125 5.515625 26.8125 \nQ 5.515625 40.234375 12.765625 48.109375 \nQ 20.015625 56 32.328125 56 \nQ 43.359375 56 49.78125 48.890625 \nQ 56.203125 41.796875 56.203125 29.59375 \nz\nM 47.21875 32.234375 \nQ 47.125 39.59375 43.09375 43.984375 \nQ 39.0625 48.390625 32.421875 48.390625 \nQ 24.90625 48.390625 20.390625 44.140625 \nQ 15.875 39.890625 15.1875 32.171875 \nz\n\" id=\"DejaVuSans-101\"/>\n      <path d=\"M 18.109375 8.203125 \nL 18.109375 -20.796875 \nL 9.078125 -20.796875 \nL 9.078125 54.6875 \nL 18.109375 54.6875 \nL 18.109375 46.390625 \nQ 20.953125 51.265625 25.265625 53.625 \nQ 29.59375 56 35.59375 56 \nQ 45.5625 56 51.78125 48.09375 \nQ 58.015625 40.1875 58.015625 27.296875 \nQ 58.015625 14.40625 51.78125 6.484375 \nQ 45.5625 -1.421875 35.59375 -1.421875 \nQ 29.59375 -1.421875 25.265625 0.953125 \nQ 20.953125 3.328125 18.109375 8.203125 \nz\nM 48.6875 27.296875 \nQ 48.6875 37.203125 44.609375 42.84375 \nQ 40.53125 48.484375 33.40625 48.484375 \nQ 26.265625 48.484375 22.1875 42.84375 \nQ 18.109375 37.203125 18.109375 27.296875 \nQ 18.109375 17.390625 22.1875 11.75 \nQ 26.265625 6.109375 33.40625 6.109375 \nQ 40.53125 6.109375 44.609375 11.75 \nQ 48.6875 17.390625 48.6875 27.296875 \nz\n\" id=\"DejaVuSans-112\"/>\n      <path d=\"M 30.609375 48.390625 \nQ 23.390625 48.390625 19.1875 42.75 \nQ 14.984375 37.109375 14.984375 27.296875 \nQ 14.984375 17.484375 19.15625 11.84375 \nQ 23.34375 6.203125 30.609375 6.203125 \nQ 37.796875 6.203125 41.984375 11.859375 \nQ 46.1875 17.53125 46.1875 27.296875 \nQ 46.1875 37.015625 41.984375 42.703125 \nQ 37.796875 48.390625 30.609375 48.390625 \nz\nM 30.609375 56 \nQ 42.328125 56 49.015625 48.375 \nQ 55.71875 40.765625 55.71875 27.296875 \nQ 55.71875 13.875 49.015625 6.21875 \nQ 42.328125 -1.421875 30.609375 -1.421875 \nQ 18.84375 -1.421875 12.171875 6.21875 \nQ 5.515625 13.875 5.515625 27.296875 \nQ 5.515625 40.765625 12.171875 48.375 \nQ 18.84375 56 30.609375 56 \nz\n\" id=\"DejaVuSans-111\"/>\n      <path d=\"M 48.78125 52.59375 \nL 48.78125 44.1875 \nQ 44.96875 46.296875 41.140625 47.34375 \nQ 37.3125 48.390625 33.40625 48.390625 \nQ 24.65625 48.390625 19.8125 42.84375 \nQ 14.984375 37.3125 14.984375 27.296875 \nQ 14.984375 17.28125 19.8125 11.734375 \nQ 24.65625 6.203125 33.40625 6.203125 \nQ 37.3125 6.203125 41.140625 7.25 \nQ 44.96875 8.296875 48.78125 10.40625 \nL 48.78125 2.09375 \nQ 45.015625 0.34375 40.984375 -0.53125 \nQ 36.96875 -1.421875 32.421875 -1.421875 \nQ 20.0625 -1.421875 12.78125 6.34375 \nQ 5.515625 14.109375 5.515625 27.296875 \nQ 5.515625 40.671875 12.859375 48.328125 \nQ 20.21875 56 33.015625 56 \nQ 37.15625 56 41.109375 55.140625 \nQ 45.0625 54.296875 48.78125 52.59375 \nz\n\" id=\"DejaVuSans-99\"/>\n      <path d=\"M 54.890625 33.015625 \nL 54.890625 0 \nL 45.90625 0 \nL 45.90625 32.71875 \nQ 45.90625 40.484375 42.875 44.328125 \nQ 39.84375 48.1875 33.796875 48.1875 \nQ 26.515625 48.1875 22.3125 43.546875 \nQ 18.109375 38.921875 18.109375 30.90625 \nL 18.109375 0 \nL 9.078125 0 \nL 9.078125 75.984375 \nL 18.109375 75.984375 \nL 18.109375 46.1875 \nQ 21.34375 51.125 25.703125 53.5625 \nQ 30.078125 56 35.796875 56 \nQ 45.21875 56 50.046875 50.171875 \nQ 54.890625 44.34375 54.890625 33.015625 \nz\n\" id=\"DejaVuSans-104\"/>\n     </defs>\n     <g transform=\"translate(310.7375 472.344759)scale(0.1 -0.1)\">\n      <use xlink:href=\"#DejaVuSans-101\"/>\n      <use x=\"61.523438\" xlink:href=\"#DejaVuSans-112\"/>\n      <use x=\"125\" xlink:href=\"#DejaVuSans-111\"/>\n      <use x=\"186.181641\" xlink:href=\"#DejaVuSans-99\"/>\n      <use x=\"241.162109\" xlink:href=\"#DejaVuSans-104\"/>\n     </g>\n    </g>\n   </g>\n   <g id=\"matplotlib.axis_2\">\n    <g id=\"ytick_1\">\n     <g id=\"line2d_6\">\n      <defs>\n       <path d=\"M 0 0 \nL -3.5 0 \n\" id=\"m90ec58dff8\" style=\"stroke:#000000;stroke-width:0.8;\"/>\n      </defs>\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"46.965625\" xlink:href=\"#m90ec58dff8\" y=\"443.523047\"/>\n      </g>\n     </g>\n     <g id=\"text_7\">\n      <!-- 0 -->\n      <g transform=\"translate(33.603125 447.322265)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-48\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_2\">\n     <g id=\"line2d_7\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"46.965625\" xlink:href=\"#m90ec58dff8\" y=\"381.733928\"/>\n      </g>\n     </g>\n     <g id=\"text_8\">\n      <!-- 100 -->\n      <g transform=\"translate(20.878125 385.533147)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-49\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-48\"/>\n       <use x=\"127.246094\" xlink:href=\"#DejaVuSans-48\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_3\">\n     <g id=\"line2d_8\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"46.965625\" xlink:href=\"#m90ec58dff8\" y=\"319.94481\"/>\n      </g>\n     </g>\n     <g id=\"text_9\">\n      <!-- 200 -->\n      <g transform=\"translate(20.878125 323.744029)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-50\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-48\"/>\n       <use x=\"127.246094\" xlink:href=\"#DejaVuSans-48\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_4\">\n     <g id=\"line2d_9\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"46.965625\" xlink:href=\"#m90ec58dff8\" y=\"258.155692\"/>\n      </g>\n     </g>\n     <g id=\"text_10\">\n      <!-- 300 -->\n      <defs>\n       <path d=\"M 40.578125 39.3125 \nQ 47.65625 37.796875 51.625 33 \nQ 55.609375 28.21875 55.609375 21.1875 \nQ 55.609375 10.40625 48.1875 4.484375 \nQ 40.765625 -1.421875 27.09375 -1.421875 \nQ 22.515625 -1.421875 17.65625 -0.515625 \nQ 12.796875 0.390625 7.625 2.203125 \nL 7.625 11.71875 \nQ 11.71875 9.328125 16.59375 8.109375 \nQ 21.484375 6.890625 26.8125 6.890625 \nQ 36.078125 6.890625 40.9375 10.546875 \nQ 45.796875 14.203125 45.796875 21.1875 \nQ 45.796875 27.640625 41.28125 31.265625 \nQ 36.765625 34.90625 28.71875 34.90625 \nL 20.21875 34.90625 \nL 20.21875 43.015625 \nL 29.109375 43.015625 \nQ 36.375 43.015625 40.234375 45.921875 \nQ 44.09375 48.828125 44.09375 54.296875 \nQ 44.09375 59.90625 40.109375 62.90625 \nQ 36.140625 65.921875 28.71875 65.921875 \nQ 24.65625 65.921875 20.015625 65.03125 \nQ 15.375 64.15625 9.8125 62.3125 \nL 9.8125 71.09375 \nQ 15.4375 72.65625 20.34375 73.4375 \nQ 25.25 74.21875 29.59375 74.21875 \nQ 40.828125 74.21875 47.359375 69.109375 \nQ 53.90625 64.015625 53.90625 55.328125 \nQ 53.90625 49.265625 50.4375 45.09375 \nQ 46.96875 40.921875 40.578125 39.3125 \nz\n\" id=\"DejaVuSans-51\"/>\n      </defs>\n      <g transform=\"translate(20.878125 261.954911)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-51\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-48\"/>\n       <use x=\"127.246094\" xlink:href=\"#DejaVuSans-48\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_5\">\n     <g id=\"line2d_10\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"46.965625\" xlink:href=\"#m90ec58dff8\" y=\"196.366574\"/>\n      </g>\n     </g>\n     <g id=\"text_11\">\n      <!-- 400 -->\n      <defs>\n       <path d=\"M 37.796875 64.3125 \nL 12.890625 25.390625 \nL 37.796875 25.390625 \nz\nM 35.203125 72.90625 \nL 47.609375 72.90625 \nL 47.609375 25.390625 \nL 58.015625 25.390625 \nL 58.015625 17.1875 \nL 47.609375 17.1875 \nL 47.609375 0 \nL 37.796875 0 \nL 37.796875 17.1875 \nL 4.890625 17.1875 \nL 4.890625 26.703125 \nz\n\" id=\"DejaVuSans-52\"/>\n      </defs>\n      <g transform=\"translate(20.878125 200.165792)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-52\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-48\"/>\n       <use x=\"127.246094\" xlink:href=\"#DejaVuSans-48\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_6\">\n     <g id=\"line2d_11\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"46.965625\" xlink:href=\"#m90ec58dff8\" y=\"134.577455\"/>\n      </g>\n     </g>\n     <g id=\"text_12\">\n      <!-- 500 -->\n      <g transform=\"translate(20.878125 138.376674)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-53\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-48\"/>\n       <use x=\"127.246094\" xlink:href=\"#DejaVuSans-48\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_7\">\n     <g id=\"line2d_12\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"46.965625\" xlink:href=\"#m90ec58dff8\" y=\"72.788337\"/>\n      </g>\n     </g>\n     <g id=\"text_13\">\n      <!-- 600 -->\n      <defs>\n       <path d=\"M 33.015625 40.375 \nQ 26.375 40.375 22.484375 35.828125 \nQ 18.609375 31.296875 18.609375 23.390625 \nQ 18.609375 15.53125 22.484375 10.953125 \nQ 26.375 6.390625 33.015625 6.390625 \nQ 39.65625 6.390625 43.53125 10.953125 \nQ 47.40625 15.53125 47.40625 23.390625 \nQ 47.40625 31.296875 43.53125 35.828125 \nQ 39.65625 40.375 33.015625 40.375 \nz\nM 52.59375 71.296875 \nL 52.59375 62.3125 \nQ 48.875 64.0625 45.09375 64.984375 \nQ 41.3125 65.921875 37.59375 65.921875 \nQ 27.828125 65.921875 22.671875 59.328125 \nQ 17.53125 52.734375 16.796875 39.40625 \nQ 19.671875 43.65625 24.015625 45.921875 \nQ 28.375 48.1875 33.59375 48.1875 \nQ 44.578125 48.1875 50.953125 41.515625 \nQ 57.328125 34.859375 57.328125 23.390625 \nQ 57.328125 12.15625 50.6875 5.359375 \nQ 44.046875 -1.421875 33.015625 -1.421875 \nQ 20.359375 -1.421875 13.671875 8.265625 \nQ 6.984375 17.96875 6.984375 36.375 \nQ 6.984375 53.65625 15.1875 63.9375 \nQ 23.390625 74.21875 37.203125 74.21875 \nQ 40.921875 74.21875 44.703125 73.484375 \nQ 48.484375 72.75 52.59375 71.296875 \nz\n\" id=\"DejaVuSans-54\"/>\n      </defs>\n      <g transform=\"translate(20.878125 76.587556)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-54\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-48\"/>\n       <use x=\"127.246094\" xlink:href=\"#DejaVuSans-48\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_8\">\n     <g id=\"line2d_13\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"46.965625\" xlink:href=\"#m90ec58dff8\" y=\"10.999219\"/>\n      </g>\n     </g>\n     <g id=\"text_14\">\n      <!-- 700 -->\n      <defs>\n       <path d=\"M 8.203125 72.90625 \nL 55.078125 72.90625 \nL 55.078125 68.703125 \nL 28.609375 0 \nL 18.3125 0 \nL 43.21875 64.59375 \nL 8.203125 64.59375 \nz\n\" id=\"DejaVuSans-55\"/>\n      </defs>\n      <g transform=\"translate(20.878125 14.798437)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-55\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-48\"/>\n       <use x=\"127.246094\" xlink:href=\"#DejaVuSans-48\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"text_15\">\n     <!-- loss -->\n     <defs>\n      <path d=\"M 9.421875 75.984375 \nL 18.40625 75.984375 \nL 18.40625 0 \nL 9.421875 0 \nz\n\" id=\"DejaVuSans-108\"/>\n      <path d=\"M 44.28125 53.078125 \nL 44.28125 44.578125 \nQ 40.484375 46.53125 36.375 47.5 \nQ 32.28125 48.484375 27.875 48.484375 \nQ 21.1875 48.484375 17.84375 46.4375 \nQ 14.5 44.390625 14.5 40.28125 \nQ 14.5 37.15625 16.890625 35.375 \nQ 19.28125 33.59375 26.515625 31.984375 \nL 29.59375 31.296875 \nQ 39.15625 29.25 43.1875 25.515625 \nQ 47.21875 21.78125 47.21875 15.09375 \nQ 47.21875 7.46875 41.1875 3.015625 \nQ 35.15625 -1.421875 24.609375 -1.421875 \nQ 20.21875 -1.421875 15.453125 -0.5625 \nQ 10.6875 0.296875 5.421875 2 \nL 5.421875 11.28125 \nQ 10.40625 8.6875 15.234375 7.390625 \nQ 20.0625 6.109375 24.8125 6.109375 \nQ 31.15625 6.109375 34.5625 8.28125 \nQ 37.984375 10.453125 37.984375 14.40625 \nQ 37.984375 18.0625 35.515625 20.015625 \nQ 33.0625 21.96875 24.703125 23.78125 \nL 21.578125 24.515625 \nQ 13.234375 26.265625 9.515625 29.90625 \nQ 5.8125 33.546875 5.8125 39.890625 \nQ 5.8125 47.609375 11.28125 51.796875 \nQ 16.75 56 26.8125 56 \nQ 31.78125 56 36.171875 55.265625 \nQ 40.578125 54.546875 44.28125 53.078125 \nz\n\" id=\"DejaVuSans-115\"/>\n     </defs>\n     <g transform=\"translate(14.798438 236.286009)rotate(-90)scale(0.1 -0.1)\">\n      <use xlink:href=\"#DejaVuSans-108\"/>\n      <use x=\"27.783203\" xlink:href=\"#DejaVuSans-111\"/>\n      <use x=\"88.964844\" xlink:href=\"#DejaVuSans-115\"/>\n      <use x=\"141.064453\" xlink:href=\"#DejaVuSans-115\"/>\n     </g>\n    </g>\n   </g>\n   <g id=\"line2d_14\">\n    <path clip-path=\"url(#p4a1a41fa4b)\" d=\"M 72.329261 28.955469 \nL 94.384597 210.557221 \nL 116.439933 271.96735 \nL 138.495269 305.968432 \nL 160.550605 327.554597 \nL 182.605941 345.725606 \nL 204.661277 360.25775 \nL 226.716613 370.704771 \nL 248.771949 378.709884 \nL 270.827285 384.653688 \nL 292.882621 390.466323 \nL 314.937957 394.92077 \nL 336.993293 399.67241 \nL 359.048629 403.063449 \nL 381.103965 406.697219 \nL 403.159301 409.828003 \nL 425.214637 412.015155 \nL 447.269973 413.681978 \nL 469.325309 417.249409 \nL 491.380645 418.442264 \nL 513.435981 420.517414 \nL 535.491317 421.792088 \nL 557.546653 422.912377 \nL 579.601989 424.300924 \n\" style=\"fill:none;stroke:#1f77b4;stroke-linecap:square;stroke-width:1.5;\"/>\n    <defs>\n     <path d=\"M 0 3 \nC 0.795609 3 1.55874 2.683901 2.12132 2.12132 \nC 2.683901 1.55874 3 0.795609 3 0 \nC 3 -0.795609 2.683901 -1.55874 2.12132 -2.12132 \nC 1.55874 -2.683901 0.795609 -3 0 -3 \nC -0.795609 -3 -1.55874 -2.683901 -2.12132 -2.12132 \nC -2.683901 -1.55874 -3 -0.795609 -3 0 \nC -3 0.795609 -2.683901 1.55874 -2.12132 2.12132 \nC -1.55874 2.683901 -0.795609 3 0 3 \nz\n\" id=\"m593a09fd0d\" style=\"stroke:#1f77b4;\"/>\n    </defs>\n    <g clip-path=\"url(#p4a1a41fa4b)\">\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"72.329261\" xlink:href=\"#m593a09fd0d\" y=\"28.955469\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"94.384597\" xlink:href=\"#m593a09fd0d\" y=\"210.557221\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"116.439933\" xlink:href=\"#m593a09fd0d\" y=\"271.96735\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"138.495269\" xlink:href=\"#m593a09fd0d\" y=\"305.968432\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"160.550605\" xlink:href=\"#m593a09fd0d\" y=\"327.554597\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"182.605941\" xlink:href=\"#m593a09fd0d\" y=\"345.725606\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"204.661277\" xlink:href=\"#m593a09fd0d\" y=\"360.25775\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"226.716613\" xlink:href=\"#m593a09fd0d\" y=\"370.704771\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"248.771949\" xlink:href=\"#m593a09fd0d\" y=\"378.709884\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"270.827285\" xlink:href=\"#m593a09fd0d\" y=\"384.653688\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"292.882621\" xlink:href=\"#m593a09fd0d\" y=\"390.466323\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"314.937957\" xlink:href=\"#m593a09fd0d\" y=\"394.92077\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"336.993293\" xlink:href=\"#m593a09fd0d\" y=\"399.67241\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"359.048629\" xlink:href=\"#m593a09fd0d\" y=\"403.063449\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"381.103965\" xlink:href=\"#m593a09fd0d\" y=\"406.697219\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"403.159301\" xlink:href=\"#m593a09fd0d\" y=\"409.828003\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"425.214637\" xlink:href=\"#m593a09fd0d\" y=\"412.015155\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"447.269973\" xlink:href=\"#m593a09fd0d\" y=\"413.681978\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"469.325309\" xlink:href=\"#m593a09fd0d\" y=\"417.249409\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"491.380645\" xlink:href=\"#m593a09fd0d\" y=\"418.442264\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"513.435981\" xlink:href=\"#m593a09fd0d\" y=\"420.517414\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"535.491317\" xlink:href=\"#m593a09fd0d\" y=\"421.792088\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"557.546653\" xlink:href=\"#m593a09fd0d\" y=\"422.912377\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"579.601989\" xlink:href=\"#m593a09fd0d\" y=\"424.300924\"/>\n    </g>\n   </g>\n   <g id=\"line2d_15\">\n    <path clip-path=\"url(#p4a1a41fa4b)\" d=\"M 72.329261 222.371752 \nL 94.384597 306.862131 \nL 116.439933 343.524632 \nL 138.495269 359.261361 \nL 160.550605 373.361301 \nL 182.605941 382.794721 \nL 204.661277 389.631125 \nL 226.716613 393.68535 \nL 248.771949 397.646681 \nL 270.827285 401.632862 \nL 292.882621 402.824719 \nL 314.937957 405.086889 \nL 336.993293 406.550181 \nL 359.048629 408.081782 \nL 381.103965 409.497482 \nL 403.159301 409.134496 \nL 425.214637 410.929512 \nL 447.269973 410.808282 \nL 469.325309 408.914491 \nL 491.380645 410.908144 \nL 513.435981 409.271328 \nL 535.491317 410.750508 \nL 557.546653 409.928588 \nL 579.601989 409.629305 \n\" style=\"fill:none;stroke:#ff7f0e;stroke-linecap:square;stroke-width:1.5;\"/>\n    <defs>\n     <path d=\"M 0 3 \nC 0.795609 3 1.55874 2.683901 2.12132 2.12132 \nC 2.683901 1.55874 3 0.795609 3 0 \nC 3 -0.795609 2.683901 -1.55874 2.12132 -2.12132 \nC 1.55874 -2.683901 0.795609 -3 0 -3 \nC -0.795609 -3 -1.55874 -2.683901 -2.12132 -2.12132 \nC -2.683901 -1.55874 -3 -0.795609 -3 0 \nC -3 0.795609 -2.683901 1.55874 -2.12132 2.12132 \nC -1.55874 2.683901 -0.795609 3 0 3 \nz\n\" id=\"m3b76ca3c5e\" style=\"stroke:#ff7f0e;\"/>\n    </defs>\n    <g clip-path=\"url(#p4a1a41fa4b)\">\n     <use style=\"fill:#ff7f0e;stroke:#ff7f0e;\" x=\"72.329261\" xlink:href=\"#m3b76ca3c5e\" y=\"222.371752\"/>\n     <use style=\"fill:#ff7f0e;stroke:#ff7f0e;\" x=\"94.384597\" xlink:href=\"#m3b76ca3c5e\" y=\"306.862131\"/>\n     <use style=\"fill:#ff7f0e;stroke:#ff7f0e;\" x=\"116.439933\" xlink:href=\"#m3b76ca3c5e\" y=\"343.524632\"/>\n     <use style=\"fill:#ff7f0e;stroke:#ff7f0e;\" x=\"138.495269\" xlink:href=\"#m3b76ca3c5e\" y=\"359.261361\"/>\n     <use style=\"fill:#ff7f0e;stroke:#ff7f0e;\" x=\"160.550605\" xlink:href=\"#m3b76ca3c5e\" y=\"373.361301\"/>\n     <use style=\"fill:#ff7f0e;stroke:#ff7f0e;\" x=\"182.605941\" xlink:href=\"#m3b76ca3c5e\" y=\"382.794721\"/>\n     <use style=\"fill:#ff7f0e;stroke:#ff7f0e;\" x=\"204.661277\" xlink:href=\"#m3b76ca3c5e\" y=\"389.631125\"/>\n     <use style=\"fill:#ff7f0e;stroke:#ff7f0e;\" x=\"226.716613\" xlink:href=\"#m3b76ca3c5e\" y=\"393.68535\"/>\n     <use style=\"fill:#ff7f0e;stroke:#ff7f0e;\" x=\"248.771949\" xlink:href=\"#m3b76ca3c5e\" y=\"397.646681\"/>\n     <use style=\"fill:#ff7f0e;stroke:#ff7f0e;\" x=\"270.827285\" xlink:href=\"#m3b76ca3c5e\" y=\"401.632862\"/>\n     <use style=\"fill:#ff7f0e;stroke:#ff7f0e;\" x=\"292.882621\" xlink:href=\"#m3b76ca3c5e\" y=\"402.824719\"/>\n     <use style=\"fill:#ff7f0e;stroke:#ff7f0e;\" x=\"314.937957\" xlink:href=\"#m3b76ca3c5e\" y=\"405.086889\"/>\n     <use style=\"fill:#ff7f0e;stroke:#ff7f0e;\" x=\"336.993293\" xlink:href=\"#m3b76ca3c5e\" y=\"406.550181\"/>\n     <use style=\"fill:#ff7f0e;stroke:#ff7f0e;\" x=\"359.048629\" xlink:href=\"#m3b76ca3c5e\" y=\"408.081782\"/>\n     <use style=\"fill:#ff7f0e;stroke:#ff7f0e;\" x=\"381.103965\" xlink:href=\"#m3b76ca3c5e\" y=\"409.497482\"/>\n     <use style=\"fill:#ff7f0e;stroke:#ff7f0e;\" x=\"403.159301\" xlink:href=\"#m3b76ca3c5e\" y=\"409.134496\"/>\n     <use style=\"fill:#ff7f0e;stroke:#ff7f0e;\" x=\"425.214637\" xlink:href=\"#m3b76ca3c5e\" y=\"410.929512\"/>\n     <use style=\"fill:#ff7f0e;stroke:#ff7f0e;\" x=\"447.269973\" xlink:href=\"#m3b76ca3c5e\" y=\"410.808282\"/>\n     <use style=\"fill:#ff7f0e;stroke:#ff7f0e;\" x=\"469.325309\" xlink:href=\"#m3b76ca3c5e\" y=\"408.914491\"/>\n     <use style=\"fill:#ff7f0e;stroke:#ff7f0e;\" x=\"491.380645\" xlink:href=\"#m3b76ca3c5e\" y=\"410.908144\"/>\n     <use style=\"fill:#ff7f0e;stroke:#ff7f0e;\" x=\"513.435981\" xlink:href=\"#m3b76ca3c5e\" y=\"409.271328\"/>\n     <use style=\"fill:#ff7f0e;stroke:#ff7f0e;\" x=\"535.491317\" xlink:href=\"#m3b76ca3c5e\" y=\"410.750508\"/>\n     <use style=\"fill:#ff7f0e;stroke:#ff7f0e;\" x=\"557.546653\" xlink:href=\"#m3b76ca3c5e\" y=\"409.928588\"/>\n     <use style=\"fill:#ff7f0e;stroke:#ff7f0e;\" x=\"579.601989\" xlink:href=\"#m3b76ca3c5e\" y=\"409.629305\"/>\n    </g>\n   </g>\n   <g id=\"patch_3\">\n    <path d=\"M 46.965625 444.068196 \nL 46.965625 9.188196 \n\" style=\"fill:none;stroke:#000000;stroke-linecap:square;stroke-linejoin:miter;stroke-width:0.8;\"/>\n   </g>\n   <g id=\"patch_4\">\n    <path d=\"M 604.965625 444.068196 \nL 604.965625 9.188196 \n\" style=\"fill:none;stroke:#000000;stroke-linecap:square;stroke-linejoin:miter;stroke-width:0.8;\"/>\n   </g>\n   <g id=\"patch_5\">\n    <path d=\"M 46.965625 444.068196 \nL 604.965625 444.068196 \n\" style=\"fill:none;stroke:#000000;stroke-linecap:square;stroke-linejoin:miter;stroke-width:0.8;\"/>\n   </g>\n   <g id=\"patch_6\">\n    <path d=\"M 46.965625 9.188196 \nL 604.965625 9.188196 \n\" style=\"fill:none;stroke:#000000;stroke-linecap:square;stroke-linejoin:miter;stroke-width:0.8;\"/>\n   </g>\n   <g id=\"legend_1\">\n    <g id=\"patch_7\">\n     <path d=\"M 53.965625 46.544446 \nL 109.240625 46.544446 \nQ 111.240625 46.544446 111.240625 44.544446 \nL 111.240625 16.188196 \nQ 111.240625 14.188196 109.240625 14.188196 \nL 53.965625 14.188196 \nQ 51.965625 14.188196 51.965625 16.188196 \nL 51.965625 44.544446 \nQ 51.965625 46.544446 53.965625 46.544446 \nz\n\" style=\"fill:#ffffff;opacity:0.8;stroke:#cccccc;stroke-linejoin:miter;\"/>\n    </g>\n    <g id=\"line2d_16\">\n     <path d=\"M 55.965625 22.286634 \nL 75.965625 22.286634 \n\" style=\"fill:none;stroke:#1f77b4;stroke-linecap:square;stroke-width:1.5;\"/>\n    </g>\n    <g id=\"line2d_17\">\n     <g>\n      <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"65.965625\" xlink:href=\"#m593a09fd0d\" y=\"22.286634\"/>\n     </g>\n    </g>\n    <g id=\"text_16\">\n     <!-- train -->\n     <defs>\n      <path d=\"M 18.3125 70.21875 \nL 18.3125 54.6875 \nL 36.8125 54.6875 \nL 36.8125 47.703125 \nL 18.3125 47.703125 \nL 18.3125 18.015625 \nQ 18.3125 11.328125 20.140625 9.421875 \nQ 21.96875 7.515625 27.59375 7.515625 \nL 36.8125 7.515625 \nL 36.8125 0 \nL 27.59375 0 \nQ 17.1875 0 13.234375 3.875 \nQ 9.28125 7.765625 9.28125 18.015625 \nL 9.28125 47.703125 \nL 2.6875 47.703125 \nL 2.6875 54.6875 \nL 9.28125 54.6875 \nL 9.28125 70.21875 \nz\n\" id=\"DejaVuSans-116\"/>\n      <path d=\"M 41.109375 46.296875 \nQ 39.59375 47.171875 37.8125 47.578125 \nQ 36.03125 48 33.890625 48 \nQ 26.265625 48 22.1875 43.046875 \nQ 18.109375 38.09375 18.109375 28.8125 \nL 18.109375 0 \nL 9.078125 0 \nL 9.078125 54.6875 \nL 18.109375 54.6875 \nL 18.109375 46.1875 \nQ 20.953125 51.171875 25.484375 53.578125 \nQ 30.03125 56 36.53125 56 \nQ 37.453125 56 38.578125 55.875 \nQ 39.703125 55.765625 41.0625 55.515625 \nz\n\" id=\"DejaVuSans-114\"/>\n      <path d=\"M 34.28125 27.484375 \nQ 23.390625 27.484375 19.1875 25 \nQ 14.984375 22.515625 14.984375 16.5 \nQ 14.984375 11.71875 18.140625 8.90625 \nQ 21.296875 6.109375 26.703125 6.109375 \nQ 34.1875 6.109375 38.703125 11.40625 \nQ 43.21875 16.703125 43.21875 25.484375 \nL 43.21875 27.484375 \nz\nM 52.203125 31.203125 \nL 52.203125 0 \nL 43.21875 0 \nL 43.21875 8.296875 \nQ 40.140625 3.328125 35.546875 0.953125 \nQ 30.953125 -1.421875 24.3125 -1.421875 \nQ 15.921875 -1.421875 10.953125 3.296875 \nQ 6 8.015625 6 15.921875 \nQ 6 25.140625 12.171875 29.828125 \nQ 18.359375 34.515625 30.609375 34.515625 \nL 43.21875 34.515625 \nL 43.21875 35.40625 \nQ 43.21875 41.609375 39.140625 45 \nQ 35.0625 48.390625 27.6875 48.390625 \nQ 23 48.390625 18.546875 47.265625 \nQ 14.109375 46.140625 10.015625 43.890625 \nL 10.015625 52.203125 \nQ 14.9375 54.109375 19.578125 55.046875 \nQ 24.21875 56 28.609375 56 \nQ 40.484375 56 46.34375 49.84375 \nQ 52.203125 43.703125 52.203125 31.203125 \nz\n\" id=\"DejaVuSans-97\"/>\n      <path d=\"M 9.421875 54.6875 \nL 18.40625 54.6875 \nL 18.40625 0 \nL 9.421875 0 \nz\nM 9.421875 75.984375 \nL 18.40625 75.984375 \nL 18.40625 64.59375 \nL 9.421875 64.59375 \nz\n\" id=\"DejaVuSans-105\"/>\n      <path d=\"M 54.890625 33.015625 \nL 54.890625 0 \nL 45.90625 0 \nL 45.90625 32.71875 \nQ 45.90625 40.484375 42.875 44.328125 \nQ 39.84375 48.1875 33.796875 48.1875 \nQ 26.515625 48.1875 22.3125 43.546875 \nQ 18.109375 38.921875 18.109375 30.90625 \nL 18.109375 0 \nL 9.078125 0 \nL 9.078125 54.6875 \nL 18.109375 54.6875 \nL 18.109375 46.1875 \nQ 21.34375 51.125 25.703125 53.5625 \nQ 30.078125 56 35.796875 56 \nQ 45.21875 56 50.046875 50.171875 \nQ 54.890625 44.34375 54.890625 33.015625 \nz\n\" id=\"DejaVuSans-110\"/>\n     </defs>\n     <g transform=\"translate(83.965625 25.786634)scale(0.1 -0.1)\">\n      <use xlink:href=\"#DejaVuSans-116\"/>\n      <use x=\"39.208984\" xlink:href=\"#DejaVuSans-114\"/>\n      <use x=\"80.322266\" xlink:href=\"#DejaVuSans-97\"/>\n      <use x=\"141.601562\" xlink:href=\"#DejaVuSans-105\"/>\n      <use x=\"169.384766\" xlink:href=\"#DejaVuSans-110\"/>\n     </g>\n    </g>\n    <g id=\"line2d_18\">\n     <path d=\"M 55.965625 36.964759 \nL 75.965625 36.964759 \n\" style=\"fill:none;stroke:#ff7f0e;stroke-linecap:square;stroke-width:1.5;\"/>\n    </g>\n    <g id=\"line2d_19\">\n     <g>\n      <use style=\"fill:#ff7f0e;stroke:#ff7f0e;\" x=\"65.965625\" xlink:href=\"#m3b76ca3c5e\" y=\"36.964759\"/>\n     </g>\n    </g>\n    <g id=\"text_17\">\n     <!-- val -->\n     <defs>\n      <path d=\"M 2.984375 54.6875 \nL 12.5 54.6875 \nL 29.59375 8.796875 \nL 46.6875 54.6875 \nL 56.203125 54.6875 \nL 35.6875 0 \nL 23.484375 0 \nz\n\" id=\"DejaVuSans-118\"/>\n     </defs>\n     <g transform=\"translate(83.965625 40.464759)scale(0.1 -0.1)\">\n      <use xlink:href=\"#DejaVuSans-118\"/>\n      <use x=\"59.179688\" xlink:href=\"#DejaVuSans-97\"/>\n      <use x=\"120.458984\" xlink:href=\"#DejaVuSans-108\"/>\n     </g>\n    </g>\n   </g>\n  </g>\n </g>\n <defs>\n  <clipPath id=\"p4a1a41fa4b\">\n   <rect height=\"434.88\" width=\"558\" x=\"46.965625\" y=\"9.188196\"/>\n  </clipPath>\n </defs>\n</svg>\n"
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "IbcO-roYFW4Z",
        "outputId": "7126993d-e84f-49cb-d146-ad65b42098ff",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 660
        }
      },
      "source": [
        "plt.plot(dev_score_history, '-o')\n",
        "plt.xlabel('epoch')\n",
        "plt.ylabel('f1-score')\n",
        "#plt.savefig(DATA_PATH / 'regular_adam_dev_fscore.svg')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 720x576 with 1 Axes>"
            ],
            "image/svg+xml": "<?xml version=\"1.0\" encoding=\"utf-8\" standalone=\"no\"?>\n<!DOCTYPE svg PUBLIC \"-//W3C//DTD SVG 1.1//EN\"\n  \"http://www.w3.org/Graphics/SVG/1.1/DTD/svg11.dtd\">\n<!-- Created with matplotlib (https://matplotlib.org/) -->\n<svg height=\"479.63625pt\" version=\"1.1\" viewBox=\"0 0 608.98125 479.63625\" width=\"608.98125pt\" xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">\n <defs>\n  <style type=\"text/css\">\n*{stroke-linecap:butt;stroke-linejoin:round;}\n  </style>\n </defs>\n <g id=\"figure_1\">\n  <g id=\"patch_1\">\n   <path d=\"M 0 479.63625 \nL 608.98125 479.63625 \nL 608.98125 0 \nL 0 0 \nz\n\" style=\"fill:none;\"/>\n  </g>\n  <g id=\"axes_1\">\n   <g id=\"patch_2\">\n    <path d=\"M 43.78125 442.08 \nL 601.78125 442.08 \nL 601.78125 7.2 \nL 43.78125 7.2 \nz\n\" style=\"fill:#ffffff;\"/>\n   </g>\n   <g id=\"matplotlib.axis_1\">\n    <g id=\"xtick_1\">\n     <g id=\"line2d_1\">\n      <defs>\n       <path d=\"M 0 0 \nL 0 3.5 \n\" id=\"m725e294167\" style=\"stroke:#000000;stroke-width:0.8;\"/>\n      </defs>\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"69.144886\" xlink:href=\"#m725e294167\" y=\"442.08\"/>\n      </g>\n     </g>\n     <g id=\"text_1\">\n      <!-- 0 -->\n      <defs>\n       <path d=\"M 31.78125 66.40625 \nQ 24.171875 66.40625 20.328125 58.90625 \nQ 16.5 51.421875 16.5 36.375 \nQ 16.5 21.390625 20.328125 13.890625 \nQ 24.171875 6.390625 31.78125 6.390625 \nQ 39.453125 6.390625 43.28125 13.890625 \nQ 47.125 21.390625 47.125 36.375 \nQ 47.125 51.421875 43.28125 58.90625 \nQ 39.453125 66.40625 31.78125 66.40625 \nz\nM 31.78125 74.21875 \nQ 44.046875 74.21875 50.515625 64.515625 \nQ 56.984375 54.828125 56.984375 36.375 \nQ 56.984375 17.96875 50.515625 8.265625 \nQ 44.046875 -1.421875 31.78125 -1.421875 \nQ 19.53125 -1.421875 13.0625 8.265625 \nQ 6.59375 17.96875 6.59375 36.375 \nQ 6.59375 54.828125 13.0625 64.515625 \nQ 19.53125 74.21875 31.78125 74.21875 \nz\n\" id=\"DejaVuSans-48\"/>\n      </defs>\n      <g transform=\"translate(65.963636 456.678437)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-48\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_2\">\n     <g id=\"line2d_2\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"179.421566\" xlink:href=\"#m725e294167\" y=\"442.08\"/>\n      </g>\n     </g>\n     <g id=\"text_2\">\n      <!-- 5 -->\n      <defs>\n       <path d=\"M 10.796875 72.90625 \nL 49.515625 72.90625 \nL 49.515625 64.59375 \nL 19.828125 64.59375 \nL 19.828125 46.734375 \nQ 21.96875 47.46875 24.109375 47.828125 \nQ 26.265625 48.1875 28.421875 48.1875 \nQ 40.625 48.1875 47.75 41.5 \nQ 54.890625 34.8125 54.890625 23.390625 \nQ 54.890625 11.625 47.5625 5.09375 \nQ 40.234375 -1.421875 26.90625 -1.421875 \nQ 22.3125 -1.421875 17.546875 -0.640625 \nQ 12.796875 0.140625 7.71875 1.703125 \nL 7.71875 11.625 \nQ 12.109375 9.234375 16.796875 8.0625 \nQ 21.484375 6.890625 26.703125 6.890625 \nQ 35.15625 6.890625 40.078125 11.328125 \nQ 45.015625 15.765625 45.015625 23.390625 \nQ 45.015625 31 40.078125 35.4375 \nQ 35.15625 39.890625 26.703125 39.890625 \nQ 22.75 39.890625 18.8125 39.015625 \nQ 14.890625 38.140625 10.796875 36.28125 \nz\n\" id=\"DejaVuSans-53\"/>\n      </defs>\n      <g transform=\"translate(176.240316 456.678437)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-53\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_3\">\n     <g id=\"line2d_3\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"289.698246\" xlink:href=\"#m725e294167\" y=\"442.08\"/>\n      </g>\n     </g>\n     <g id=\"text_3\">\n      <!-- 10 -->\n      <defs>\n       <path d=\"M 12.40625 8.296875 \nL 28.515625 8.296875 \nL 28.515625 63.921875 \nL 10.984375 60.40625 \nL 10.984375 69.390625 \nL 28.421875 72.90625 \nL 38.28125 72.90625 \nL 38.28125 8.296875 \nL 54.390625 8.296875 \nL 54.390625 0 \nL 12.40625 0 \nz\n\" id=\"DejaVuSans-49\"/>\n      </defs>\n      <g transform=\"translate(283.335746 456.678437)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-49\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-48\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_4\">\n     <g id=\"line2d_4\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"399.974926\" xlink:href=\"#m725e294167\" y=\"442.08\"/>\n      </g>\n     </g>\n     <g id=\"text_4\">\n      <!-- 15 -->\n      <g transform=\"translate(393.612426 456.678437)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-49\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-53\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_5\">\n     <g id=\"line2d_5\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"510.251606\" xlink:href=\"#m725e294167\" y=\"442.08\"/>\n      </g>\n     </g>\n     <g id=\"text_5\">\n      <!-- 20 -->\n      <defs>\n       <path d=\"M 19.1875 8.296875 \nL 53.609375 8.296875 \nL 53.609375 0 \nL 7.328125 0 \nL 7.328125 8.296875 \nQ 12.9375 14.109375 22.625 23.890625 \nQ 32.328125 33.6875 34.8125 36.53125 \nQ 39.546875 41.84375 41.421875 45.53125 \nQ 43.3125 49.21875 43.3125 52.78125 \nQ 43.3125 58.59375 39.234375 62.25 \nQ 35.15625 65.921875 28.609375 65.921875 \nQ 23.96875 65.921875 18.8125 64.3125 \nQ 13.671875 62.703125 7.8125 59.421875 \nL 7.8125 69.390625 \nQ 13.765625 71.78125 18.9375 73 \nQ 24.125 74.21875 28.421875 74.21875 \nQ 39.75 74.21875 46.484375 68.546875 \nQ 53.21875 62.890625 53.21875 53.421875 \nQ 53.21875 48.921875 51.53125 44.890625 \nQ 49.859375 40.875 45.40625 35.40625 \nQ 44.1875 33.984375 37.640625 27.21875 \nQ 31.109375 20.453125 19.1875 8.296875 \nz\n\" id=\"DejaVuSans-50\"/>\n      </defs>\n      <g transform=\"translate(503.889106 456.678437)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-50\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-48\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"text_6\">\n     <!-- epoch -->\n     <defs>\n      <path d=\"M 56.203125 29.59375 \nL 56.203125 25.203125 \nL 14.890625 25.203125 \nQ 15.484375 15.921875 20.484375 11.0625 \nQ 25.484375 6.203125 34.421875 6.203125 \nQ 39.59375 6.203125 44.453125 7.46875 \nQ 49.3125 8.734375 54.109375 11.28125 \nL 54.109375 2.78125 \nQ 49.265625 0.734375 44.1875 -0.34375 \nQ 39.109375 -1.421875 33.890625 -1.421875 \nQ 20.796875 -1.421875 13.15625 6.1875 \nQ 5.515625 13.8125 5.515625 26.8125 \nQ 5.515625 40.234375 12.765625 48.109375 \nQ 20.015625 56 32.328125 56 \nQ 43.359375 56 49.78125 48.890625 \nQ 56.203125 41.796875 56.203125 29.59375 \nz\nM 47.21875 32.234375 \nQ 47.125 39.59375 43.09375 43.984375 \nQ 39.0625 48.390625 32.421875 48.390625 \nQ 24.90625 48.390625 20.390625 44.140625 \nQ 15.875 39.890625 15.1875 32.171875 \nz\n\" id=\"DejaVuSans-101\"/>\n      <path d=\"M 18.109375 8.203125 \nL 18.109375 -20.796875 \nL 9.078125 -20.796875 \nL 9.078125 54.6875 \nL 18.109375 54.6875 \nL 18.109375 46.390625 \nQ 20.953125 51.265625 25.265625 53.625 \nQ 29.59375 56 35.59375 56 \nQ 45.5625 56 51.78125 48.09375 \nQ 58.015625 40.1875 58.015625 27.296875 \nQ 58.015625 14.40625 51.78125 6.484375 \nQ 45.5625 -1.421875 35.59375 -1.421875 \nQ 29.59375 -1.421875 25.265625 0.953125 \nQ 20.953125 3.328125 18.109375 8.203125 \nz\nM 48.6875 27.296875 \nQ 48.6875 37.203125 44.609375 42.84375 \nQ 40.53125 48.484375 33.40625 48.484375 \nQ 26.265625 48.484375 22.1875 42.84375 \nQ 18.109375 37.203125 18.109375 27.296875 \nQ 18.109375 17.390625 22.1875 11.75 \nQ 26.265625 6.109375 33.40625 6.109375 \nQ 40.53125 6.109375 44.609375 11.75 \nQ 48.6875 17.390625 48.6875 27.296875 \nz\n\" id=\"DejaVuSans-112\"/>\n      <path d=\"M 30.609375 48.390625 \nQ 23.390625 48.390625 19.1875 42.75 \nQ 14.984375 37.109375 14.984375 27.296875 \nQ 14.984375 17.484375 19.15625 11.84375 \nQ 23.34375 6.203125 30.609375 6.203125 \nQ 37.796875 6.203125 41.984375 11.859375 \nQ 46.1875 17.53125 46.1875 27.296875 \nQ 46.1875 37.015625 41.984375 42.703125 \nQ 37.796875 48.390625 30.609375 48.390625 \nz\nM 30.609375 56 \nQ 42.328125 56 49.015625 48.375 \nQ 55.71875 40.765625 55.71875 27.296875 \nQ 55.71875 13.875 49.015625 6.21875 \nQ 42.328125 -1.421875 30.609375 -1.421875 \nQ 18.84375 -1.421875 12.171875 6.21875 \nQ 5.515625 13.875 5.515625 27.296875 \nQ 5.515625 40.765625 12.171875 48.375 \nQ 18.84375 56 30.609375 56 \nz\n\" id=\"DejaVuSans-111\"/>\n      <path d=\"M 48.78125 52.59375 \nL 48.78125 44.1875 \nQ 44.96875 46.296875 41.140625 47.34375 \nQ 37.3125 48.390625 33.40625 48.390625 \nQ 24.65625 48.390625 19.8125 42.84375 \nQ 14.984375 37.3125 14.984375 27.296875 \nQ 14.984375 17.28125 19.8125 11.734375 \nQ 24.65625 6.203125 33.40625 6.203125 \nQ 37.3125 6.203125 41.140625 7.25 \nQ 44.96875 8.296875 48.78125 10.40625 \nL 48.78125 2.09375 \nQ 45.015625 0.34375 40.984375 -0.53125 \nQ 36.96875 -1.421875 32.421875 -1.421875 \nQ 20.0625 -1.421875 12.78125 6.34375 \nQ 5.515625 14.109375 5.515625 27.296875 \nQ 5.515625 40.671875 12.859375 48.328125 \nQ 20.21875 56 33.015625 56 \nQ 37.15625 56 41.109375 55.140625 \nQ 45.0625 54.296875 48.78125 52.59375 \nz\n\" id=\"DejaVuSans-99\"/>\n      <path d=\"M 54.890625 33.015625 \nL 54.890625 0 \nL 45.90625 0 \nL 45.90625 32.71875 \nQ 45.90625 40.484375 42.875 44.328125 \nQ 39.84375 48.1875 33.796875 48.1875 \nQ 26.515625 48.1875 22.3125 43.546875 \nQ 18.109375 38.921875 18.109375 30.90625 \nL 18.109375 0 \nL 9.078125 0 \nL 9.078125 75.984375 \nL 18.109375 75.984375 \nL 18.109375 46.1875 \nQ 21.34375 51.125 25.703125 53.5625 \nQ 30.078125 56 35.796875 56 \nQ 45.21875 56 50.046875 50.171875 \nQ 54.890625 44.34375 54.890625 33.015625 \nz\n\" id=\"DejaVuSans-104\"/>\n     </defs>\n     <g transform=\"translate(307.553125 470.356562)scale(0.1 -0.1)\">\n      <use xlink:href=\"#DejaVuSans-101\"/>\n      <use x=\"61.523438\" xlink:href=\"#DejaVuSans-112\"/>\n      <use x=\"125\" xlink:href=\"#DejaVuSans-111\"/>\n      <use x=\"186.181641\" xlink:href=\"#DejaVuSans-99\"/>\n      <use x=\"241.162109\" xlink:href=\"#DejaVuSans-104\"/>\n     </g>\n    </g>\n   </g>\n   <g id=\"matplotlib.axis_2\">\n    <g id=\"ytick_1\">\n     <g id=\"line2d_6\">\n      <defs>\n       <path d=\"M 0 0 \nL -3.5 0 \n\" id=\"m9e0071af43\" style=\"stroke:#000000;stroke-width:0.8;\"/>\n      </defs>\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"43.78125\" xlink:href=\"#m9e0071af43\" y=\"433.531088\"/>\n      </g>\n     </g>\n     <g id=\"text_7\">\n      <!-- 0.0 -->\n      <defs>\n       <path d=\"M 10.6875 12.40625 \nL 21 12.40625 \nL 21 0 \nL 10.6875 0 \nz\n\" id=\"DejaVuSans-46\"/>\n      </defs>\n      <g transform=\"translate(20.878125 437.330307)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-48\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-46\"/>\n       <use x=\"95.410156\" xlink:href=\"#DejaVuSans-48\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_2\">\n     <g id=\"line2d_7\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"43.78125\" xlink:href=\"#m9e0071af43\" y=\"384.558628\"/>\n      </g>\n     </g>\n     <g id=\"text_8\">\n      <!-- 0.1 -->\n      <g transform=\"translate(20.878125 388.357847)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-48\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-46\"/>\n       <use x=\"95.410156\" xlink:href=\"#DejaVuSans-49\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_3\">\n     <g id=\"line2d_8\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"43.78125\" xlink:href=\"#m9e0071af43\" y=\"335.586169\"/>\n      </g>\n     </g>\n     <g id=\"text_9\">\n      <!-- 0.2 -->\n      <g transform=\"translate(20.878125 339.385388)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-48\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-46\"/>\n       <use x=\"95.410156\" xlink:href=\"#DejaVuSans-50\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_4\">\n     <g id=\"line2d_9\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"43.78125\" xlink:href=\"#m9e0071af43\" y=\"286.613709\"/>\n      </g>\n     </g>\n     <g id=\"text_10\">\n      <!-- 0.3 -->\n      <defs>\n       <path d=\"M 40.578125 39.3125 \nQ 47.65625 37.796875 51.625 33 \nQ 55.609375 28.21875 55.609375 21.1875 \nQ 55.609375 10.40625 48.1875 4.484375 \nQ 40.765625 -1.421875 27.09375 -1.421875 \nQ 22.515625 -1.421875 17.65625 -0.515625 \nQ 12.796875 0.390625 7.625 2.203125 \nL 7.625 11.71875 \nQ 11.71875 9.328125 16.59375 8.109375 \nQ 21.484375 6.890625 26.8125 6.890625 \nQ 36.078125 6.890625 40.9375 10.546875 \nQ 45.796875 14.203125 45.796875 21.1875 \nQ 45.796875 27.640625 41.28125 31.265625 \nQ 36.765625 34.90625 28.71875 34.90625 \nL 20.21875 34.90625 \nL 20.21875 43.015625 \nL 29.109375 43.015625 \nQ 36.375 43.015625 40.234375 45.921875 \nQ 44.09375 48.828125 44.09375 54.296875 \nQ 44.09375 59.90625 40.109375 62.90625 \nQ 36.140625 65.921875 28.71875 65.921875 \nQ 24.65625 65.921875 20.015625 65.03125 \nQ 15.375 64.15625 9.8125 62.3125 \nL 9.8125 71.09375 \nQ 15.4375 72.65625 20.34375 73.4375 \nQ 25.25 74.21875 29.59375 74.21875 \nQ 40.828125 74.21875 47.359375 69.109375 \nQ 53.90625 64.015625 53.90625 55.328125 \nQ 53.90625 49.265625 50.4375 45.09375 \nQ 46.96875 40.921875 40.578125 39.3125 \nz\n\" id=\"DejaVuSans-51\"/>\n      </defs>\n      <g transform=\"translate(20.878125 290.412928)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-48\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-46\"/>\n       <use x=\"95.410156\" xlink:href=\"#DejaVuSans-51\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_5\">\n     <g id=\"line2d_10\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"43.78125\" xlink:href=\"#m9e0071af43\" y=\"237.64125\"/>\n      </g>\n     </g>\n     <g id=\"text_11\">\n      <!-- 0.4 -->\n      <defs>\n       <path d=\"M 37.796875 64.3125 \nL 12.890625 25.390625 \nL 37.796875 25.390625 \nz\nM 35.203125 72.90625 \nL 47.609375 72.90625 \nL 47.609375 25.390625 \nL 58.015625 25.390625 \nL 58.015625 17.1875 \nL 47.609375 17.1875 \nL 47.609375 0 \nL 37.796875 0 \nL 37.796875 17.1875 \nL 4.890625 17.1875 \nL 4.890625 26.703125 \nz\n\" id=\"DejaVuSans-52\"/>\n      </defs>\n      <g transform=\"translate(20.878125 241.440469)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-48\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-46\"/>\n       <use x=\"95.410156\" xlink:href=\"#DejaVuSans-52\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_6\">\n     <g id=\"line2d_11\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"43.78125\" xlink:href=\"#m9e0071af43\" y=\"188.66879\"/>\n      </g>\n     </g>\n     <g id=\"text_12\">\n      <!-- 0.5 -->\n      <g transform=\"translate(20.878125 192.468009)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-48\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-46\"/>\n       <use x=\"95.410156\" xlink:href=\"#DejaVuSans-53\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_7\">\n     <g id=\"line2d_12\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"43.78125\" xlink:href=\"#m9e0071af43\" y=\"139.696331\"/>\n      </g>\n     </g>\n     <g id=\"text_13\">\n      <!-- 0.6 -->\n      <defs>\n       <path d=\"M 33.015625 40.375 \nQ 26.375 40.375 22.484375 35.828125 \nQ 18.609375 31.296875 18.609375 23.390625 \nQ 18.609375 15.53125 22.484375 10.953125 \nQ 26.375 6.390625 33.015625 6.390625 \nQ 39.65625 6.390625 43.53125 10.953125 \nQ 47.40625 15.53125 47.40625 23.390625 \nQ 47.40625 31.296875 43.53125 35.828125 \nQ 39.65625 40.375 33.015625 40.375 \nz\nM 52.59375 71.296875 \nL 52.59375 62.3125 \nQ 48.875 64.0625 45.09375 64.984375 \nQ 41.3125 65.921875 37.59375 65.921875 \nQ 27.828125 65.921875 22.671875 59.328125 \nQ 17.53125 52.734375 16.796875 39.40625 \nQ 19.671875 43.65625 24.015625 45.921875 \nQ 28.375 48.1875 33.59375 48.1875 \nQ 44.578125 48.1875 50.953125 41.515625 \nQ 57.328125 34.859375 57.328125 23.390625 \nQ 57.328125 12.15625 50.6875 5.359375 \nQ 44.046875 -1.421875 33.015625 -1.421875 \nQ 20.359375 -1.421875 13.671875 8.265625 \nQ 6.984375 17.96875 6.984375 36.375 \nQ 6.984375 53.65625 15.1875 63.9375 \nQ 23.390625 74.21875 37.203125 74.21875 \nQ 40.921875 74.21875 44.703125 73.484375 \nQ 48.484375 72.75 52.59375 71.296875 \nz\n\" id=\"DejaVuSans-54\"/>\n      </defs>\n      <g transform=\"translate(20.878125 143.495549)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-48\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-46\"/>\n       <use x=\"95.410156\" xlink:href=\"#DejaVuSans-54\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_8\">\n     <g id=\"line2d_13\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"43.78125\" xlink:href=\"#m9e0071af43\" y=\"90.723871\"/>\n      </g>\n     </g>\n     <g id=\"text_14\">\n      <!-- 0.7 -->\n      <defs>\n       <path d=\"M 8.203125 72.90625 \nL 55.078125 72.90625 \nL 55.078125 68.703125 \nL 28.609375 0 \nL 18.3125 0 \nL 43.21875 64.59375 \nL 8.203125 64.59375 \nz\n\" id=\"DejaVuSans-55\"/>\n      </defs>\n      <g transform=\"translate(20.878125 94.52309)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-48\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-46\"/>\n       <use x=\"95.410156\" xlink:href=\"#DejaVuSans-55\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_9\">\n     <g id=\"line2d_14\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"43.78125\" xlink:href=\"#m9e0071af43\" y=\"41.751411\"/>\n      </g>\n     </g>\n     <g id=\"text_15\">\n      <!-- 0.8 -->\n      <defs>\n       <path d=\"M 31.78125 34.625 \nQ 24.75 34.625 20.71875 30.859375 \nQ 16.703125 27.09375 16.703125 20.515625 \nQ 16.703125 13.921875 20.71875 10.15625 \nQ 24.75 6.390625 31.78125 6.390625 \nQ 38.8125 6.390625 42.859375 10.171875 \nQ 46.921875 13.96875 46.921875 20.515625 \nQ 46.921875 27.09375 42.890625 30.859375 \nQ 38.875 34.625 31.78125 34.625 \nz\nM 21.921875 38.8125 \nQ 15.578125 40.375 12.03125 44.71875 \nQ 8.5 49.078125 8.5 55.328125 \nQ 8.5 64.0625 14.71875 69.140625 \nQ 20.953125 74.21875 31.78125 74.21875 \nQ 42.671875 74.21875 48.875 69.140625 \nQ 55.078125 64.0625 55.078125 55.328125 \nQ 55.078125 49.078125 51.53125 44.71875 \nQ 48 40.375 41.703125 38.8125 \nQ 48.828125 37.15625 52.796875 32.3125 \nQ 56.78125 27.484375 56.78125 20.515625 \nQ 56.78125 9.90625 50.3125 4.234375 \nQ 43.84375 -1.421875 31.78125 -1.421875 \nQ 19.734375 -1.421875 13.25 4.234375 \nQ 6.78125 9.90625 6.78125 20.515625 \nQ 6.78125 27.484375 10.78125 32.3125 \nQ 14.796875 37.15625 21.921875 38.8125 \nz\nM 18.3125 54.390625 \nQ 18.3125 48.734375 21.84375 45.5625 \nQ 25.390625 42.390625 31.78125 42.390625 \nQ 38.140625 42.390625 41.71875 45.5625 \nQ 45.3125 48.734375 45.3125 54.390625 \nQ 45.3125 60.0625 41.71875 63.234375 \nQ 38.140625 66.40625 31.78125 66.40625 \nQ 25.390625 66.40625 21.84375 63.234375 \nQ 18.3125 60.0625 18.3125 54.390625 \nz\n\" id=\"DejaVuSans-56\"/>\n      </defs>\n      <g transform=\"translate(20.878125 45.55063)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-48\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-46\"/>\n       <use x=\"95.410156\" xlink:href=\"#DejaVuSans-56\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"text_16\">\n     <!-- f1-score -->\n     <defs>\n      <path d=\"M 37.109375 75.984375 \nL 37.109375 68.5 \nL 28.515625 68.5 \nQ 23.6875 68.5 21.796875 66.546875 \nQ 19.921875 64.59375 19.921875 59.515625 \nL 19.921875 54.6875 \nL 34.71875 54.6875 \nL 34.71875 47.703125 \nL 19.921875 47.703125 \nL 19.921875 0 \nL 10.890625 0 \nL 10.890625 47.703125 \nL 2.296875 47.703125 \nL 2.296875 54.6875 \nL 10.890625 54.6875 \nL 10.890625 58.5 \nQ 10.890625 67.625 15.140625 71.796875 \nQ 19.390625 75.984375 28.609375 75.984375 \nz\n\" id=\"DejaVuSans-102\"/>\n      <path d=\"M 4.890625 31.390625 \nL 31.203125 31.390625 \nL 31.203125 23.390625 \nL 4.890625 23.390625 \nz\n\" id=\"DejaVuSans-45\"/>\n      <path d=\"M 44.28125 53.078125 \nL 44.28125 44.578125 \nQ 40.484375 46.53125 36.375 47.5 \nQ 32.28125 48.484375 27.875 48.484375 \nQ 21.1875 48.484375 17.84375 46.4375 \nQ 14.5 44.390625 14.5 40.28125 \nQ 14.5 37.15625 16.890625 35.375 \nQ 19.28125 33.59375 26.515625 31.984375 \nL 29.59375 31.296875 \nQ 39.15625 29.25 43.1875 25.515625 \nQ 47.21875 21.78125 47.21875 15.09375 \nQ 47.21875 7.46875 41.1875 3.015625 \nQ 35.15625 -1.421875 24.609375 -1.421875 \nQ 20.21875 -1.421875 15.453125 -0.5625 \nQ 10.6875 0.296875 5.421875 2 \nL 5.421875 11.28125 \nQ 10.40625 8.6875 15.234375 7.390625 \nQ 20.0625 6.109375 24.8125 6.109375 \nQ 31.15625 6.109375 34.5625 8.28125 \nQ 37.984375 10.453125 37.984375 14.40625 \nQ 37.984375 18.0625 35.515625 20.015625 \nQ 33.0625 21.96875 24.703125 23.78125 \nL 21.578125 24.515625 \nQ 13.234375 26.265625 9.515625 29.90625 \nQ 5.8125 33.546875 5.8125 39.890625 \nQ 5.8125 47.609375 11.28125 51.796875 \nQ 16.75 56 26.8125 56 \nQ 31.78125 56 36.171875 55.265625 \nQ 40.578125 54.546875 44.28125 53.078125 \nz\n\" id=\"DejaVuSans-115\"/>\n      <path d=\"M 41.109375 46.296875 \nQ 39.59375 47.171875 37.8125 47.578125 \nQ 36.03125 48 33.890625 48 \nQ 26.265625 48 22.1875 43.046875 \nQ 18.109375 38.09375 18.109375 28.8125 \nL 18.109375 0 \nL 9.078125 0 \nL 9.078125 54.6875 \nL 18.109375 54.6875 \nL 18.109375 46.1875 \nQ 20.953125 51.171875 25.484375 53.578125 \nQ 30.03125 56 36.53125 56 \nQ 37.453125 56 38.578125 55.875 \nQ 39.703125 55.765625 41.0625 55.515625 \nz\n\" id=\"DejaVuSans-114\"/>\n     </defs>\n     <g transform=\"translate(14.798438 244.818125)rotate(-90)scale(0.1 -0.1)\">\n      <use xlink:href=\"#DejaVuSans-102\"/>\n      <use x=\"35.205078\" xlink:href=\"#DejaVuSans-49\"/>\n      <use x=\"98.828125\" xlink:href=\"#DejaVuSans-45\"/>\n      <use x=\"134.912109\" xlink:href=\"#DejaVuSans-115\"/>\n      <use x=\"187.011719\" xlink:href=\"#DejaVuSans-99\"/>\n      <use x=\"241.992188\" xlink:href=\"#DejaVuSans-111\"/>\n      <use x=\"303.173828\" xlink:href=\"#DejaVuSans-114\"/>\n      <use x=\"342.037109\" xlink:href=\"#DejaVuSans-101\"/>\n     </g>\n    </g>\n   </g>\n   <g id=\"line2d_15\">\n    <path clip-path=\"url(#p428172f770)\" d=\"M 69.144886 422.312727 \nL 91.200222 260.239041 \nL 113.255558 158.044851 \nL 135.310894 131.622018 \nL 157.36623 110.489864 \nL 179.421566 92.623147 \nL 201.476902 84.303195 \nL 223.532238 71.829735 \nL 245.587574 69.18153 \nL 267.64291 54.736003 \nL 289.698246 52.992503 \nL 311.753582 47.8127 \nL 333.808918 45.90945 \nL 355.864254 41.934401 \nL 377.91959 35.700411 \nL 399.974926 36.905763 \nL 422.030262 31.947926 \nL 444.085598 27.900615 \nL 466.140934 38.053675 \nL 488.19627 29.730899 \nL 510.251606 37.098021 \nL 532.306942 29.46345 \nL 554.362278 26.967273 \nL 576.417614 29.857456 \n\" style=\"fill:none;stroke:#1f77b4;stroke-linecap:square;stroke-width:1.5;\"/>\n    <defs>\n     <path d=\"M 0 3 \nC 0.795609 3 1.55874 2.683901 2.12132 2.12132 \nC 2.683901 1.55874 3 0.795609 3 0 \nC 3 -0.795609 2.683901 -1.55874 2.12132 -2.12132 \nC 1.55874 -2.683901 0.795609 -3 0 -3 \nC -0.795609 -3 -1.55874 -2.683901 -2.12132 -2.12132 \nC -2.683901 -1.55874 -3 -0.795609 -3 0 \nC -3 0.795609 -2.683901 1.55874 -2.12132 2.12132 \nC -1.55874 2.683901 -0.795609 3 0 3 \nz\n\" id=\"md3f629a6b6\" style=\"stroke:#1f77b4;\"/>\n    </defs>\n    <g clip-path=\"url(#p428172f770)\">\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"69.144886\" xlink:href=\"#md3f629a6b6\" y=\"422.312727\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"91.200222\" xlink:href=\"#md3f629a6b6\" y=\"260.239041\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"113.255558\" xlink:href=\"#md3f629a6b6\" y=\"158.044851\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"135.310894\" xlink:href=\"#md3f629a6b6\" y=\"131.622018\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"157.36623\" xlink:href=\"#md3f629a6b6\" y=\"110.489864\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"179.421566\" xlink:href=\"#md3f629a6b6\" y=\"92.623147\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"201.476902\" xlink:href=\"#md3f629a6b6\" y=\"84.303195\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"223.532238\" xlink:href=\"#md3f629a6b6\" y=\"71.829735\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"245.587574\" xlink:href=\"#md3f629a6b6\" y=\"69.18153\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"267.64291\" xlink:href=\"#md3f629a6b6\" y=\"54.736003\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"289.698246\" xlink:href=\"#md3f629a6b6\" y=\"52.992503\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"311.753582\" xlink:href=\"#md3f629a6b6\" y=\"47.8127\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"333.808918\" xlink:href=\"#md3f629a6b6\" y=\"45.90945\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"355.864254\" xlink:href=\"#md3f629a6b6\" y=\"41.934401\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"377.91959\" xlink:href=\"#md3f629a6b6\" y=\"35.700411\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"399.974926\" xlink:href=\"#md3f629a6b6\" y=\"36.905763\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"422.030262\" xlink:href=\"#md3f629a6b6\" y=\"31.947926\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"444.085598\" xlink:href=\"#md3f629a6b6\" y=\"27.900615\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"466.140934\" xlink:href=\"#md3f629a6b6\" y=\"38.053675\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"488.19627\" xlink:href=\"#md3f629a6b6\" y=\"29.730899\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"510.251606\" xlink:href=\"#md3f629a6b6\" y=\"37.098021\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"532.306942\" xlink:href=\"#md3f629a6b6\" y=\"29.46345\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"554.362278\" xlink:href=\"#md3f629a6b6\" y=\"26.967273\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"576.417614\" xlink:href=\"#md3f629a6b6\" y=\"29.857456\"/>\n    </g>\n   </g>\n   <g id=\"patch_3\">\n    <path d=\"M 43.78125 442.08 \nL 43.78125 7.2 \n\" style=\"fill:none;stroke:#000000;stroke-linecap:square;stroke-linejoin:miter;stroke-width:0.8;\"/>\n   </g>\n   <g id=\"patch_4\">\n    <path d=\"M 601.78125 442.08 \nL 601.78125 7.2 \n\" style=\"fill:none;stroke:#000000;stroke-linecap:square;stroke-linejoin:miter;stroke-width:0.8;\"/>\n   </g>\n   <g id=\"patch_5\">\n    <path d=\"M 43.78125 442.08 \nL 601.78125 442.08 \n\" style=\"fill:none;stroke:#000000;stroke-linecap:square;stroke-linejoin:miter;stroke-width:0.8;\"/>\n   </g>\n   <g id=\"patch_6\">\n    <path d=\"M 43.78125 7.2 \nL 601.78125 7.2 \n\" style=\"fill:none;stroke:#000000;stroke-linecap:square;stroke-linejoin:miter;stroke-width:0.8;\"/>\n   </g>\n  </g>\n </g>\n <defs>\n  <clipPath id=\"p428172f770\">\n   <rect height=\"434.88\" width=\"558\" x=\"43.78125\" y=\"7.2\"/>\n  </clipPath>\n </defs>\n</svg>\n"
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pnmoT8jrTkur",
        "colab_type": "code",
        "outputId": "0282bebf-74aa-4853-ddf8-71ee52f0c854",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "# Load the last checkpoint with the best model\n",
        "trainer.model.load_state_dict(torch.load(DATA_PATH / 'regular_checkpoint.pt'))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<All keys matched successfully>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 56
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_8R-TF2boKCQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Read the test set and evaluate on that\n",
        "#test_doc = Document(DATA_PATH / 'project_test_manual_corrs_bilou.txt')\n",
        "test_doc = Document(DATA_PATH / 'project_test_manual_corrs.txt')\n",
        "test_dataset = CONLLUDataset(test_doc, pretrain, vocab=vocab, test=True)\n",
        "test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=shuffle_dataset, collate_fn=pad_collate)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6LnlPANZoNm_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "test_loss, test_score, test_words, test_preds, test_report = trainer.evaluate(test_loader, test=True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TbXaAo-ooPUT",
        "colab_type": "code",
        "outputId": "fde9e5c5-ccaa-41ee-9240-7421b269e2b4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "test_score"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.7962962962962963"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 59
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yQgI--iPoRie",
        "colab_type": "code",
        "outputId": "cd79c9f0-744d-491a-bc6e-c8dc427c64e0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 157
        }
      },
      "source": [
        "from pprint import pprint\n",
        "pprint(test_report)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "('           precision    recall  f1-score   support\\n'\n",
            " '\\n'\n",
            " '      ORG      0.711     0.660     0.684       626\\n'\n",
            " '      LOC      0.877     0.810     0.842       825\\n'\n",
            " '      PER      0.844     0.821     0.832       883\\n'\n",
            " '\\n'\n",
            " 'micro avg      0.820     0.774     0.796      2334\\n'\n",
            " 'macro avg      0.820     0.774     0.796      2334\\n')\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "OquJjKyoHenf",
        "colab": {}
      },
      "source": [
        "import os\n",
        "\n",
        "output_dir = DATA_PATH / 'model'\n",
        "\n",
        "# Create output directory if needed\n",
        "if not os.path.exists(output_dir):\n",
        "    os.makedirs(output_dir)\n",
        "\n",
        "print(\"Saving model to %s\" % output_dir)\n",
        "\n",
        "output_file = output_dir / 'latest_ner.bin'\n",
        "\n",
        "torch.save(trainer.model.state_dict(), output_file)\n",
        "\n",
        "# To load the saved state later:\n",
        "#the_model = TheModelClass(*args, **kwargs)\n",
        "#the_model.load_state_dict(torch.load(PATH))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "5Fjsv-ITH2lJ",
        "colab": {}
      },
      "source": [
        "# Test whether the model can overfit on small subset of data\n",
        "\n",
        "trainer = Trainer(vocab, word_emb_dim, char_emb_dim, transformed_dim,\n",
        "                  emb_matrix, hidden_dim, char_hidden_dim, label_clf_hidden_dim,\n",
        "                  num_layers, dropout)\n",
        "\n",
        "epochs = 100\n",
        "eval_interval = 100\n",
        "global_step = 0\n",
        "train_loss = 0\n",
        "for idx in range(epochs):\n",
        "    #print(f'Epoch: {idx+1}/{epochs}')\n",
        "    for batch in small_train_loader:\n",
        "        start_time = time()\n",
        "        global_step += 1\n",
        "        loss = trainer.update(batch, eval=False)\n",
        "        train_loss += loss\n",
        "\n",
        "        if global_step % log_step == 0:\n",
        "            duration = time() - start_time\n",
        "            print(format_str.format(datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\"), global_step,\\\n",
        "                    max_steps, loss, duration))\n",
        "            \n",
        "        if global_step % eval_interval == 0 or global_step == max_steps:\n",
        "            print('Evaluating on train set...')\n",
        "            train_loss, train_score, train_words, train_preds = trainer.evaluate(small_train_loader)\n",
        "            \n",
        "            print('Evaluating on dev set...')\n",
        "            dev_loss, dev_score, dev_words, dev_preds = trainer.evaluate(dev_loader)\n",
        "\n",
        "            print(\"step {}: train_score = {:.6f}, dev_score = {:.6f}\".format(global_step, train_score, dev_score))\n",
        "            # Shows one prediction for a sanity check\n",
        "            print(f'Preds: {list(zip(dev_preds[0], dev_words[0]))}')\n",
        "            train_loss = 0"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}